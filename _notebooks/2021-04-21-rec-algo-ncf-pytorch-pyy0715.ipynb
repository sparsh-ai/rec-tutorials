{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020-04-21-rec-algo-ncf-pytorch-pyy0715.ipynb","provenance":[{"file_id":"https://github.com/sparsh-ai/rec-code/blob/main/notebooks/rec_algo_ncf_pytorch_pyy0715.ipynb","timestamp":1618941721731}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"uWZA89eShbXd"},"source":["# Neural Matrix Factorization from scratch in PyTorch\n","> A tutorial to understand the process of building a **Neural Matrix Factorization** model from scratch in PyTorch on MovieLens-1M dataset.\n","\n","- toc: true\n","- badges: true\n","- comments: true\n","- categories: [matrixfactorization movielens pytorch scratch]\n","- image: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RlWV9QEheB29","executionInfo":{"status":"ok","timestamp":1618942130306,"user_tz":-330,"elapsed":4109,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}},"outputId":"61640e5f-6218-4619-a6fa-45812d6f772d"},"source":["!pip install -q tensorboardX"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |██▊                             | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 20.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30kB 11.2MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 8.3MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p1E54ayu9RON","executionInfo":{"status":"ok","timestamp":1618942134274,"user_tz":-330,"elapsed":1088,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}}},"source":["import os\n","import time\n","import random\n","import argparse\n","import numpy as np \n","import pandas as pd \n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from tensorboardX import SummaryWriter"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J17ZmmLNcx-G"},"source":["## Downloading Movielens-1M Ratings"]},{"cell_type":"code","metadata":{"id":"YAa3Vwhq6agf","executionInfo":{"status":"ok","timestamp":1618942135820,"user_tz":-330,"elapsed":1183,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}}},"source":["DATA_URL = \"https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-1m-dat/ratings.dat\"\n","MAIN_PATH = '/content/'\n","DATA_PATH = MAIN_PATH + 'ratings.dat'\n","MODEL_PATH = MAIN_PATH + 'models/'\n","MODEL = 'ml-1m_Neu_MF'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zM4ltadpAygl","executionInfo":{"status":"ok","timestamp":1618942138272,"user_tz":-330,"elapsed":3344,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}},"outputId":"c142095f-8e37-45cd-de4f-eefd2b87b01d"},"source":["#hide-output\n","!wget -nc https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-1m-dat/ratings.dat"],"execution_count":5,"outputs":[{"output_type":"stream","text":["--2021-04-20 18:08:55--  https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-1m-dat/ratings.dat\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 24594131 (23M) [text/plain]\n","Saving to: ‘ratings.dat’\n","\n","ratings.dat         100%[===================>]  23.45M  81.3MB/s    in 0.3s    \n","\n","2021-04-20 18:08:57 (81.3 MB/s) - ‘ratings.dat’ saved [24594131/24594131]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VT89-ZSZ9pMl","executionInfo":{"status":"ok","timestamp":1618942138274,"user_tz":-330,"elapsed":1399,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}}},"source":["#hide\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KJH1FfkIems4"},"source":["## Defining Dataset Classes"]},{"cell_type":"code","metadata":{"id":"663oFevqek3a","executionInfo":{"status":"ok","timestamp":1618942256689,"user_tz":-330,"elapsed":1179,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}}},"source":["class Rating_Datset(torch.utils.data.Dataset):\n","\tdef __init__(self, user_list, item_list, rating_list):\n","\t\tsuper(Rating_Datset, self).__init__()\n","\t\tself.user_list = user_list\n","\t\tself.item_list = item_list\n","\t\tself.rating_list = rating_list\n","\n","\tdef __len__(self):\n","\t\treturn len(self.user_list)\n","\n","\tdef __getitem__(self, idx):\n","\t\tuser = self.user_list[idx]\n","\t\titem = self.item_list[idx]\n","\t\trating = self.rating_list[idx]\n","\t\t\n","\t\treturn (\n","\t\t\ttorch.tensor(user, dtype=torch.long),\n","\t\t\ttorch.tensor(item, dtype=torch.long),\n","\t\t\ttorch.tensor(rating, dtype=torch.float)\n","\t\t\t)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4xgxyBsfoJM"},"source":["### NCF Dataset Class\n","- *_reindex*: process dataset to reindex userID and itemID, also set rating as binary feedback\n","- *_leave_one_out*: leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n","- *negative_sampling*: randomly selects n negative examples for each positive one"]},{"cell_type":"code","metadata":{"id":"HDXTuM0C6BGr","executionInfo":{"status":"ok","timestamp":1618942279914,"user_tz":-330,"elapsed":1213,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}}},"source":["class NCF_Data(object):\n","\t\"\"\"\n","\tConstruct Dataset for NCF\n","\t\"\"\"\n","\tdef __init__(self, args, ratings):\n","\t\tself.ratings = ratings\n","\t\tself.num_ng = args.num_ng\n","\t\tself.num_ng_test = args.num_ng_test\n","\t\tself.batch_size = args.batch_size\n","\n","\t\tself.preprocess_ratings = self._reindex(self.ratings)\n","\n","\t\tself.user_pool = set(self.ratings['user_id'].unique())\n","\t\tself.item_pool = set(self.ratings['item_id'].unique())\n","\n","\t\tself.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n","\t\tself.negatives = self._negative_sampling(self.preprocess_ratings)\n","\t\trandom.seed(args.seed)\n","\t\n","\tdef _reindex(self, ratings):\n","\t\t\"\"\"\n","\t\tProcess dataset to reindex userID and itemID, also set rating as binary feedback\n","\t\t\"\"\"\n","\t\tuser_list = list(ratings['user_id'].drop_duplicates())\n","\t\tuser2id = {w: i for i, w in enumerate(user_list)}\n","\n","\t\titem_list = list(ratings['item_id'].drop_duplicates())\n","\t\titem2id = {w: i for i, w in enumerate(item_list)}\n","\n","\t\tratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n","\t\tratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n","\t\tratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n","\t\treturn ratings\n","\n","\tdef _leave_one_out(self, ratings):\n","\t\t\"\"\"\n","\t\tleave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n","\t\t\"\"\"\n","\t\tratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n","\t\ttest = ratings.loc[ratings['rank_latest'] == 1]\n","\t\ttrain = ratings.loc[ratings['rank_latest'] > 1]\n","\t\tassert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n","\t\treturn train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n","\n","\tdef _negative_sampling(self, ratings):\n","\t\tinteract_status = (\n","\t\t\tratings.groupby('user_id')['item_id']\n","\t\t\t.apply(set)\n","\t\t\t.reset_index()\n","\t\t\t.rename(columns={'item_id': 'interacted_items'}))\n","\t\tinteract_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n","\t\tinteract_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n","\t\treturn interact_status[['user_id', 'negative_items', 'negative_samples']]\n","\n","\tdef get_train_instance(self):\n","\t\tusers, items, ratings = [], [], []\n","\t\ttrain_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n","\t\ttrain_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n","\t\tfor row in train_ratings.itertuples():\n","\t\t\tusers.append(int(row.user_id))\n","\t\t\titems.append(int(row.item_id))\n","\t\t\tratings.append(float(row.rating))\n","\t\t\tfor i in range(self.num_ng):\n","\t\t\t\tusers.append(int(row.user_id))\n","\t\t\t\titems.append(int(row.negatives[i]))\n","\t\t\t\tratings.append(float(0))  # negative samples get 0 rating\n","\t\tdataset = Rating_Datset(\n","\t\t\tuser_list=users,\n","\t\t\titem_list=items,\n","\t\t\trating_list=ratings)\n","\t\treturn torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n","\n","\tdef get_test_instance(self):\n","\t\tusers, items, ratings = [], [], []\n","\t\ttest_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n","\t\tfor row in test_ratings.itertuples():\n","\t\t\tusers.append(int(row.user_id))\n","\t\t\titems.append(int(row.item_id))\n","\t\t\tratings.append(float(row.rating))\n","\t\t\tfor i in getattr(row, 'negative_samples'):\n","\t\t\t\tusers.append(int(row.user_id))\n","\t\t\t\titems.append(int(i))\n","\t\t\t\tratings.append(float(0))\n","\t\tdataset = Rating_Datset(\n","\t\t\tuser_list=users,\n","\t\t\titem_list=items,\n","\t\t\trating_list=ratings)\n","\t\treturn torch.utils.data.DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=2)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"POQwMIk2dSki"},"source":["## Defining Metrics\n","Using Hit Rate and NDCG as our evaluation *metrics*"]},{"cell_type":"code","metadata":{"id":"WH6x2T559cL1","executionInfo":{"status":"ok","timestamp":1618942283644,"user_tz":-330,"elapsed":723,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}}},"source":["def hit(ng_item, pred_items):\n","\tif ng_item in pred_items:\n","\t\treturn 1\n","\treturn 0\n","\n","\n","def ndcg(ng_item, pred_items):\n","\tif ng_item in pred_items:\n","\t\tindex = pred_items.index(ng_item)\n","\t\treturn np.reciprocal(np.log2(index+2))\n","\treturn 0\n","\n","\n","def metrics(model, test_loader, top_k, device):\n","\tHR, NDCG = [], []\n","\n","\tfor user, item, label in test_loader:\n","\t\tuser = user.to(device)\n","\t\titem = item.to(device)\n","\n","\t\tpredictions = model(user, item)\n","\t\t_, indices = torch.topk(predictions, top_k)\n","\t\trecommends = torch.take(\n","\t\t\t\titem, indices).cpu().numpy().tolist()\n","\n","\t\tng_item = item[0].item() # leave one-out evaluation has only one item per user\n","\t\tHR.append(hit(ng_item, recommends))\n","\t\tNDCG.append(ndcg(ng_item, recommends))\n","\n","\treturn np.mean(HR), np.mean(NDCG)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"waDSZHz7dlNo"},"source":["### Defining Model Architectures\n","1. Generalized Matrix Factorization\n","2. Multi Layer Perceptron\n","3. Neural Matrix Factorization"]},{"cell_type":"code","metadata":{"id":"aTQaitu7d1R3","executionInfo":{"status":"ok","timestamp":1618942291462,"user_tz":-330,"elapsed":1410,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}}},"source":["class Generalized_Matrix_Factorization(nn.Module):\n","    def __init__(self, args, num_users, num_items):\n","        super(Generalized_Matrix_Factorization, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.factor_num = args.factor_num\n","\n","        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n","        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n","\n","        self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1)\n","        self.logistic = nn.Sigmoid()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding = self.embedding_user(user_indices)\n","        item_embedding = self.embedding_item(item_indices)\n","        element_product = torch.mul(user_embedding, item_embedding)\n","        logits = self.affine_output(element_product)\n","        rating = self.logistic(logits)\n","        return rating\n","\n","    def init_weight(self):\n","        pass"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kSFzPlNd50f","executionInfo":{"status":"ok","timestamp":1618942291463,"user_tz":-330,"elapsed":1275,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}}},"source":["class Multi_Layer_Perceptron(nn.Module):\n","    def __init__(self, args, num_users, num_items):\n","        super(Multi_Layer_Perceptron, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.factor_num = args.factor_num\n","        self.layers = args.layers\n","\n","        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n","        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n","\n","        self.fc_layers = nn.ModuleList()\n","        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n","            self.fc_layers.append(nn.Linear(in_size, out_size))\n","\n","        self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1)\n","        self.logistic = nn.Sigmoid()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding = self.embedding_user(user_indices)\n","        item_embedding = self.embedding_item(item_indices)\n","        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n","        for idx, _ in enumerate(range(len(self.fc_layers))):\n","            vector = self.fc_layers[idx](vector)\n","            vector = nn.ReLU()(vector)\n","            # vector = nn.BatchNorm1d()(vector)\n","            # vector = nn.Dropout(p=0.5)(vector)\n","        logits = self.affine_output(vector)\n","        rating = self.logistic(logits)\n","        return rating\n","\n","    def init_weight(self):\n","        pass"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DQpVuaV9cF0","executionInfo":{"status":"ok","timestamp":1618942291464,"user_tz":-330,"elapsed":1126,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}}},"source":["class NeuMF(nn.Module):\n","    def __init__(self, args, num_users, num_items):\n","        super(NeuMF, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.factor_num_mf = args.factor_num\n","        self.factor_num_mlp =  int(args.layers[0]/2)\n","        self.layers = args.layers\n","        self.dropout = args.dropout\n","\n","        self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)\n","        self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)\n","\n","        self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf)\n","        self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf)\n","\n","        self.fc_layers = nn.ModuleList()\n","        for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])):\n","            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n","            self.fc_layers.append(nn.ReLU())\n","\n","        self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_mf, out_features=1)\n","        self.logistic = nn.Sigmoid()\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        nn.init.normal_(self.embedding_user_mlp.weight, std=0.01)\n","        nn.init.normal_(self.embedding_item_mlp.weight, std=0.01)\n","        nn.init.normal_(self.embedding_user_mf.weight, std=0.01)\n","        nn.init.normal_(self.embedding_item_mf.weight, std=0.01)\n","        \n","        for m in self.fc_layers:\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_uniform_(m.weight)\n","                \n","        nn.init.xavier_uniform_(self.affine_output.weight)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n","        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n","\n","        user_embedding_mf = self.embedding_user_mf(user_indices)\n","        item_embedding_mf = self.embedding_item_mf(item_indices)\n","\n","        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)\n","        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n","\n","        for idx, _ in enumerate(range(len(self.fc_layers))):\n","            mlp_vector = self.fc_layers[idx](mlp_vector)\n","\n","        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n","        logits = self.affine_output(vector)\n","        rating = self.logistic(logits)\n","        return rating.squeeze()"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpBX6rqNfSc9"},"source":["### Setting Arguments\n","\n","Here is the brief description of important ones:\n","- Learning rate is 0.001\n","- Dropout rate is 0.2\n","- Running for 10 epochs\n","- HitRate@10 and NDCG@10\n","- 4 negative samples for each positive one"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bc5Vg1Ik_gnF","executionInfo":{"status":"ok","timestamp":1618942419272,"user_tz":-330,"elapsed":1415,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}},"outputId":"0eacaea6-3e24-48b2-de51-b9396070c22a"},"source":["#collapse-hide\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--seed\", \n","\ttype=int, \n","\tdefault=42, \n","\thelp=\"Seed\")\n","parser.add_argument(\"--lr\", \n","\ttype=float, \n","\tdefault=0.001, \n","\thelp=\"learning rate\")\n","parser.add_argument(\"--dropout\", \n","\ttype=float,\n","\tdefault=0.2,  \n","\thelp=\"dropout rate\")\n","parser.add_argument(\"--batch_size\", \n","\ttype=int, \n","\tdefault=256, \n","\thelp=\"batch size for training\")\n","parser.add_argument(\"--epochs\", \n","\ttype=int,\n","\tdefault=10,  \n","\thelp=\"training epoches\")\n","parser.add_argument(\"--top_k\", \n","\ttype=int, \n","\tdefault=10, \n","\thelp=\"compute metrics@top_k\")\n","parser.add_argument(\"--factor_num\", \n","\ttype=int,\n","\tdefault=32, \n","\thelp=\"predictive factors numbers in the model\")\n","parser.add_argument(\"--layers\",\n","    nargs='+', \n","    default=[64,32,16,8],\n","    help=\"MLP layers. Note that the first layer is the concatenation of user \\\n","    and item embeddings. So layers[0]/2 is the embedding size.\")\n","parser.add_argument(\"--num_ng\", \n","\ttype=int,\n","\tdefault=4, \n","\thelp=\"Number of negative samples for training set\")\n","parser.add_argument(\"--num_ng_test\", \n","\ttype=int,\n","\tdefault=100, \n","\thelp=\"Number of negative samples for test set\")\n","parser.add_argument(\"--out\", \n","\tdefault=True,\n","\thelp=\"save model or not\")"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--out'], dest='out', nargs=None, const=None, default=True, type=None, choices=None, help='save model or not', metavar=None)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"RnaRWy2gg_Nw"},"source":["## Training NeuMF Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VyWquJG893CV","executionInfo":{"status":"ok","timestamp":1618943988553,"user_tz":-330,"elapsed":1570080,"user":{"displayName":"sparsh agarwal","photoUrl":"","userId":"00322518567794762549"}},"outputId":"53668847-1b49-4a77-e5d6-b5a2b2f5bf82"},"source":["# set device and parameters\n","args = parser.parse_args(\"\")\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","writer = SummaryWriter()\n","\n","# seed for Reproducibility\n","seed_everything(args.seed)\n","\n","# load data\n","ml_1m = pd.read_csv(\n","\tDATA_PATH, \n","\tsep=\"::\", \n","\tnames = ['user_id', 'item_id', 'rating', 'timestamp'], \n","\tengine='python')\n","\n","# set the num_users, items\n","num_users = ml_1m['user_id'].nunique()+1\n","num_items = ml_1m['item_id'].nunique()+1\n","\n","# construct the train and test datasets\n","data = NCF_Data(args, ml_1m)\n","train_loader = data.get_train_instance()\n","test_loader = data.get_test_instance()\n","\n","# set model and loss, optimizer\n","model = NeuMF(args, num_users, num_items)\n","model = model.to(device)\n","loss_function = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","\n","# train, evaluation\n","best_hr = 0\n","for epoch in range(1, args.epochs+1):\n","\tmodel.train() # Enable dropout (if have).\n","\tstart_time = time.time()\n","\n","\tfor user, item, label in train_loader:\n","\t\tuser = user.to(device)\n","\t\titem = item.to(device)\n","\t\tlabel = label.to(device)\n","\n","\t\toptimizer.zero_grad()\n","\t\tprediction = model(user, item)\n","\t\tloss = loss_function(prediction, label)\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\t\twriter.add_scalar('loss/Train_loss', loss.item(), epoch)\n","\n","\tmodel.eval()\n","\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n","\twriter.add_scalar('Perfomance/HR@10', HR, epoch)\n","\twriter.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n","\n","\telapsed_time = time.time() - start_time\n","\tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n","\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n","\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n","\n","\tif HR > best_hr:\n","\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n","\t\tif args.out:\n","\t\t\tif not os.path.exists(MODEL_PATH):\n","\t\t\t\tos.mkdir(MODEL_PATH)\n","\t\t\ttorch.save(model, \n","\t\t\t\t'{}{}.pth'.format(MODEL_PATH, MODEL))\n","\n","writer.close()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["The time elapse of epoch 001 is: 00: 02: 30\n","HR: 0.624\tNDCG: 0.362\n","The time elapse of epoch 002 is: 00: 02: 31\n","HR: 0.663\tNDCG: 0.392\n","The time elapse of epoch 003 is: 00: 02: 30\n","HR: 0.673\tNDCG: 0.399\n","The time elapse of epoch 004 is: 00: 02: 30\n","HR: 0.677\tNDCG: 0.402\n","The time elapse of epoch 005 is: 00: 02: 31\n","HR: 0.671\tNDCG: 0.399\n","The time elapse of epoch 006 is: 00: 02: 32\n","HR: 0.671\tNDCG: 0.400\n","The time elapse of epoch 007 is: 00: 02: 32\n","HR: 0.669\tNDCG: 0.400\n","The time elapse of epoch 008 is: 00: 02: 31\n","HR: 0.665\tNDCG: 0.395\n","The time elapse of epoch 009 is: 00: 02: 33\n","HR: 0.664\tNDCG: 0.393\n","The time elapse of epoch 010 is: 00: 02: 32\n","HR: 0.667\tNDCG: 0.394\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LbZRS25AhD_p"},"source":["## Final Output"]},{"cell_type":"code","metadata":{"id":"fkiRJWeD_trR"},"source":["print(\"Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format(\n","\t\t\t\t\t\t\t\t\tbest_epoch, best_hr, best_ndcg))"],"execution_count":null,"outputs":[]}]}