{
  
    
        "post0": {
            "title": "Neural Matrix Factorization from scratch in PyTorch",
            "content": "!pip install -q tensorboardX . |████████████████████████████████| 122kB 8.3MB/s . import os import time import random import argparse import numpy as np import pandas as pd import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as data from tensorboardX import SummaryWriter . Downloading Movielens-1M Ratings . DATA_URL = &quot;https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-1m-dat/ratings.dat&quot; MAIN_PATH = &#39;/content/&#39; DATA_PATH = MAIN_PATH + &#39;ratings.dat&#39; MODEL_PATH = MAIN_PATH + &#39;models/&#39; MODEL = &#39;ml-1m_Neu_MF&#39; . !wget -nc https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-1m-dat/ratings.dat . Defining Dataset Classes . class Rating_Datset(torch.utils.data.Dataset): def __init__(self, user_list, item_list, rating_list): super(Rating_Datset, self).__init__() self.user_list = user_list self.item_list = item_list self.rating_list = rating_list def __len__(self): return len(self.user_list) def __getitem__(self, idx): user = self.user_list[idx] item = self.item_list[idx] rating = self.rating_list[idx] return ( torch.tensor(user, dtype=torch.long), torch.tensor(item, dtype=torch.long), torch.tensor(rating, dtype=torch.float) ) . NCF Dataset Class . _reindex: process dataset to reindex userID and itemID, also set rating as binary feedback | _leave_one_out: leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf | negative_sampling: randomly selects n negative examples for each positive one | . class NCF_Data(object): &quot;&quot;&quot; Construct Dataset for NCF &quot;&quot;&quot; def __init__(self, args, ratings): self.ratings = ratings self.num_ng = args.num_ng self.num_ng_test = args.num_ng_test self.batch_size = args.batch_size self.preprocess_ratings = self._reindex(self.ratings) self.user_pool = set(self.ratings[&#39;user_id&#39;].unique()) self.item_pool = set(self.ratings[&#39;item_id&#39;].unique()) self.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings) self.negatives = self._negative_sampling(self.preprocess_ratings) random.seed(args.seed) def _reindex(self, ratings): &quot;&quot;&quot; Process dataset to reindex userID and itemID, also set rating as binary feedback &quot;&quot;&quot; user_list = list(ratings[&#39;user_id&#39;].drop_duplicates()) user2id = {w: i for i, w in enumerate(user_list)} item_list = list(ratings[&#39;item_id&#39;].drop_duplicates()) item2id = {w: i for i, w in enumerate(item_list)} ratings[&#39;user_id&#39;] = ratings[&#39;user_id&#39;].apply(lambda x: user2id[x]) ratings[&#39;item_id&#39;] = ratings[&#39;item_id&#39;].apply(lambda x: item2id[x]) ratings[&#39;rating&#39;] = ratings[&#39;rating&#39;].apply(lambda x: float(x &gt; 0)) return ratings def _leave_one_out(self, ratings): &quot;&quot;&quot; leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf &quot;&quot;&quot; ratings[&#39;rank_latest&#39;] = ratings.groupby([&#39;user_id&#39;])[&#39;timestamp&#39;].rank(method=&#39;first&#39;, ascending=False) test = ratings.loc[ratings[&#39;rank_latest&#39;] == 1] train = ratings.loc[ratings[&#39;rank_latest&#39;] &gt; 1] assert train[&#39;user_id&#39;].nunique()==test[&#39;user_id&#39;].nunique(), &#39;Not Match Train User with Test User&#39; return train[[&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;]], test[[&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;]] def _negative_sampling(self, ratings): interact_status = ( ratings.groupby(&#39;user_id&#39;)[&#39;item_id&#39;] .apply(set) .reset_index() .rename(columns={&#39;item_id&#39;: &#39;interacted_items&#39;})) interact_status[&#39;negative_items&#39;] = interact_status[&#39;interacted_items&#39;].apply(lambda x: self.item_pool - x) interact_status[&#39;negative_samples&#39;] = interact_status[&#39;negative_items&#39;].apply(lambda x: random.sample(x, self.num_ng_test)) return interact_status[[&#39;user_id&#39;, &#39;negative_items&#39;, &#39;negative_samples&#39;]] def get_train_instance(self): users, items, ratings = [], [], [] train_ratings = pd.merge(self.train_ratings, self.negatives[[&#39;user_id&#39;, &#39;negative_items&#39;]], on=&#39;user_id&#39;) train_ratings[&#39;negatives&#39;] = train_ratings[&#39;negative_items&#39;].apply(lambda x: random.sample(x, self.num_ng)) for row in train_ratings.itertuples(): users.append(int(row.user_id)) items.append(int(row.item_id)) ratings.append(float(row.rating)) for i in range(self.num_ng): users.append(int(row.user_id)) items.append(int(row.negatives[i])) ratings.append(float(0)) # negative samples get 0 rating dataset = Rating_Datset( user_list=users, item_list=items, rating_list=ratings) return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4) def get_test_instance(self): users, items, ratings = [], [], [] test_ratings = pd.merge(self.test_ratings, self.negatives[[&#39;user_id&#39;, &#39;negative_samples&#39;]], on=&#39;user_id&#39;) for row in test_ratings.itertuples(): users.append(int(row.user_id)) items.append(int(row.item_id)) ratings.append(float(row.rating)) for i in getattr(row, &#39;negative_samples&#39;): users.append(int(row.user_id)) items.append(int(i)) ratings.append(float(0)) dataset = Rating_Datset( user_list=users, item_list=items, rating_list=ratings) return torch.utils.data.DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=2) . Defining Metrics . Using Hit Rate and NDCG as our evaluation metrics . def hit(ng_item, pred_items): if ng_item in pred_items: return 1 return 0 def ndcg(ng_item, pred_items): if ng_item in pred_items: index = pred_items.index(ng_item) return np.reciprocal(np.log2(index+2)) return 0 def metrics(model, test_loader, top_k, device): HR, NDCG = [], [] for user, item, label in test_loader: user = user.to(device) item = item.to(device) predictions = model(user, item) _, indices = torch.topk(predictions, top_k) recommends = torch.take( item, indices).cpu().numpy().tolist() ng_item = item[0].item() # leave one-out evaluation has only one item per user HR.append(hit(ng_item, recommends)) NDCG.append(ndcg(ng_item, recommends)) return np.mean(HR), np.mean(NDCG) . Defining Model Architectures . Generalized Matrix Factorization | Multi Layer Perceptron | Neural Matrix Factorization | class Generalized_Matrix_Factorization(nn.Module): def __init__(self, args, num_users, num_items): super(Generalized_Matrix_Factorization, self).__init__() self.num_users = num_users self.num_items = num_items self.factor_num = args.factor_num self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num) self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num) self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1) self.logistic = nn.Sigmoid() def forward(self, user_indices, item_indices): user_embedding = self.embedding_user(user_indices) item_embedding = self.embedding_item(item_indices) element_product = torch.mul(user_embedding, item_embedding) logits = self.affine_output(element_product) rating = self.logistic(logits) return rating def init_weight(self): pass . class Multi_Layer_Perceptron(nn.Module): def __init__(self, args, num_users, num_items): super(Multi_Layer_Perceptron, self).__init__() self.num_users = num_users self.num_items = num_items self.factor_num = args.factor_num self.layers = args.layers self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num) self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num) self.fc_layers = nn.ModuleList() for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])): self.fc_layers.append(nn.Linear(in_size, out_size)) self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1) self.logistic = nn.Sigmoid() def forward(self, user_indices, item_indices): user_embedding = self.embedding_user(user_indices) item_embedding = self.embedding_item(item_indices) vector = torch.cat([user_embedding, item_embedding], dim=-1) # the concat latent vector for idx, _ in enumerate(range(len(self.fc_layers))): vector = self.fc_layers[idx](vector) vector = nn.ReLU()(vector) # vector = nn.BatchNorm1d()(vector) # vector = nn.Dropout(p=0.5)(vector) logits = self.affine_output(vector) rating = self.logistic(logits) return rating def init_weight(self): pass . class NeuMF(nn.Module): def __init__(self, args, num_users, num_items): super(NeuMF, self).__init__() self.num_users = num_users self.num_items = num_items self.factor_num_mf = args.factor_num self.factor_num_mlp = int(args.layers[0]/2) self.layers = args.layers self.dropout = args.dropout self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp) self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp) self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf) self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf) self.fc_layers = nn.ModuleList() for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])): self.fc_layers.append(torch.nn.Linear(in_size, out_size)) self.fc_layers.append(nn.ReLU()) self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_mf, out_features=1) self.logistic = nn.Sigmoid() self.init_weight() def init_weight(self): nn.init.normal_(self.embedding_user_mlp.weight, std=0.01) nn.init.normal_(self.embedding_item_mlp.weight, std=0.01) nn.init.normal_(self.embedding_user_mf.weight, std=0.01) nn.init.normal_(self.embedding_item_mf.weight, std=0.01) for m in self.fc_layers: if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight) nn.init.xavier_uniform_(self.affine_output.weight) for m in self.modules(): if isinstance(m, nn.Linear) and m.bias is not None: m.bias.data.zero_() def forward(self, user_indices, item_indices): user_embedding_mlp = self.embedding_user_mlp(user_indices) item_embedding_mlp = self.embedding_item_mlp(item_indices) user_embedding_mf = self.embedding_user_mf(user_indices) item_embedding_mf = self.embedding_item_mf(item_indices) mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1) mf_vector =torch.mul(user_embedding_mf, item_embedding_mf) for idx, _ in enumerate(range(len(self.fc_layers))): mlp_vector = self.fc_layers[idx](mlp_vector) vector = torch.cat([mlp_vector, mf_vector], dim=-1) logits = self.affine_output(vector) rating = self.logistic(logits) return rating.squeeze() . Setting Arguments . Here is the brief description of important ones: . Learning rate is 0.001 | Dropout rate is 0.2 | Running for 10 epochs | HitRate@10 and NDCG@10 | 4 negative samples for each positive one | . parser = argparse.ArgumentParser() parser.add_argument(&quot;--seed&quot;, type=int, default=42, help=&quot;Seed&quot;) parser.add_argument(&quot;--lr&quot;, type=float, default=0.001, help=&quot;learning rate&quot;) parser.add_argument(&quot;--dropout&quot;, type=float, default=0.2, help=&quot;dropout rate&quot;) parser.add_argument(&quot;--batch_size&quot;, type=int, default=256, help=&quot;batch size for training&quot;) parser.add_argument(&quot;--epochs&quot;, type=int, default=10, help=&quot;training epoches&quot;) parser.add_argument(&quot;--top_k&quot;, type=int, default=10, help=&quot;compute metrics@top_k&quot;) parser.add_argument(&quot;--factor_num&quot;, type=int, default=32, help=&quot;predictive factors numbers in the model&quot;) parser.add_argument(&quot;--layers&quot;, nargs=&#39;+&#39;, default=[64,32,16,8], help=&quot;MLP layers. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.&quot;) parser.add_argument(&quot;--num_ng&quot;, type=int, default=4, help=&quot;Number of negative samples for training set&quot;) parser.add_argument(&quot;--num_ng_test&quot;, type=int, default=100, help=&quot;Number of negative samples for test set&quot;) parser.add_argument(&quot;--out&quot;, default=True, help=&quot;save model or not&quot;) . . _StoreAction(option_strings=[&#39;--out&#39;], dest=&#39;out&#39;, nargs=None, const=None, default=True, type=None, choices=None, help=&#39;save model or not&#39;, metavar=None) . Training NeuMF Model . args = parser.parse_args(&quot;&quot;) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) writer = SummaryWriter() # seed for Reproducibility seed_everything(args.seed) # load data ml_1m = pd.read_csv( DATA_PATH, sep=&quot;::&quot;, names = [&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;, &#39;timestamp&#39;], engine=&#39;python&#39;) # set the num_users, items num_users = ml_1m[&#39;user_id&#39;].nunique()+1 num_items = ml_1m[&#39;item_id&#39;].nunique()+1 # construct the train and test datasets data = NCF_Data(args, ml_1m) train_loader = data.get_train_instance() test_loader = data.get_test_instance() # set model and loss, optimizer model = NeuMF(args, num_users, num_items) model = model.to(device) loss_function = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=args.lr) # train, evaluation best_hr = 0 for epoch in range(1, args.epochs+1): model.train() # Enable dropout (if have). start_time = time.time() for user, item, label in train_loader: user = user.to(device) item = item.to(device) label = label.to(device) optimizer.zero_grad() prediction = model(user, item) loss = loss_function(prediction, label) loss.backward() optimizer.step() writer.add_scalar(&#39;loss/Train_loss&#39;, loss.item(), epoch) model.eval() HR, NDCG = metrics(model, test_loader, args.top_k, device) writer.add_scalar(&#39;Perfomance/HR@10&#39;, HR, epoch) writer.add_scalar(&#39;Perfomance/NDCG@10&#39;, NDCG, epoch) elapsed_time = time.time() - start_time print(&quot;The time elapse of epoch {:03d}&quot;.format(epoch) + &quot; is: &quot; + time.strftime(&quot;%H: %M: %S&quot;, time.gmtime(elapsed_time))) print(&quot;HR: {:.3f} tNDCG: {:.3f}&quot;.format(np.mean(HR), np.mean(NDCG))) if HR &gt; best_hr: best_hr, best_ndcg, best_epoch = HR, NDCG, epoch if args.out: if not os.path.exists(MODEL_PATH): os.mkdir(MODEL_PATH) torch.save(model, &#39;{}{}.pth&#39;.format(MODEL_PATH, MODEL)) writer.close() . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) . The time elapse of epoch 001 is: 00: 02: 30 HR: 0.624 NDCG: 0.362 The time elapse of epoch 002 is: 00: 02: 31 HR: 0.663 NDCG: 0.392 The time elapse of epoch 003 is: 00: 02: 30 HR: 0.673 NDCG: 0.399 The time elapse of epoch 004 is: 00: 02: 30 HR: 0.677 NDCG: 0.402 The time elapse of epoch 005 is: 00: 02: 31 HR: 0.671 NDCG: 0.399 The time elapse of epoch 006 is: 00: 02: 32 HR: 0.671 NDCG: 0.400 The time elapse of epoch 007 is: 00: 02: 32 HR: 0.669 NDCG: 0.400 The time elapse of epoch 008 is: 00: 02: 31 HR: 0.665 NDCG: 0.395 The time elapse of epoch 009 is: 00: 02: 33 HR: 0.664 NDCG: 0.393 The time elapse of epoch 010 is: 00: 02: 32 HR: 0.667 NDCG: 0.394 . Final Output . print(&quot;Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}&quot;.format( best_epoch, best_hr, best_ndcg)) .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/matrixfactorization%20movielens%20pytorch%20scratch/2020/04/21/rec-algo-ncf-pytorch-pyy0715.html",
            "relUrl": "/matrixfactorization%20movielens%20pytorch%20scratch/2020/04/21/rec-algo-ncf-pytorch-pyy0715.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Large-scale Document Retrieval with ElasticSearch",
            "content": "Retrieval Flow Overview . . Part 1 - Setting up Elasticsearch . Download the elasticsearch archive (linux), setup a local server | Create a client connection to the local elasticsearch instance | . # download the latest elasticsearch version !wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.11.1-linux-x86_64.tar.gz !tar -xzvf elasticsearch-7.11.1-linux-x86_64.tar.gz !chown -R daemon:daemon elasticsearch-7.11.1 # prep the elasticsearch server import os from subprocess import Popen, PIPE, STDOUT es_subprocess = Popen([&#39;elasticsearch-7.11.1/bin/elasticsearch&#39;], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda : os.setuid(1)) # wait for a few minutes for the local host to start !curl -X GET &quot;localhost:9200/&quot; # install elasticsearch python api !pip install -q elasticsearch . # check if elasticsearch server is properly running in the background from elasticsearch import Elasticsearch, helpers es_client = Elasticsearch([&#39;localhost&#39;]) es_client.info() . {&#39;cluster_name&#39;: &#39;elasticsearch&#39;, &#39;cluster_uuid&#39;: &#39;WQS1QVG8RX6FQ65LS6MyrA&#39;, &#39;name&#39;: &#39;50176241ce38&#39;, &#39;tagline&#39;: &#39;You Know, for Search&#39;, &#39;version&#39;: {&#39;build_date&#39;: &#39;2021-02-15T13:44:09.394032Z&#39;, &#39;build_flavor&#39;: &#39;default&#39;, &#39;build_hash&#39;: &#39;ff17057114c2199c9c1bbecc727003a907c0db7a&#39;, &#39;build_snapshot&#39;: False, &#39;build_type&#39;: &#39;tar&#39;, &#39;lucene_version&#39;: &#39;8.7.0&#39;, &#39;minimum_index_compatibility_version&#39;: &#39;6.0.0-beta1&#39;, &#39;minimum_wire_compatibility_version&#39;: &#39;6.8.0&#39;, &#39;number&#39;: &#39;7.11.1&#39;}} . . Part 2 - Walking through an embedding-based retrieval system . Download MovieLens dataset . !wget https://files.grouplens.org/datasets/movielens/ml-25m.zip --no-check-certificate !unzip ml-25m.zip . Archive: ml-25m.zip creating: ml-25m/ inflating: ml-25m/tags.csv inflating: ml-25m/links.csv inflating: ml-25m/README.txt inflating: ml-25m/ratings.csv inflating: ml-25m/genome-tags.csv inflating: ml-25m/genome-scores.csv inflating: ml-25m/movies.csv . import pandas as pd data = pd.read_csv(&#39;ml-25m/movies.csv&#39;).drop_duplicates() data.head() . movieId title genres . 0 1 | Toy Story (1995) | Adventure|Animation|Children|Comedy|Fantasy | . 1 2 | Jumanji (1995) | Adventure|Children|Fantasy | . 2 3 | Grumpier Old Men (1995) | Comedy|Romance | . 3 4 | Waiting to Exhale (1995) | Comedy|Drama|Romance | . 4 5 | Father of the Bride Part II (1995) | Comedy | . Build index with document vectors . import tensorflow_hub as hub from timeit import default_timer as timer import json . embed = hub.load(&quot;https://tfhub.dev/google/universal-sentence-encoder-large/5&quot;) . INDEX_NAME = &quot;movie_title&quot; BATCH_SIZE = 200 SEARCH_SIZE = 10 MAPPINGS = { &#39;mappings&#39;: {&#39;_source&#39;: {&#39;enabled&#39;: &#39;true&#39;}, &#39;dynamic&#39;: &#39;true&#39;, &#39;properties&#39;: {&#39;title_vector&#39;: {&#39;dims&#39;: 512, &#39;type&#39;: &#39;dense_vector&#39;}, &#39;movie_id&#39;: {&#39;type&#39;: &#39;keyword&#39;}, &#39;genres&#39;: {&#39;type&#39;: &#39;keyword&#39;} } }, &#39;settings&#39;: {&#39;number_of_replicas&#39;: 1, &#39;number_of_shards&#39;:2} } . Ref - https://youtu.be/F4D08uU3mPA . index_movie_lens(data, num_doc=2000) . creating the movie_title index. Indexed 400 documents in 27.59 seconds. Indexed 800 documents in 48.96 seconds. Indexed 1200 documents in 70.18 seconds. Indexed 1600 documents in 90.92 seconds. Indexed 2000 documents in 111.85 seconds. Done indexing 2000 documents in 111.85 seconds . Search with query vector . return_top_movies(&quot;war&quot;) . 2000 total hits. id: 335, score: 0.5282537 {&#39;genres&#39;: &#39;Adventure|Drama|War&#39;, &#39;title&#39;: &#39;War, The (1994)&#39;} id: 712, score: 0.43743240000000005 {&#39;genres&#39;: &#39;Documentary&#39;, &#39;title&#39;: &#39;War Stories (1995)&#39;} id: 1493, score: 0.3954858000000001 {&#39;genres&#39;: &#39;Drama&#39;, &#39;title&#39;: &#39;War at Home, The (1996)&#39;} id: 1362, score: 0.32700850000000004 {&#39;genres&#39;: &#39;Romance|War&#39;, &#39;title&#39;: &#39;In Love and War (1996)&#39;} id: 550, score: 0.3104720000000001 {&#39;genres&#39;: &#39;Documentary&#39;, &#39;title&#39;: &#39;War Room, The (1993)&#39;} id: 1828, score: 0.30568780000000007 {&#39;genres&#39;: &#39;Action|Romance|Sci-Fi|Thriller&#39;, &#39;title&#39;: &#39;Armageddon (1998)&#39;} id: 1932, score: 0.3055576 {&#39;genres&#39;: &#39;Adventure|Sci-Fi&#39;, &#39;title&#39;: &#39;Dune (1984)&#39;} id: 1265, score: 0.2961224 {&#39;genres&#39;: &#39;Drama|War&#39;, &#39;title&#39;: &#39;Killing Fields, The (1984)&#39;} id: 1063, score: 0.2951368999999999 {&#39;genres&#39;: &#39;Drama|War&#39;, &#39;title&#39;: &#39;Platoon (1986)&#39;} id: 1676, score: 0.2776048 {&#39;genres&#39;: &#39;Comedy&#39;, &#39;title&#39;: &#39;Senseless (1998)&#39;} . Part 3 - Approximate Nearest Neighbor (ANN) Algorithms . . !pip install faiss !pip install nmslib !apt-get install libomp-dev import faiss import nmslib . documents = data[&#39;title&#39;].to_list()[:2000] # # OOM for large document size embeddings = embed(documents).numpy() embeddings.shape . (2000, 512) . class DemoIndexLSH(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, num_bits=8): self.index = faiss.IndexLSH(self.dimension, num_bits) self.index.add(self.embeddings) def query(self, input_embedding, k=5): distances, indices = self.index.search(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances[0], indices[0])] index_lsh = DemoIndexLSH(512, documents, embeddings) index_lsh.build(num_bits=16) . class DemoIndexIVFPQ(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, number_of_partition=2, number_of_subquantizers=2, subvector_bits=4): quantizer = faiss.IndexFlatL2(self.dimension) self.index = faiss.IndexIVFPQ(quantizer, self.dimension, number_of_partition, number_of_subquantizers, subvector_bits) self.index.train(self.embeddings) self.index.add(self.embeddings) def query(self, input_embedding, k=5): distances, indices = self.index.search(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances[0], indices[0])] index_pq = DemoIndexIVFPQ(512, documents, embeddings) index_pq.build() . class DemoHNSW(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, num_bits=8): self.index = nmslib.init(method=&#39;hnsw&#39;, space=&#39;cosinesimil&#39;) self.index.addDataPointBatch(self.embeddings) self.index.createIndex({&#39;post&#39;: 2}, print_progress=True) def query(self, input_embedding, k=5): indices, distances = self.index.knnQuery(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances, indices)] index_hnsw = DemoHNSW(512, documents, embeddings) index_hnsw.build() . class DemoIndexFlatL2(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, num_bits=8): self.index = faiss.IndexFlatL2(self.dimension) self.index.add(self.embeddings) def query(self, input_embedding, k=5): distances, indices = self.index.search(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances[0], indices[0])] index_flat = DemoIndexFlatL2(512, documents, embeddings) index_flat.build() . def return_ann_top_movies(ann_index, query, k=SEARCH_SIZE): query_vector = embed([query]).numpy() search_start = timer() top_docs = ann_index.query(query_vector, k) search_time = timer() - search_start print(&quot;search time: {:.2f} ms&quot;.format(search_time * 1000)) return top_docs . return_ann_top_movies(index_flat, &quot;romance&quot;) . search time: 0.82 ms . [(0.95573366, &#39;True Romance (1993)&#39;), (1.2160163, &#39;Love Serenade (1996)&#39;), (1.2626679, &#39;Love Affair (1994)&#39;), (1.3447753, &#39;Kissed (1996)&#39;), (1.3752131, &#39;In Love and War (1996)&#39;), (1.3804029, &#39;Casablanca (1942)&#39;), (1.3832319, &#39;Flirt (1995)&#39;), (1.38626, &#39;Moonlight and Valentino (1995)&#39;), (1.3862813, &#39;Hotel de Love (1996)&#39;), (1.3907104, &#39;Intimate Relations (1996)&#39;)] . return_ann_top_movies(index_lsh, &quot;romance&quot;) . search time: 0.56 ms . [(2.0, &#39;Visitors, The (Visiteurs, Les) (1993)&#39;), (2.0, &#39;City Hall (1996)&#39;), (2.0, &#39;Paradise Road (1997)&#39;), (3.0, &#39;When a Man Loves a Woman (1994)&#39;), (3.0, &#39;Cosi (1996)&#39;), (3.0, &#39;Haunted World of Edward D. Wood Jr., The (1996)&#39;), (3.0, &#39;Eddie (1996)&#39;), (3.0, &#39;Ransom (1996)&#39;), (3.0, &#39;Time to Kill, A (1996)&#39;), (3.0, &#39;Mirage (1995)&#39;)] . return_ann_top_movies(index_pq, &quot;romance&quot;) . search time: 0.19 ms . [(1.07124, &#39;Streetcar Named Desire, A (1951)&#39;), (1.07124, &#39;Moonlight Murder (1936)&#39;), (1.0847104, &#39;To Kill a Mockingbird (1962)&#39;), (1.0847104, &#39;Meet John Doe (1941)&#39;), (1.0867723, &#39;Moonlight and Valentino (1995)&#39;), (1.0901785, &#39;Laura (1944)&#39;), (1.0901785, &#39;Rebecca (1940)&#39;), (1.0901785, &#39;African Queen, The (1951)&#39;), (1.0901785, &#39;Gigi (1958)&#39;), (1.0901785, &#39;Scarlet Letter, The (1926)&#39;)] . return_ann_top_movies(index_hnsw, &quot;romance&quot;) . search time: 0.29 ms . [(0.47786665, &#39;True Romance (1993)&#39;), (0.6080081, &#39;Love Serenade (1996)&#39;), (0.63133395, &#39;Love Affair (1994)&#39;), (0.6723877, &#39;Kissed (1996)&#39;), (0.6876065, &#39;In Love and War (1996)&#39;), (0.6916158, &#39;Flirt (1995)&#39;), (0.69312984, &#39;Moonlight and Valentino (1995)&#39;), (0.69314075, &#39;Hotel de Love (1996)&#39;), (0.69535506, &#39;Intimate Relations (1996)&#39;), (0.6985383, &#39;Love in Bloom (1935)&#39;)] .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/large-scale%20elasticsearch/2020/04/20/dl-retrieval.html",
            "relUrl": "/large-scale%20elasticsearch/2020/04/20/dl-retrieval.html",
            "date": " • Apr 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sparsh-ai.github.io/rec-tutorials/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sparsh-ai.github.io/rec-tutorials/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}