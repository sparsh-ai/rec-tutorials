{
  
    
        "post0": {
            "title": "Similar Product Recommender system using Deep Learning for an online e-commerce store",
            "content": ". Import libraries required for file operations . import os import pickle from glob import glob # import basic numerical libraries import numpy as np import pandas as pd # import keras libraries for image recognition from keras.applications import VGG16 from keras.applications.vgg16 import preprocess_input from keras.preprocessing import image as kimage . Data preparation . # download and unzip shirts folder from the directory !wget https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/shirts.zip !unzip shirts.zip . shirts_dict = dict() for shirt in glob(&#39;shirts/*.jpg&#39;): # load all shirts img = kimage.load_img(shirt, target_size=(224, 224)) # VGG accepts images in 224 X 224 pixels img = preprocess_input(np.expand_dims(kimage.img_to_array(img), axis=0)) # so some preprocessing id = shirt.split(&#39;/&#39;)[-1].split(&#39;.&#39;)[0] shirts_dict[id] = img # map image &amp; shirt id . Number of shirts = 2908 . . Model training . model = VGG16(include_top=False, weights=&#39;imagenet&#39;) shirts_matrix = np.zeros([no_of_shirts, 25088]) # initialize the matrix with zeros for i, (id, img) in enumerate(shirts_dict.items()): shirts_matrix[i, :] = model.predict(img).ravel() # flatten the matrix . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 58892288/58889256 [==============================] - 0s 0us/step . model.summary() . Model: &#34;vgg16&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, None, None, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, None, None, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, None, None, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, None, None, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, None, None, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, None, None, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, None, None, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, None, None, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, None, None, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, None, None, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, None, None, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, None, None, 512) 0 ================================================================= Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________ . . Inference pipeline . matrix_id_to_shirt_id = dict() shirt_id_to_matrix_id = dict() for i, (id, img) in enumerate(shirts_dict.items()): matrix_id_to_shirt_id[i] = id shirt_id_to_matrix_id[id] = i . . Finding top 10 similar shirts . Display the sample shirt . from IPython.display import Image Image(&#39;shirts/1015.jpg&#39;) . Print images of top-10 similar shirts . import glob import matplotlib.pyplot as plt import matplotlib.image as mpimg %matplotlib inline images = [] for shirt in closest_shirts: shirt = &#39;shirts/&#39;+shirt+&#39;.jpg&#39; for img_path in glob.glob(shirt): images.append(mpimg.imread(img_path)) plt.figure(figsize=(20,10)) columns = 5 for i, image in enumerate(images): plt.subplot(len(images) / columns + 1, columns, i + 1) plt.imshow(image) . Model persistence . from sklearn.externals import joblib joblib.dump(similarity, &#39;similarity.pkl&#39;) joblib.dump(shirt_id_to_matrix_id, &#39;shirt_id_to_matrix_id.pkl&#39;) joblib.dump(matrix_id_to_shirt_id, &#39;matrix_id_to_shirt_id.pkl&#39;) . loaded_model = joblib.load(&#39;similarity.pkl&#39;) . closest_ids = np.argsort(loaded_model[target_id, :])[::-1][0:10] closest_shirts = [matrix_id_to_shirt_id[matrix_id] for matrix_id in closest_ids] closest_shirts . [&#39;1015&#39;, &#39;1308&#39;, &#39;1187&#39;, &#39;2554&#39;, &#39;2420&#39;, &#39;2526&#39;, &#39;1174&#39;, &#39;2197&#39;, &#39;2545&#39;, &#39;1290&#39;] . . Credits .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/similarity%20vision%20retail/2021/04/23/similar-product-recommender.html",
            "relUrl": "/similarity%20vision%20retail/2021/04/23/similar-product-recommender.html",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Simulating a news personalization scenario using Contextual Bandits",
            "content": "In the Contextual Bandit(CB) introduction tutorial, we learnt about CB and different CB algorithms. In this tutorial we will simulate the scenario of personalizing news content on a site, using CB, to users. The goal is to maximize user engagement quantified by measuring click through rate (CTR). . Let&#39;s recall that in a CB setting, a data point has four components, . Context | Action | Probability of choosing action | Reward/cost for chosen action | . In our simulator, we will need to generate a context, get an action/decision for the given context and also simulate generating a reward. . In our simulator, our goal is to maximize reward (click through rate/CTR) or minimize loss (-CTR) . We have two website visitors: &#39;Tom&#39; and &#39;Anna&#39; | Each of them may visit the website either in the morning or in the afternoon | . The context is therefore (user, time_of_day) . We have the option of recommending a variety of articles to Tom and Anna. Therefore, actions are the different choices of articles: &quot;politics&quot;, &quot;sports&quot;, &quot;music&quot;, &quot;food&quot;, &quot;finance&quot;, &quot;health&quot;, &quot;cheese&quot; . The reward is whether they click on the article or not: &#39;click&#39; or &#39;no click&#39; . Let&#39;s first start with importing the necessary packages: . Simulate reward . In the real world, we will have to learn Tom and Anna&#39;s preferences for articles as we observe their interactions. Since this is a simulation, we will have to define Tom and Anna&#39;s preference profile. The reward that we provide to the learner will follow this preference profile. Our hope is to see if the learner can take better and better decisions as we see more samples which in turn means we are maximizing the reward. . We will also modify the reward function in a few different ways and see if the CB learner picks up the changes. We will compare the CTR with and without learning. . VW optimizes to minimize cost which is negative of reward. Therefore, we will always pass negative of reward as cost to VW. . USER_LIKED_ARTICLE = -1.0 USER_DISLIKED_ARTICLE = 0.0 . The reward function below specifies that Tom likes politics in the morning and music in the afternoon whereas Anna likes sports in the morning and politics in the afternoon. It looks dense but we are just simulating our hypothetical world in the format of the feedback the learner understands: cost. If the learner recommends an article that aligns with the reward function, we give a positive reward. In our simulated world this is a click. . def get_cost(context,action): if context[&#39;user&#39;] == &quot;Tom&quot;: if context[&#39;time_of_day&#39;] == &quot;morning&quot; and action == &#39;politics&#39;: return USER_LIKED_ARTICLE elif context[&#39;time_of_day&#39;] == &quot;afternoon&quot; and action == &#39;music&#39;: return USER_LIKED_ARTICLE else: return USER_DISLIKED_ARTICLE elif context[&#39;user&#39;] == &quot;Anna&quot;: if context[&#39;time_of_day&#39;] == &quot;morning&quot; and action == &#39;sports&#39;: return USER_LIKED_ARTICLE elif context[&#39;time_of_day&#39;] == &quot;afternoon&quot; and action == &#39;politics&#39;: return USER_LIKED_ARTICLE else: return USER_DISLIKED_ARTICLE . Understanding VW format . There are some things we need to do to get our input into a format VW understands. This function handles converting from our context as a dictionary, list of articles and the cost if there is one into the text format VW understands. . def to_vw_example_format(context, actions, cb_label = None): if cb_label is not None: chosen_action, cost, prob = cb_label example_string = &quot;&quot; example_string += &quot;shared |User user={} time_of_day={} n&quot;.format(context[&quot;user&quot;], context[&quot;time_of_day&quot;]) for action in actions: if cb_label is not None and action == chosen_action: example_string += &quot;0:{}:{} &quot;.format(cost, prob) example_string += &quot;|Action article={} n&quot;.format(action) #Strip the last newline return example_string[:-1] . To understand what&#39;s going on here let&#39;s go through an example. Here, it&#39;s the morning and the user is Tom. There are four possible articles. So in the VW format there is one line that starts with shared, this is the shared context, followed by four lines each corresponding to an article. . context = {&quot;user&quot;:&quot;Tom&quot;,&quot;time_of_day&quot;:&quot;morning&quot;} actions = [&quot;politics&quot;, &quot;sports&quot;, &quot;music&quot;, &quot;food&quot;] print(to_vw_example_format(context,actions)) . shared |User user=Tom time_of_day=morning |Action article=politics |Action article=sports |Action article=music |Action article=food . Getting a decision . When we call VW we get a pmf, probability mass function, as the output. Since we are incorporating exploration into our strategy, VW will give us a list of probabilities over the set of actions. This means that the probability at a given index in the list corresponds to the likelihood of picking that specific action. In order to arrive at a decision/action, we will have to sample from this list. . So, given a list [0.7, 0.1, 0.1, 0.1], we would choose the first item with a 70% chance. sample_custom_pmf takes such a list and gives us the index it chose and what the probability of choosing that index was. . def sample_custom_pmf(pmf): total = sum(pmf) scale = 1/total pmf = [x * scale for x in pmf] draw = random.random() sum_prob = 0.0 for index, prob in enumerate(pmf): sum_prob += prob if(sum_prob &gt; draw): return index, prob . We have all of the information we need to choose an action for a specific user and context. To use VW to achieve this, we will do the following: . We convert our context and actions into the text format we need | We pass this example to vw and get the pmf out | Now, we sample this pmf to get what article we will end up showing | Finally we return the article chosen, and the probability of choosing it (we are going to need the probability when we learn form this example) | def get_action(vw, context, actions): vw_text_example = to_vw_example_format(context,actions) pmf = vw.predict(vw_text_example) chosen_action_index, prob = sample_custom_pmf(pmf) return actions[chosen_action_index], prob . Simulation set up . Now that we have done all of the setup work and know how to interface with VW, let&#39;s simulate the world of Tom and Anna. The scenario is they go to a website and are shown an article. Remember that the reward function allows us to define the worlds reaction to what VW recommends. . We will choose between Tom and Anna uniformly at random and also choose their time of visit uniformly at random. You can think of this as us tossing a coin to choose between Tom and Anna (Anna if heads and Tom if tails) and another coin toss for choosing time of day. . actions camping finance food health music politics sports . users times_of_day . Anna afternoon 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | -1.0 | 0.0 | . morning 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | -1.0 | . Tom afternoon 0.0 | 0.0 | 0.0 | 0.0 | -1.0 | 0.0 | 0.0 | . morning 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | -1.0 | 0.0 | . We will instantiate a CB learner in VW and then simulate Tom and Anna&#39;s website visits num_iterations number of times. In each visit, we: . Decide between Tom and Anna | Decide time of day | Pass context i.e. (user, time of day) to learner to get action i.e. article recommendation and probability of choosing action | Receive reward i.e. see if user clicked or not. Remember that cost is just negative reward. | Format context, action, probability, reward in VW format | Learn from the example VW reduces a CB problem to a cost sensitive multiclass classification problem. | . | This is the same for every one of our simulations, so we define the process in the run_simulation function. The cost function must be supplied as this is essentially us simulating how the world works. . def run_simulation(vw, num_iterations, users, times_of_day, actions, cost_function, do_learn = True): cost_sum = 0. ctr = [] for i in range(1, num_iterations+1): # 1. In each simulation choose a user user = choose_user(users) # 2. Choose time of day for a given user time_of_day = choose_time_of_day(times_of_day) # 3. Pass context to vw to get an action context = {&#39;user&#39;: user, &#39;time_of_day&#39;: time_of_day} action, prob = get_action(vw, context, actions) # 4. Get cost of the action we chose cost = cost_function(context, action) cost_sum += cost if do_learn: # 5. Inform VW of what happened so we can learn from it vw_format = vw.parse(to_vw_example_format(context, actions, (action, cost, prob)),pyvw.vw.lContextualBandit) # 6. Learn vw.learn(vw_format) # We negate this so that on the plot instead of minimizing cost, we are maximizing reward ctr.append(-1*cost_sum/i) return ctr . We want to be able to visualize what is occurring, so we are going to plot the click through rate over each iteration of the simulation. If VW is showing actions the get rewards the ctr will be higher. Below is a little utility function to make showing the plot easier. . Scenario 1 . We will use the first reward function get_cost and assume that Tom and Anna do not change their preferences over time and see what happens to user engagement as we learn. We will also see what happens when there is no learning. We will use the &quot;no learning&quot; case as our baseline to compare to. . With learning . vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations = 5000 ctr = run_simulation(vw, num_iterations, users, times_of_day, actions, get_cost) plot_ctr(num_iterations, ctr) . Aside: interactions . You&#39;ll notice in the arguments we supply to VW, we include -q UA. This is telling VW to create additional features which are the features in the (U)ser namespace and (A)ction namespaces multiplied together. This allows us to learn the interaction between when certain actions are good in certain times of days and for particular users. If we didn&#39;t do that, the learning wouldn&#39;t really work. We can see that in action below. . vw = pyvw.vw(&quot;--cb_explore_adf --quiet --epsilon 0.2&quot;) num_iterations = 5000 ctr = run_simulation(vw, num_iterations, users, times_of_day, actions, get_cost) plot_ctr(num_iterations, ctr) . Without learning . Let&#39;s do the same thing again (but with -q, but this time show the effect if we don&#39;t learn from what happens. The ctr never improves are we just hover around 0.2. . vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations = 5000 ctr = run_simulation(vw, num_iterations, users, times_of_day, actions, get_cost, do_learn=False) plot_ctr(num_iterations, ctr) . Scenario 2 . In the real world people&#39;s preferences change over time. So now in the simulation we are going to incorporate two different cost functions, and swap over to the second one halfway through. Below is a a table of the new reward function we are going to use, get_cost_1: . Tom . get_cost get_cost_new1 . Morning | Politics | Politics | . Afternoon | Music | Sports | . Anna . get_cost get_cost_new1 . Morning | Sports | Sports | . Afternoon | Politics | Sports | . This reward function is still working with actions that the learner has seen previously. . To make it easy to show the effect of the cost function changing we are going to modify the run_simulation function. It is a little less readable now, but it supports accepting a list of cost functions and it will operate over each cost function in turn. This is perfect for what we need. . With learning . Let us now switch to the second reward function after a few samples (running the first reward function). Recall that this reward function changes the preferences of the web users but it is still working with the same action space as before. We should see the learner pick up these changes and optimize towards the new preferences. . # Instantiate learner in VW vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations_per_cost_func = 5000 cost_functions = [get_cost, get_cost_new1] total_iterations = num_iterations_per_cost_func * len(cost_functions) ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions) plot_ctr(total_iterations, ctr) . Note: The initial spike in CTR depends on the rewards received for the first few examples. When you run on your own, you may see something different initially because our simulator is designed to have randomness. . Without learning . # use first reward function initially and then switch to second reward function # Instantiate learner in VW vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations_per_cost_func = 5000 cost_functions = [get_cost, get_cost_new1] total_iterations = num_iterations_per_cost_func * len(cost_functions) ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions, do_learn=False) plot_ctr(total_iterations, ctr) . Scenario 3 . In this scenario we are going to start rewarding actions that have never seen a reward previously when we change the cost function. . Tom . get_cost get_cost_new2 . Morning | Politics | Politics | . Afternoon | Music | Food | . Anna . get_cost get_cost_new2 . Morning | Sports | Food | . Afternoon | Politics | Food | . def get_cost_new2(context,action): if context[&#39;user&#39;] == &quot;Tom&quot;: if context[&#39;time_of_day&#39;] == &quot;morning&quot; and action == &#39;politics&#39;: return USER_LIKED_ARTICLE elif context[&#39;time_of_day&#39;] == &quot;afternoon&quot; and action == &#39;food&#39;: return USER_LIKED_ARTICLE else: return USER_DISLIKED_ARTICLE elif context[&#39;user&#39;] == &quot;Anna&quot;: if context[&#39;time_of_day&#39;] == &quot;morning&quot; and action == &#39;food&#39;: return USER_LIKED_ARTICLE elif context[&#39;time_of_day&#39;] == &quot;afternoon&quot; and action == &#39;food&#39;: return USER_LIKED_ARTICLE else: return USER_DISLIKED_ARTICLE . With learning . Let us now switch to the third reward function after a few samples (running the first reward function). Recall that this reward function changes the preferences of the users and is working with a different action space than before. We should see the learner pick up these changes and optimize towards the new preferences . # Instantiate learner in VW vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations_per_cost_func = 5000 cost_functions = [get_cost, get_cost_new2] total_iterations = num_iterations_per_cost_func * len(cost_functions) ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions) plot_ctr(total_iterations, ctr) . Without Learning . # use first reward function initially and then switch to third reward function # Instantiate learner in VW vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations_per_cost_func = 5000 cost_functions = [get_cost, get_cost_new2] total_iterations = num_iterations_per_cost_func * len(cost_functions) ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions, do_learn=False) plot_ctr(total_iterations, ctr) . Summary . This tutorial aimed at showcasing a real world scenario where contextual bandit algorithms can be used. We were able to take a context and set of actions and learn what actions worked best for a given context. We saw that the learner was able to respond rapidly to changes in the world. We showed that allowing the learner to interact with the world resulted in higher rewards than the no learning baseline. . This tutorial worked with simplistic features. VW supports high dimensional sparse features, different exploration algorithms and policy evaluation approaches. . credits .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/reinforcement%20contextual/2021/04/22/vowpal-wabbit-contextual-recommender.html",
            "relUrl": "/reinforcement%20contextual/2021/04/22/vowpal-wabbit-contextual-recommender.html",
            "date": " • Apr 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Neural Matrix Factorization from scratch in PyTorch",
            "content": "!pip install -q tensorboardX . |████████████████████████████████| 122kB 8.3MB/s . import os import time import random import argparse import numpy as np import pandas as pd import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as data from tensorboardX import SummaryWriter . Downloading Movielens-1M Ratings . DATA_URL = &quot;https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-1m-dat/ratings.dat&quot; MAIN_PATH = &#39;/content/&#39; DATA_PATH = MAIN_PATH + &#39;ratings.dat&#39; MODEL_PATH = MAIN_PATH + &#39;models/&#39; MODEL = &#39;ml-1m_Neu_MF&#39; . !wget -nc https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-1m-dat/ratings.dat . Defining Dataset Classes . class Rating_Datset(torch.utils.data.Dataset): def __init__(self, user_list, item_list, rating_list): super(Rating_Datset, self).__init__() self.user_list = user_list self.item_list = item_list self.rating_list = rating_list def __len__(self): return len(self.user_list) def __getitem__(self, idx): user = self.user_list[idx] item = self.item_list[idx] rating = self.rating_list[idx] return ( torch.tensor(user, dtype=torch.long), torch.tensor(item, dtype=torch.long), torch.tensor(rating, dtype=torch.float) ) . NCF Dataset Class . _reindex: process dataset to reindex userID and itemID, also set rating as binary feedback | _leave_one_out: leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf | negative_sampling: randomly selects n negative examples for each positive one | . class NCF_Data(object): &quot;&quot;&quot; Construct Dataset for NCF &quot;&quot;&quot; def __init__(self, args, ratings): self.ratings = ratings self.num_ng = args.num_ng self.num_ng_test = args.num_ng_test self.batch_size = args.batch_size self.preprocess_ratings = self._reindex(self.ratings) self.user_pool = set(self.ratings[&#39;user_id&#39;].unique()) self.item_pool = set(self.ratings[&#39;item_id&#39;].unique()) self.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings) self.negatives = self._negative_sampling(self.preprocess_ratings) random.seed(args.seed) def _reindex(self, ratings): &quot;&quot;&quot; Process dataset to reindex userID and itemID, also set rating as binary feedback &quot;&quot;&quot; user_list = list(ratings[&#39;user_id&#39;].drop_duplicates()) user2id = {w: i for i, w in enumerate(user_list)} item_list = list(ratings[&#39;item_id&#39;].drop_duplicates()) item2id = {w: i for i, w in enumerate(item_list)} ratings[&#39;user_id&#39;] = ratings[&#39;user_id&#39;].apply(lambda x: user2id[x]) ratings[&#39;item_id&#39;] = ratings[&#39;item_id&#39;].apply(lambda x: item2id[x]) ratings[&#39;rating&#39;] = ratings[&#39;rating&#39;].apply(lambda x: float(x &gt; 0)) return ratings def _leave_one_out(self, ratings): &quot;&quot;&quot; leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf &quot;&quot;&quot; ratings[&#39;rank_latest&#39;] = ratings.groupby([&#39;user_id&#39;])[&#39;timestamp&#39;].rank(method=&#39;first&#39;, ascending=False) test = ratings.loc[ratings[&#39;rank_latest&#39;] == 1] train = ratings.loc[ratings[&#39;rank_latest&#39;] &gt; 1] assert train[&#39;user_id&#39;].nunique()==test[&#39;user_id&#39;].nunique(), &#39;Not Match Train User with Test User&#39; return train[[&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;]], test[[&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;]] def _negative_sampling(self, ratings): interact_status = ( ratings.groupby(&#39;user_id&#39;)[&#39;item_id&#39;] .apply(set) .reset_index() .rename(columns={&#39;item_id&#39;: &#39;interacted_items&#39;})) interact_status[&#39;negative_items&#39;] = interact_status[&#39;interacted_items&#39;].apply(lambda x: self.item_pool - x) interact_status[&#39;negative_samples&#39;] = interact_status[&#39;negative_items&#39;].apply(lambda x: random.sample(x, self.num_ng_test)) return interact_status[[&#39;user_id&#39;, &#39;negative_items&#39;, &#39;negative_samples&#39;]] def get_train_instance(self): users, items, ratings = [], [], [] train_ratings = pd.merge(self.train_ratings, self.negatives[[&#39;user_id&#39;, &#39;negative_items&#39;]], on=&#39;user_id&#39;) train_ratings[&#39;negatives&#39;] = train_ratings[&#39;negative_items&#39;].apply(lambda x: random.sample(x, self.num_ng)) for row in train_ratings.itertuples(): users.append(int(row.user_id)) items.append(int(row.item_id)) ratings.append(float(row.rating)) for i in range(self.num_ng): users.append(int(row.user_id)) items.append(int(row.negatives[i])) ratings.append(float(0)) # negative samples get 0 rating dataset = Rating_Datset( user_list=users, item_list=items, rating_list=ratings) return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4) def get_test_instance(self): users, items, ratings = [], [], [] test_ratings = pd.merge(self.test_ratings, self.negatives[[&#39;user_id&#39;, &#39;negative_samples&#39;]], on=&#39;user_id&#39;) for row in test_ratings.itertuples(): users.append(int(row.user_id)) items.append(int(row.item_id)) ratings.append(float(row.rating)) for i in getattr(row, &#39;negative_samples&#39;): users.append(int(row.user_id)) items.append(int(i)) ratings.append(float(0)) dataset = Rating_Datset( user_list=users, item_list=items, rating_list=ratings) return torch.utils.data.DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=2) . Defining Metrics . Using Hit Rate and NDCG as our evaluation metrics . def hit(ng_item, pred_items): if ng_item in pred_items: return 1 return 0 def ndcg(ng_item, pred_items): if ng_item in pred_items: index = pred_items.index(ng_item) return np.reciprocal(np.log2(index+2)) return 0 def metrics(model, test_loader, top_k, device): HR, NDCG = [], [] for user, item, label in test_loader: user = user.to(device) item = item.to(device) predictions = model(user, item) _, indices = torch.topk(predictions, top_k) recommends = torch.take( item, indices).cpu().numpy().tolist() ng_item = item[0].item() # leave one-out evaluation has only one item per user HR.append(hit(ng_item, recommends)) NDCG.append(ndcg(ng_item, recommends)) return np.mean(HR), np.mean(NDCG) . Defining Model Architectures . Generalized Matrix Factorization | Multi Layer Perceptron | Neural Matrix Factorization | class Generalized_Matrix_Factorization(nn.Module): def __init__(self, args, num_users, num_items): super(Generalized_Matrix_Factorization, self).__init__() self.num_users = num_users self.num_items = num_items self.factor_num = args.factor_num self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num) self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num) self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1) self.logistic = nn.Sigmoid() def forward(self, user_indices, item_indices): user_embedding = self.embedding_user(user_indices) item_embedding = self.embedding_item(item_indices) element_product = torch.mul(user_embedding, item_embedding) logits = self.affine_output(element_product) rating = self.logistic(logits) return rating def init_weight(self): pass . class Multi_Layer_Perceptron(nn.Module): def __init__(self, args, num_users, num_items): super(Multi_Layer_Perceptron, self).__init__() self.num_users = num_users self.num_items = num_items self.factor_num = args.factor_num self.layers = args.layers self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num) self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num) self.fc_layers = nn.ModuleList() for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])): self.fc_layers.append(nn.Linear(in_size, out_size)) self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1) self.logistic = nn.Sigmoid() def forward(self, user_indices, item_indices): user_embedding = self.embedding_user(user_indices) item_embedding = self.embedding_item(item_indices) vector = torch.cat([user_embedding, item_embedding], dim=-1) # the concat latent vector for idx, _ in enumerate(range(len(self.fc_layers))): vector = self.fc_layers[idx](vector) vector = nn.ReLU()(vector) # vector = nn.BatchNorm1d()(vector) # vector = nn.Dropout(p=0.5)(vector) logits = self.affine_output(vector) rating = self.logistic(logits) return rating def init_weight(self): pass . class NeuMF(nn.Module): def __init__(self, args, num_users, num_items): super(NeuMF, self).__init__() self.num_users = num_users self.num_items = num_items self.factor_num_mf = args.factor_num self.factor_num_mlp = int(args.layers[0]/2) self.layers = args.layers self.dropout = args.dropout self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp) self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp) self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf) self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf) self.fc_layers = nn.ModuleList() for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])): self.fc_layers.append(torch.nn.Linear(in_size, out_size)) self.fc_layers.append(nn.ReLU()) self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_mf, out_features=1) self.logistic = nn.Sigmoid() self.init_weight() def init_weight(self): nn.init.normal_(self.embedding_user_mlp.weight, std=0.01) nn.init.normal_(self.embedding_item_mlp.weight, std=0.01) nn.init.normal_(self.embedding_user_mf.weight, std=0.01) nn.init.normal_(self.embedding_item_mf.weight, std=0.01) for m in self.fc_layers: if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight) nn.init.xavier_uniform_(self.affine_output.weight) for m in self.modules(): if isinstance(m, nn.Linear) and m.bias is not None: m.bias.data.zero_() def forward(self, user_indices, item_indices): user_embedding_mlp = self.embedding_user_mlp(user_indices) item_embedding_mlp = self.embedding_item_mlp(item_indices) user_embedding_mf = self.embedding_user_mf(user_indices) item_embedding_mf = self.embedding_item_mf(item_indices) mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1) mf_vector =torch.mul(user_embedding_mf, item_embedding_mf) for idx, _ in enumerate(range(len(self.fc_layers))): mlp_vector = self.fc_layers[idx](mlp_vector) vector = torch.cat([mlp_vector, mf_vector], dim=-1) logits = self.affine_output(vector) rating = self.logistic(logits) return rating.squeeze() . Setting Arguments . Here is the brief description of important ones: . Learning rate is 0.001 | Dropout rate is 0.2 | Running for 10 epochs | HitRate@10 and NDCG@10 | 4 negative samples for each positive one | . parser = argparse.ArgumentParser() parser.add_argument(&quot;--seed&quot;, type=int, default=42, help=&quot;Seed&quot;) parser.add_argument(&quot;--lr&quot;, type=float, default=0.001, help=&quot;learning rate&quot;) parser.add_argument(&quot;--dropout&quot;, type=float, default=0.2, help=&quot;dropout rate&quot;) parser.add_argument(&quot;--batch_size&quot;, type=int, default=256, help=&quot;batch size for training&quot;) parser.add_argument(&quot;--epochs&quot;, type=int, default=10, help=&quot;training epoches&quot;) parser.add_argument(&quot;--top_k&quot;, type=int, default=10, help=&quot;compute metrics@top_k&quot;) parser.add_argument(&quot;--factor_num&quot;, type=int, default=32, help=&quot;predictive factors numbers in the model&quot;) parser.add_argument(&quot;--layers&quot;, nargs=&#39;+&#39;, default=[64,32,16,8], help=&quot;MLP layers. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.&quot;) parser.add_argument(&quot;--num_ng&quot;, type=int, default=4, help=&quot;Number of negative samples for training set&quot;) parser.add_argument(&quot;--num_ng_test&quot;, type=int, default=100, help=&quot;Number of negative samples for test set&quot;) parser.add_argument(&quot;--out&quot;, default=True, help=&quot;save model or not&quot;) . . _StoreAction(option_strings=[&#39;--out&#39;], dest=&#39;out&#39;, nargs=None, const=None, default=True, type=None, choices=None, help=&#39;save model or not&#39;, metavar=None) . Training NeuMF Model . args = parser.parse_args(&quot;&quot;) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) writer = SummaryWriter() # seed for Reproducibility seed_everything(args.seed) # load data ml_1m = pd.read_csv( DATA_PATH, sep=&quot;::&quot;, names = [&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;, &#39;timestamp&#39;], engine=&#39;python&#39;) # set the num_users, items num_users = ml_1m[&#39;user_id&#39;].nunique()+1 num_items = ml_1m[&#39;item_id&#39;].nunique()+1 # construct the train and test datasets data = NCF_Data(args, ml_1m) train_loader = data.get_train_instance() test_loader = data.get_test_instance() # set model and loss, optimizer model = NeuMF(args, num_users, num_items) model = model.to(device) loss_function = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=args.lr) # train, evaluation best_hr = 0 for epoch in range(1, args.epochs+1): model.train() # Enable dropout (if have). start_time = time.time() for user, item, label in train_loader: user = user.to(device) item = item.to(device) label = label.to(device) optimizer.zero_grad() prediction = model(user, item) loss = loss_function(prediction, label) loss.backward() optimizer.step() writer.add_scalar(&#39;loss/Train_loss&#39;, loss.item(), epoch) model.eval() HR, NDCG = metrics(model, test_loader, args.top_k, device) writer.add_scalar(&#39;Perfomance/HR@10&#39;, HR, epoch) writer.add_scalar(&#39;Perfomance/NDCG@10&#39;, NDCG, epoch) elapsed_time = time.time() - start_time print(&quot;The time elapse of epoch {:03d}&quot;.format(epoch) + &quot; is: &quot; + time.strftime(&quot;%H: %M: %S&quot;, time.gmtime(elapsed_time))) print(&quot;HR: {:.3f} tNDCG: {:.3f}&quot;.format(np.mean(HR), np.mean(NDCG))) if HR &gt; best_hr: best_hr, best_ndcg, best_epoch = HR, NDCG, epoch if args.out: if not os.path.exists(MODEL_PATH): os.mkdir(MODEL_PATH) torch.save(model, &#39;{}{}.pth&#39;.format(MODEL_PATH, MODEL)) writer.close() . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) . The time elapse of epoch 001 is: 00: 02: 30 HR: 0.624 NDCG: 0.362 The time elapse of epoch 002 is: 00: 02: 31 HR: 0.663 NDCG: 0.392 The time elapse of epoch 003 is: 00: 02: 30 HR: 0.673 NDCG: 0.399 The time elapse of epoch 004 is: 00: 02: 30 HR: 0.677 NDCG: 0.402 The time elapse of epoch 005 is: 00: 02: 31 HR: 0.671 NDCG: 0.399 The time elapse of epoch 006 is: 00: 02: 32 HR: 0.671 NDCG: 0.400 The time elapse of epoch 007 is: 00: 02: 32 HR: 0.669 NDCG: 0.400 The time elapse of epoch 008 is: 00: 02: 31 HR: 0.665 NDCG: 0.395 The time elapse of epoch 009 is: 00: 02: 33 HR: 0.664 NDCG: 0.393 The time elapse of epoch 010 is: 00: 02: 32 HR: 0.667 NDCG: 0.394 . Final Output . print(&quot;Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}&quot;.format( best_epoch, best_hr, best_ndcg)) .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/matrixfactorization%20movielens%20pytorch%20scratch/2021/04/21/rec-algo-ncf-pytorch-pyy0715.html",
            "relUrl": "/matrixfactorization%20movielens%20pytorch%20scratch/2021/04/21/rec-algo-ncf-pytorch-pyy0715.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Large-scale Document Retrieval with ElasticSearch",
            "content": "Retrieval Flow Overview . . Part 1 - Setting up Elasticsearch . Download the elasticsearch archive (linux), setup a local server | Create a client connection to the local elasticsearch instance | . # download the latest elasticsearch version !wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.11.1-linux-x86_64.tar.gz !tar -xzvf elasticsearch-7.11.1-linux-x86_64.tar.gz !chown -R daemon:daemon elasticsearch-7.11.1 # prep the elasticsearch server import os from subprocess import Popen, PIPE, STDOUT es_subprocess = Popen([&#39;elasticsearch-7.11.1/bin/elasticsearch&#39;], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda : os.setuid(1)) # wait for a few minutes for the local host to start !curl -X GET &quot;localhost:9200/&quot; # install elasticsearch python api !pip install -q elasticsearch . # check if elasticsearch server is properly running in the background from elasticsearch import Elasticsearch, helpers es_client = Elasticsearch([&#39;localhost&#39;]) es_client.info() . {&#39;cluster_name&#39;: &#39;elasticsearch&#39;, &#39;cluster_uuid&#39;: &#39;WQS1QVG8RX6FQ65LS6MyrA&#39;, &#39;name&#39;: &#39;50176241ce38&#39;, &#39;tagline&#39;: &#39;You Know, for Search&#39;, &#39;version&#39;: {&#39;build_date&#39;: &#39;2021-02-15T13:44:09.394032Z&#39;, &#39;build_flavor&#39;: &#39;default&#39;, &#39;build_hash&#39;: &#39;ff17057114c2199c9c1bbecc727003a907c0db7a&#39;, &#39;build_snapshot&#39;: False, &#39;build_type&#39;: &#39;tar&#39;, &#39;lucene_version&#39;: &#39;8.7.0&#39;, &#39;minimum_index_compatibility_version&#39;: &#39;6.0.0-beta1&#39;, &#39;minimum_wire_compatibility_version&#39;: &#39;6.8.0&#39;, &#39;number&#39;: &#39;7.11.1&#39;}} . . Part 2 - Walking through an embedding-based retrieval system . Download MovieLens dataset . !wget https://files.grouplens.org/datasets/movielens/ml-25m.zip --no-check-certificate !unzip ml-25m.zip . Archive: ml-25m.zip creating: ml-25m/ inflating: ml-25m/tags.csv inflating: ml-25m/links.csv inflating: ml-25m/README.txt inflating: ml-25m/ratings.csv inflating: ml-25m/genome-tags.csv inflating: ml-25m/genome-scores.csv inflating: ml-25m/movies.csv . import pandas as pd data = pd.read_csv(&#39;ml-25m/movies.csv&#39;).drop_duplicates() data.head() . movieId title genres . 0 1 | Toy Story (1995) | Adventure|Animation|Children|Comedy|Fantasy | . 1 2 | Jumanji (1995) | Adventure|Children|Fantasy | . 2 3 | Grumpier Old Men (1995) | Comedy|Romance | . 3 4 | Waiting to Exhale (1995) | Comedy|Drama|Romance | . 4 5 | Father of the Bride Part II (1995) | Comedy | . Build index with document vectors . import tensorflow_hub as hub from timeit import default_timer as timer import json . embed = hub.load(&quot;https://tfhub.dev/google/universal-sentence-encoder-large/5&quot;) . INDEX_NAME = &quot;movie_title&quot; BATCH_SIZE = 200 SEARCH_SIZE = 10 MAPPINGS = { &#39;mappings&#39;: {&#39;_source&#39;: {&#39;enabled&#39;: &#39;true&#39;}, &#39;dynamic&#39;: &#39;true&#39;, &#39;properties&#39;: {&#39;title_vector&#39;: {&#39;dims&#39;: 512, &#39;type&#39;: &#39;dense_vector&#39;}, &#39;movie_id&#39;: {&#39;type&#39;: &#39;keyword&#39;}, &#39;genres&#39;: {&#39;type&#39;: &#39;keyword&#39;} } }, &#39;settings&#39;: {&#39;number_of_replicas&#39;: 1, &#39;number_of_shards&#39;:2} } . Ref - https://youtu.be/F4D08uU3mPA . index_movie_lens(data, num_doc=2000) . creating the movie_title index. Indexed 400 documents in 27.59 seconds. Indexed 800 documents in 48.96 seconds. Indexed 1200 documents in 70.18 seconds. Indexed 1600 documents in 90.92 seconds. Indexed 2000 documents in 111.85 seconds. Done indexing 2000 documents in 111.85 seconds . Search with query vector . return_top_movies(&quot;war&quot;) . 2000 total hits. id: 335, score: 0.5282537 {&#39;genres&#39;: &#39;Adventure|Drama|War&#39;, &#39;title&#39;: &#39;War, The (1994)&#39;} id: 712, score: 0.43743240000000005 {&#39;genres&#39;: &#39;Documentary&#39;, &#39;title&#39;: &#39;War Stories (1995)&#39;} id: 1493, score: 0.3954858000000001 {&#39;genres&#39;: &#39;Drama&#39;, &#39;title&#39;: &#39;War at Home, The (1996)&#39;} id: 1362, score: 0.32700850000000004 {&#39;genres&#39;: &#39;Romance|War&#39;, &#39;title&#39;: &#39;In Love and War (1996)&#39;} id: 550, score: 0.3104720000000001 {&#39;genres&#39;: &#39;Documentary&#39;, &#39;title&#39;: &#39;War Room, The (1993)&#39;} id: 1828, score: 0.30568780000000007 {&#39;genres&#39;: &#39;Action|Romance|Sci-Fi|Thriller&#39;, &#39;title&#39;: &#39;Armageddon (1998)&#39;} id: 1932, score: 0.3055576 {&#39;genres&#39;: &#39;Adventure|Sci-Fi&#39;, &#39;title&#39;: &#39;Dune (1984)&#39;} id: 1265, score: 0.2961224 {&#39;genres&#39;: &#39;Drama|War&#39;, &#39;title&#39;: &#39;Killing Fields, The (1984)&#39;} id: 1063, score: 0.2951368999999999 {&#39;genres&#39;: &#39;Drama|War&#39;, &#39;title&#39;: &#39;Platoon (1986)&#39;} id: 1676, score: 0.2776048 {&#39;genres&#39;: &#39;Comedy&#39;, &#39;title&#39;: &#39;Senseless (1998)&#39;} . Part 3 - Approximate Nearest Neighbor (ANN) Algorithms . . !pip install faiss !pip install nmslib !apt-get install libomp-dev import faiss import nmslib . documents = data[&#39;title&#39;].to_list()[:2000] # # OOM for large document size embeddings = embed(documents).numpy() embeddings.shape . (2000, 512) . class DemoIndexLSH(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, num_bits=8): self.index = faiss.IndexLSH(self.dimension, num_bits) self.index.add(self.embeddings) def query(self, input_embedding, k=5): distances, indices = self.index.search(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances[0], indices[0])] index_lsh = DemoIndexLSH(512, documents, embeddings) index_lsh.build(num_bits=16) . class DemoIndexIVFPQ(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, number_of_partition=2, number_of_subquantizers=2, subvector_bits=4): quantizer = faiss.IndexFlatL2(self.dimension) self.index = faiss.IndexIVFPQ(quantizer, self.dimension, number_of_partition, number_of_subquantizers, subvector_bits) self.index.train(self.embeddings) self.index.add(self.embeddings) def query(self, input_embedding, k=5): distances, indices = self.index.search(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances[0], indices[0])] index_pq = DemoIndexIVFPQ(512, documents, embeddings) index_pq.build() . class DemoHNSW(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, num_bits=8): self.index = nmslib.init(method=&#39;hnsw&#39;, space=&#39;cosinesimil&#39;) self.index.addDataPointBatch(self.embeddings) self.index.createIndex({&#39;post&#39;: 2}, print_progress=True) def query(self, input_embedding, k=5): indices, distances = self.index.knnQuery(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances, indices)] index_hnsw = DemoHNSW(512, documents, embeddings) index_hnsw.build() . class DemoIndexFlatL2(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, num_bits=8): self.index = faiss.IndexFlatL2(self.dimension) self.index.add(self.embeddings) def query(self, input_embedding, k=5): distances, indices = self.index.search(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances[0], indices[0])] index_flat = DemoIndexFlatL2(512, documents, embeddings) index_flat.build() . def return_ann_top_movies(ann_index, query, k=SEARCH_SIZE): query_vector = embed([query]).numpy() search_start = timer() top_docs = ann_index.query(query_vector, k) search_time = timer() - search_start print(&quot;search time: {:.2f} ms&quot;.format(search_time * 1000)) return top_docs . return_ann_top_movies(index_flat, &quot;romance&quot;) . search time: 0.82 ms . [(0.95573366, &#39;True Romance (1993)&#39;), (1.2160163, &#39;Love Serenade (1996)&#39;), (1.2626679, &#39;Love Affair (1994)&#39;), (1.3447753, &#39;Kissed (1996)&#39;), (1.3752131, &#39;In Love and War (1996)&#39;), (1.3804029, &#39;Casablanca (1942)&#39;), (1.3832319, &#39;Flirt (1995)&#39;), (1.38626, &#39;Moonlight and Valentino (1995)&#39;), (1.3862813, &#39;Hotel de Love (1996)&#39;), (1.3907104, &#39;Intimate Relations (1996)&#39;)] . return_ann_top_movies(index_lsh, &quot;romance&quot;) . search time: 0.56 ms . [(2.0, &#39;Visitors, The (Visiteurs, Les) (1993)&#39;), (2.0, &#39;City Hall (1996)&#39;), (2.0, &#39;Paradise Road (1997)&#39;), (3.0, &#39;When a Man Loves a Woman (1994)&#39;), (3.0, &#39;Cosi (1996)&#39;), (3.0, &#39;Haunted World of Edward D. Wood Jr., The (1996)&#39;), (3.0, &#39;Eddie (1996)&#39;), (3.0, &#39;Ransom (1996)&#39;), (3.0, &#39;Time to Kill, A (1996)&#39;), (3.0, &#39;Mirage (1995)&#39;)] . return_ann_top_movies(index_pq, &quot;romance&quot;) . search time: 0.19 ms . [(1.07124, &#39;Streetcar Named Desire, A (1951)&#39;), (1.07124, &#39;Moonlight Murder (1936)&#39;), (1.0847104, &#39;To Kill a Mockingbird (1962)&#39;), (1.0847104, &#39;Meet John Doe (1941)&#39;), (1.0867723, &#39;Moonlight and Valentino (1995)&#39;), (1.0901785, &#39;Laura (1944)&#39;), (1.0901785, &#39;Rebecca (1940)&#39;), (1.0901785, &#39;African Queen, The (1951)&#39;), (1.0901785, &#39;Gigi (1958)&#39;), (1.0901785, &#39;Scarlet Letter, The (1926)&#39;)] . return_ann_top_movies(index_hnsw, &quot;romance&quot;) . search time: 0.29 ms . [(0.47786665, &#39;True Romance (1993)&#39;), (0.6080081, &#39;Love Serenade (1996)&#39;), (0.63133395, &#39;Love Affair (1994)&#39;), (0.6723877, &#39;Kissed (1996)&#39;), (0.6876065, &#39;In Love and War (1996)&#39;), (0.6916158, &#39;Flirt (1995)&#39;), (0.69312984, &#39;Moonlight and Valentino (1995)&#39;), (0.69314075, &#39;Hotel de Love (1996)&#39;), (0.69535506, &#39;Intimate Relations (1996)&#39;), (0.6985383, &#39;Love in Bloom (1935)&#39;)] .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/large-scale%20elasticsearch/2021/04/20/dl-retrieval.html",
            "relUrl": "/large-scale%20elasticsearch/2021/04/20/dl-retrieval.html",
            "date": " • Apr 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sparsh-ai.github.io/rec-tutorials/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sparsh-ai.github.io/rec-tutorials/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}