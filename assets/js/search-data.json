{
  
    
        "post0": {
            "title": "How I personalized my YouTube recommendation using YT API",
            "content": ". Source: https://pub.towardsai.net/how-i-personalized-my-youtube-recommendation-using-yt-api-d20f6174bdaa . pip install google-api-python-client . from datetime import datetime, timedelta import pandas as pd start_time = datetime(year=2020, month=10, day=1).strftime(&#39;%Y-%m-%dT%H:%M:%SZ&#39;) end_time = datetime(year=2021, month=5, day=11).strftime(&#39;%Y-%m-%dT%H:%M:%SZ&#39;) from apiclient.discovery import build api_key = &#39;AIzaSyCjHWHTmed0fhMZJDRdedQDku5qJv12xkY&#39; # Enter your own API key ‚Äì this one won‚Äôt work youtube = build(&#39;youtube&#39;, &#39;v3&#39;, developerKey=api_key) results = youtube.search().list(q=&quot;Twenty One Pilots&quot;, part=&quot;snippet&quot;, type=&quot;video&quot;, order=&quot;viewCount&quot;,publishedAfter=start_time, publishedBefore=end_time, maxResults=5).execute() . results . {&#39;etag&#39;: &#39;M_SjX0wrTNP-85kVxzdTtkOur6w&#39;, &#39;items&#39;: [{&#39;etag&#39;: &#39;6LzSrCbs_mcbkY8Ybal4xSU94A0&#39;, &#39;id&#39;: {&#39;kind&#39;: &#39;youtube#video&#39;, &#39;videoId&#39;: &#39;3sO-Y1Zbft4&#39;}, &#39;kind&#39;: &#39;youtube#searchResult&#39;, &#39;snippet&#39;: {&#39;channelId&#39;: &#39;UCBQZwaNPFfJ1gZ1fLZpAEGw&#39;, &#39;channelTitle&#39;: &#39;twenty one pilots&#39;, &#39;description&#39;: &#39;Twenty One Pilots - Shy Away (Official Video) Twenty One Pilots official video for &#34;Shy Away&#34; from the forthcoming album Scaled And Icy, available May 21st on ...&#39;, &#39;liveBroadcastContent&#39;: &#39;none&#39;, &#39;publishTime&#39;: &#39;2021-04-07T17:11:05Z&#39;, &#39;publishedAt&#39;: &#39;2021-04-07T17:11:05Z&#39;, &#39;thumbnails&#39;: {&#39;default&#39;: {&#39;height&#39;: 90, &#39;url&#39;: &#39;https://i.ytimg.com/vi/3sO-Y1Zbft4/default.jpg&#39;, &#39;width&#39;: 120}, &#39;high&#39;: {&#39;height&#39;: 360, &#39;url&#39;: &#39;https://i.ytimg.com/vi/3sO-Y1Zbft4/hqdefault.jpg&#39;, &#39;width&#39;: 480}, &#39;medium&#39;: {&#39;height&#39;: 180, &#39;url&#39;: &#39;https://i.ytimg.com/vi/3sO-Y1Zbft4/mqdefault.jpg&#39;, &#39;width&#39;: 320}}, &#39;title&#39;: &#39;Twenty One Pilots - Shy Away (Official Video)&#39;}}, {&#39;etag&#39;: &#39;9YhOTRTgaSDErxxIE5RLEIPzi3o&#39;, &#39;id&#39;: {&#39;kind&#39;: &#39;youtube#video&#39;, &#39;videoId&#39;: &#39;2sBRnnnZyFw&#39;}, &#39;kind&#39;: &#39;youtube#searchResult&#39;, &#39;snippet&#39;: {&#39;channelId&#39;: &#39;UCBQZwaNPFfJ1gZ1fLZpAEGw&#39;, &#39;channelTitle&#39;: &#39;twenty one pilots&#39;, &#39;description&#39;: &#39;Twenty One Pilots official video for &#34;Choker&#34; from the forthcoming album Scaled And Icy, available May 21st on Fueled By Ramen. Twenty One Pilots ...&#39;, &#39;liveBroadcastContent&#39;: &#39;none&#39;, &#39;publishTime&#39;: &#39;2021-04-30T04:00:15Z&#39;, &#39;publishedAt&#39;: &#39;2021-04-30T04:00:15Z&#39;, &#39;thumbnails&#39;: {&#39;default&#39;: {&#39;height&#39;: 90, &#39;url&#39;: &#39;https://i.ytimg.com/vi/2sBRnnnZyFw/default.jpg&#39;, &#39;width&#39;: 120}, &#39;high&#39;: {&#39;height&#39;: 360, &#39;url&#39;: &#39;https://i.ytimg.com/vi/2sBRnnnZyFw/hqdefault.jpg&#39;, &#39;width&#39;: 480}, &#39;medium&#39;: {&#39;height&#39;: 180, &#39;url&#39;: &#39;https://i.ytimg.com/vi/2sBRnnnZyFw/mqdefault.jpg&#39;, &#39;width&#39;: 320}}, &#39;title&#39;: &#39;Twenty One Pilots - Choker (Official Video)&#39;}}, {&#39;etag&#39;: &#39;pZVtx8z5Xm4y0C76koR4V1_b9rg&#39;, &#39;id&#39;: {&#39;kind&#39;: &#39;youtube#video&#39;, &#39;videoId&#39;: &#39;ozXb10fOi2A&#39;}, &#39;kind&#39;: &#39;youtube#searchResult&#39;, &#39;snippet&#39;: {&#39;channelId&#39;: &#39;UCBQZwaNPFfJ1gZ1fLZpAEGw&#39;, &#39;channelTitle&#39;: &#39;twenty one pilots&#39;, &#39;description&#39;: &#39;twenty one pilots - Christmas Saves The Year (Official Audio) official audio for twenty one pilots holiday track ‚ÄúChristmas Saves The Year&#34; - available now on ...&#39;, &#39;liveBroadcastContent&#39;: &#39;none&#39;, &#39;publishTime&#39;: &#39;2020-12-09T02:00:09Z&#39;, &#39;publishedAt&#39;: &#39;2020-12-09T02:00:09Z&#39;, &#39;thumbnails&#39;: {&#39;default&#39;: {&#39;height&#39;: 90, &#39;url&#39;: &#39;https://i.ytimg.com/vi/ozXb10fOi2A/default.jpg&#39;, &#39;width&#39;: 120}, &#39;high&#39;: {&#39;height&#39;: 360, &#39;url&#39;: &#39;https://i.ytimg.com/vi/ozXb10fOi2A/hqdefault.jpg&#39;, &#39;width&#39;: 480}, &#39;medium&#39;: {&#39;height&#39;: 180, &#39;url&#39;: &#39;https://i.ytimg.com/vi/ozXb10fOi2A/mqdefault.jpg&#39;, &#39;width&#39;: 320}}, &#39;title&#39;: &#39;twenty one pilots - Christmas Saves The Year (Official Audio)&#39;}}, {&#39;etag&#39;: &#39;Iej2LWe6Cgn0JYBNDagJgIo66Co&#39;, &#39;id&#39;: {&#39;kind&#39;: &#39;youtube#video&#39;, &#39;videoId&#39;: &#39;IAVy8gI7HOQ&#39;}, &#39;kind&#39;: &#39;youtube#searchResult&#39;, &#39;snippet&#39;: {&#39;channelId&#39;: &#39;UCnX0L9QiftAcWdzeBx31xCw&#39;, &#39;channelTitle&#39;: &#39;Twenty One Pilots - Topic&#39;, &#39;description&#39;: &#39;Provided to YouTube by Fueled By Ramen Shy Away ¬∑ twenty one pilots Shy Away ‚Ñó 2021 Fueled By Ramen LLC Writer: Tyler Joseph Auto-generated by ...&#39;, &#39;liveBroadcastContent&#39;: &#39;none&#39;, &#39;publishTime&#39;: &#39;2021-04-07T17:16:54Z&#39;, &#39;publishedAt&#39;: &#39;2021-04-07T17:16:54Z&#39;, &#39;thumbnails&#39;: {&#39;default&#39;: {&#39;height&#39;: 90, &#39;url&#39;: &#39;https://i.ytimg.com/vi/IAVy8gI7HOQ/default.jpg&#39;, &#39;width&#39;: 120}, &#39;high&#39;: {&#39;height&#39;: 360, &#39;url&#39;: &#39;https://i.ytimg.com/vi/IAVy8gI7HOQ/hqdefault.jpg&#39;, &#39;width&#39;: 480}, &#39;medium&#39;: {&#39;height&#39;: 180, &#39;url&#39;: &#39;https://i.ytimg.com/vi/IAVy8gI7HOQ/mqdefault.jpg&#39;, &#39;width&#39;: 320}}, &#39;title&#39;: &#39;Shy Away&#39;}}, {&#39;etag&#39;: &#39;PqY7uJ2kHbBDzswh7S5NjxZmI1U&#39;, &#39;id&#39;: {&#39;kind&#39;: &#39;youtube#video&#39;, &#39;videoId&#39;: &#39;DpvUuXNiQ3w&#39;}, &#39;kind&#39;: &#39;youtube#searchResult&#39;, &#39;snippet&#39;: {&#39;channelId&#39;: &#39;UCgXHRvdkaJmGtW-h-KnR7TA&#39;, &#39;channelTitle&#39;: &#39;Nightdrives&#39;, &#39;description&#39;: &#39;This is a 1 hour version of the song Hometown by Twenty one Pilots. This the sad part of the song in a slowed version for 1 hour straight. Perfect for when you ...&#39;, &#39;liveBroadcastContent&#39;: &#39;none&#39;, &#39;publishTime&#39;: &#39;2020-10-26T19:02:50Z&#39;, &#39;publishedAt&#39;: &#39;2020-10-26T19:02:50Z&#39;, &#39;thumbnails&#39;: {&#39;default&#39;: {&#39;height&#39;: 90, &#39;url&#39;: &#39;https://i.ytimg.com/vi/DpvUuXNiQ3w/default.jpg&#39;, &#39;width&#39;: 120}, &#39;high&#39;: {&#39;height&#39;: 360, &#39;url&#39;: &#39;https://i.ytimg.com/vi/DpvUuXNiQ3w/hqdefault.jpg&#39;, &#39;width&#39;: 480}, &#39;medium&#39;: {&#39;height&#39;: 180, &#39;url&#39;: &#39;https://i.ytimg.com/vi/DpvUuXNiQ3w/mqdefault.jpg&#39;, &#39;width&#39;: 320}}, &#39;title&#39;: &#39;Hometown (Slowed // Sad part) 1 HOUR&#39;}}], &#39;kind&#39;: &#39;youtube#searchListResponse&#39;, &#39;nextPageToken&#39;: &#39;CAUQAA&#39;, &#39;pageInfo&#39;: {&#39;resultsPerPage&#39;: 5, &#39;totalResults&#39;: 1000000}, &#39;regionCode&#39;: &#39;US&#39;} . video_statistics = youtube.videos().list(id=&#39;geW09OOqieU&#39;, part=&#39;statistics&#39;).execute() video_statistics . {&#39;etag&#39;: &#39;mwdHggSG7B3y-3WwPQak2___hAs&#39;, &#39;items&#39;: [{&#39;etag&#39;: &#39;9KcOBfpSxFpJ8LqOMh8I5WnRjOw&#39;, &#39;id&#39;: &#39;geW09OOqieU&#39;, &#39;kind&#39;: &#39;youtube#video&#39;, &#39;statistics&#39;: {&#39;commentCount&#39;: &#39;7652&#39;, &#39;dislikeCount&#39;: &#39;78214&#39;, &#39;favoriteCount&#39;: &#39;0&#39;, &#39;likeCount&#39;: &#39;441332&#39;, &#39;viewCount&#39;: &#39;63588476&#39;}}], &#39;kind&#39;: &#39;youtube#videoListResponse&#39;, &#39;pageInfo&#39;: {&#39;resultsPerPage&#39;: 1, &#39;totalResults&#39;: 1}} . for item in sorted(results[&#39;items&#39;], key=lambda x:x[&#39;snippet&#39;][&#39;publishedAt&#39;]): print(item[&#39;snippet&#39;][&#39;title&#39;], item[&#39;snippet&#39;][&#39;publishedAt&#39;], item[&#39;id&#39;][&#39;videoId&#39;]) . Hometown (Slowed // Sad part) 1 HOUR 2020-10-26T19:02:50Z DpvUuXNiQ3w twenty one pilots - Christmas Saves The Year (Official Audio) 2020-12-09T02:00:09Z ozXb10fOi2A Twenty One Pilots - Shy Away (Official Video) 2021-04-07T17:11:05Z 3sO-Y1Zbft4 Shy Away 2021-04-07T17:16:54Z IAVy8gI7HOQ Twenty One Pilots - Choker (Official Video) 2021-04-30T04:00:15Z 2sBRnnnZyFw . def get_start_date_string(search_period_days): &quot;&quot;&quot;Returns string for date at start of search period.&quot;&quot;&quot; search_start_date = datetime.today() - timedelta(search_period_days) date_string = datetime(year=search_start_date.year,month=search_start_date.month, day=search_start_date.day).strftime(&#39;%Y-%m-%dT%H:%M:%SZ&#39;) return date_string get_start_date_string(30) . &#39;2021-04-12T00:00:00Z&#39; . def find_title(item): title = item[&#39;snippet&#39;][&#39;title&#39;] return title def find_video_url(item): video_id = item[&#39;id&#39;][&#39;videoId&#39;] video_url = &quot;https://www.youtube.com/watch?v=&quot; + video_id return video_url def find_viewcount(item, youtube): video_id = item[&#39;id&#39;][&#39;videoId&#39;] video_statistics = youtube.videos().list(id=video_id, part=&#39;statistics&#39;).execute() viewcount = int(video_statistics[&#39;items&#39;][0][&#39;statistics&#39;][&#39;viewCount&#39;]) return viewcount def find_likecount(item, youtube): video_id = item[&#39;id&#39;][&#39;videoId&#39;] video_statistics = youtube.videos().list(id=video_id, part=&#39;statistics&#39;).execute() likecount = int(video_statistics[&#39;items&#39;][0][&#39;statistics&#39;][&#39;likeCount&#39;]) return likecount def find_dislikecount(item, youtube): video_id = item[&#39;id&#39;][&#39;videoId&#39;] video_statistics = youtube.videos().list(id=video_id, part=&#39;statistics&#39;).execute() dislikecount = int(video_statistics[&#39;items&#39;][0][&#39;statistics&#39;][&#39;dislikeCount&#39;]) return dislikecount def find_channel_id(item): channel_id = item[&#39;snippet&#39;][&#39;channelId&#39;] return channel_id def find_channel_url(item): channel_id = item[&#39;snippet&#39;][&#39;channelId&#39;] channel_url = &quot;https://www.youtube.com/channel/&quot; + channel_id return channel_url def find_channel_title(channel_id, youtube): channel_search = youtube.channels().list(id=channel_id, part=&#39;brandingSettings&#39;).execute() channel_name = channel_search[&#39;items&#39;][0] [&#39;brandingSettings&#39;][&#39;channel&#39;][&#39;title&#39;] return channel_name def find_num_subscribers(channel_id, youtube): subs_search = youtube.channels().list(id=channel_id, part=&#39;statistics&#39;).execute() if subs_search[&#39;items&#39;][0][&#39;statistics&#39;][&#39;hiddenSubscriberCount&#39;]: num_subscribers = 1000000 else: num_subscribers = int(subs_search[&#39;items&#39;][0] [&#39;statistics&#39;][&#39;subscriberCount&#39;]) return num_subscribers def view_to_sub_ratio(viewcount, num_subscribers): if num_subscribers == 0: return 0 else: ratio = viewcount / num_subscribers return ratio def how_old(item): when_published = item[&#39;snippet&#39;][&#39;publishedAt&#39;] when_published_datetime_object = datetime.strptime(when_published, &#39;%Y-%m-%dT%H:%M:%SZ&#39;) today_date = datetime.today() days_since_published = int((today_date - when_published_datetime_object).days) if days_since_published == 0: days_since_published = 1 return days_since_published def find_count(q, item): if q in item[&#39;snippet&#39;][&#39;title&#39;] and item[&#39;snippet&#39;][&#39;description&#39;]: count+=1 return count def custom_score(likecount, dislikecount, viewcount, ratio, days_since_published): ratio = min(ratio, 10) score = (viewcount * ratio) / days_since_published return score + (likecount/dislikecount) + count . def search_each_term(search_terms, api_key, uploaded_since, views_threshold=10000, num_to_print=5): &quot;&quot;&quot;Uses search term list to execute API calls and print results.&quot;&quot;&quot; if type(search_terms) == str: search_terms = [search_terms] list_of_dfs = [] for index, search_term in enumerate(search_terms): df = find_videos(search_terms[index], api_key, views_threshold=views_threshold, uploaded_since = uploaded_since) df = df.sort_values([&#39;Custom_Score&#39;], ascending=[0]) list_of_dfs.append(df) # 1 - concatenate them all full_df = pd.concat((list_of_dfs),axis=0) full_df = full_df.sort_values([&#39;Custom_Score&#39;], ascending=[0]) print(&quot;THE TOP VIDEOS OVERALL ARE:&quot;) print_featured_videos(full_df, num_to_print) print(&quot;========================== n&quot;) # 2 - in total for index, search_term in enumerate(search_terms): results_df = list_of_dfs[index] print(&quot;THE TOP VIDEOS FOR SEARCH TERM &#39;{}&#39;:&quot;.format(search_terms[index])) print_featured_videos(results_df, num_to_print) results_df_dict = dict(zip(search_terms, list_of_dfs)) results_df_dict[&#39;top_videos&#39;] = full_df return results_df_dict def find_videos(search_terms, api_key, views_threshold, uploaded_since): &quot;&quot;&quot;Calls other functions (below) to find results and populate dataframe.&quot;&quot;&quot; # Initialise results dataframe dataframe = pd.DataFrame(columns=(&#39;Title&#39;, &#39;Video URL&#39;, &#39;Custom_Score&#39;, &#39;Views&#39;, &#39;Channel Name&#39;,&#39;Num_subscribers&#39;, &#39;View-Subscriber Ratio&#39;,&#39;Channel URL&#39;)) # Run search search_results, youtube_api = search_api(search_terms, api_key, uploaded_since) results_df = fill_dataframe(search_results, youtube_api, dataframe, views_threshold) return results_df def search_api(search_terms, api_key, uploaded_since): &quot;&quot;&quot;Executes search through API and returns result.&quot;&quot;&quot; # Initialise API call youtube_api = build(&#39;youtube&#39;, &#39;v3&#39;, developerKey = api_key) #Make the search results = youtube_api.search().list(q=search_terms, part=&#39;snippet&#39;, type=&#39;video&#39;, order=&#39;viewCount&#39;, maxResults=50, publishedAfter=uploaded_since).execute() return results, youtube_api def fill_dataframe(results, youtube_api, df, views_threshold): &quot;&quot;&quot;Extracts relevant information and puts into dataframe&quot;&quot;&quot; # Loop over search results and add key information to dataframe i = 1 for item in results[&#39;items&#39;]: viewcount = find_viewcount(item, youtube_api) #likecount = find_likecount(item, youtube_api) #dislikecount = find_dislikecount(item, youtube_api) if viewcount &gt; views_threshold: title = find_title(item) video_url = find_video_url(item) channel_url = find_channel_url(item) channel_id = find_channel_id(item) channel_name = find_channel_title(channel_id, youtube_api) num_subs = find_num_subscribers(channel_id, youtube_api) ratio = view_to_sub_ratio(viewcount, num_subs) days_since_published = how_old(item) likecount = find_likecount(item, youtube) dislikecount = find_dislikecount(item, youtube) score = custom_score(likecount, dislikecount, viewcount, ratio, days_since_published) df.loc[i] = [title, video_url, score, viewcount, channel_name, num_subs, ratio, channel_url] i += 1 return df def print_featured_videos(df, num_to_print): &quot;&quot;&quot;Prints top videos to console, with details and link to video.&quot;&quot;&quot; if len(df) &lt; num_to_print: num_to_print = len(df) if num_to_print == 0: print(&quot;NO RESULTS&quot;) else: for i in range(num_to_print): video = df.iloc[i] title = video[&#39;Title&#39;] views = video[&#39;Views&#39;] subs = video[&#39;Num_subscribers&#39;] link = video[&#39;Video URL&#39;] print(&quot;Video #{}: nThe video &#39;{}&#39; has {} views, from a channel with {} subscribers and can be viewed here: {} n&quot; .format(i+1, title, views, subs, link)) print(&quot;]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] n&quot;) . search_each_term(&quot;Data Science&quot;, api_key, &#39;2021-01-11T00:00:00Z&#39; ) . THE TOP VIDEOS OVERALL ARE: Video #1: The video &#39;Become a DATA ANALYST with NO degree?!? The Google Data Analytics Professional Certificate&#39; has 731020 views, from a channel with 37200 subscribers and can be viewed here: https://www.youtube.com/watch?v=fmLPS6FBbac ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] Video #2: The video &#39;Work Week in My Life as a Data Scientist&#39; has 204983 views, from a channel with 9460 subscribers and can be viewed here: https://www.youtube.com/watch?v=yfLczGFw-ok ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] Video #3: The video &#39;Google vs IBM Data Analyst Certificate - BEST Certificate for Data Analysts&#39; has 188597 views, from a channel with 37200 subscribers and can be viewed here: https://www.youtube.com/watch?v=jp-Lv_3a2VI ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] Video #4: The video &#39;Luminar Technolab, Data Science, AI, ML, Big Data Analytics, Python, Software Testing, MEAN STACK&#39; has 57204 views, from a channel with 600 subscribers and can be viewed here: https://www.youtube.com/watch?v=FzMncStFvns ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] Video #5: The video &#39;Data Science ‚Äì –±—É–¥—É—â–µ–µ —É–∂–µ –∑–¥–µ—Å—å // –ö–∞–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –º–µ–Ω—è—é—Ç –∂–∏–∑–Ω—å? 12+&#39; has 168541 views, from a channel with 1930 subscribers and can be viewed here: https://www.youtube.com/watch?v=tk-vPly7vf4 ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] ========================== THE TOP VIDEOS FOR SEARCH TERM &#39;Data Science&#39;: Video #1: The video &#39;Become a DATA ANALYST with NO degree?!? The Google Data Analytics Professional Certificate&#39; has 731020 views, from a channel with 37200 subscribers and can be viewed here: https://www.youtube.com/watch?v=fmLPS6FBbac ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] Video #2: The video &#39;Work Week in My Life as a Data Scientist&#39; has 204983 views, from a channel with 9460 subscribers and can be viewed here: https://www.youtube.com/watch?v=yfLczGFw-ok ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] Video #3: The video &#39;Google vs IBM Data Analyst Certificate - BEST Certificate for Data Analysts&#39; has 188597 views, from a channel with 37200 subscribers and can be viewed here: https://www.youtube.com/watch?v=jp-Lv_3a2VI ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] Video #4: The video &#39;Luminar Technolab, Data Science, AI, ML, Big Data Analytics, Python, Software Testing, MEAN STACK&#39; has 57204 views, from a channel with 600 subscribers and can be viewed here: https://www.youtube.com/watch?v=FzMncStFvns ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] Video #5: The video &#39;Data Science ‚Äì –±—É–¥—É—â–µ–µ —É–∂–µ –∑–¥–µ—Å—å // –ö–∞–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –º–µ–Ω—è—é—Ç –∂–∏–∑–Ω—å? 12+&#39; has 168541 views, from a channel with 1930 subscribers and can be viewed here: https://www.youtube.com/watch?v=tk-vPly7vf4 ]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]] . {&#39;Data Science&#39;: Title ... Channel URL 1 Become a DATA ANALYST with NO degree?!? The Go... ... https://www.youtube.com/channel/UCLLw7jmFsvfIV... 6 Work Week in My Life as a Data Scientist ... https://www.youtube.com/channel/UCm4RlxY2d-bFl... 7 Google vs IBM Data Analyst Certificate - BEST ... ... https://www.youtube.com/channel/UCLLw7jmFsvfIV... 40 Luminar Technolab, Data Science, AI, ML, Big D... ... https://www.youtube.com/channel/UCya9kry_lyqBo... 10 Data Science ‚Äì –±—É–¥—É—â–µ–µ —É–∂–µ –∑–¥–µ—Å—å // –ö–∞–∫ –∏—Å–∫—É—Å—Å... ... https://www.youtube.com/channel/UCFu9xJbz-DzBp... 31 Data Science –≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–º –∫–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥–µ ‚Äì –∫–∞... ... https://www.youtube.com/channel/UCFu9xJbz-DzBp... 2 ‡§Ø‡•á Free IBM Courses ‡§ú‡•Ä‡§µ‡§® ‡§¨‡§¶‡§≤ ‡§¶‡•á‡§Ç‡§ó‡•Ä | 6-Month F... ... https://www.youtube.com/channel/UCKVdr_Lro6WDK... 8 Data Analytics for Beginners | Google Data Ana... ... https://www.youtube.com/channel/UC_fyAp919RnkK... 26 –ù–µ–π—Ä–æ—Å–µ—Ç–∏ –ø–æ–º–æ–≥–∞—é—Ç –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –Ω–∞ —Å–≤—è–∑–∏ // Data... ... https://www.youtube.com/channel/UCFu9xJbz-DzBp... 4 Internship that made me rethink my career...(t... ... https://www.youtube.com/channel/UC2UXDak6o7rBm... 20 NJIT MAKES Data Science ... https://www.youtube.com/channel/UC1a7isVSMhccO... 5 Got a new Job as Data scientist in Englandü•∞ | ... ... https://www.youtube.com/channel/UC_r5eKP41yegj... 17 Data Scientist vs Data Analyst (funny!) ... https://www.youtube.com/channel/UCLLw7jmFsvfIV... 30 [FSH SPECIAL TOPIC] Data Science Topics for HI... ... https://www.youtube.com/channel/UCugKa_dTMkVvs... 12 Data Science For Beginners | Career In Data Sc... ... https://www.youtube.com/channel/UCO3pT4ZI8x2RV... 9 Data scientist Rebekah Jones arrested in Florida ... https://www.youtube.com/channel/UCjpzEgbbDUg4Y... 28 Combien gagne un Data Scientist ? Les salaires... ... https://www.youtube.com/channel/UCnEHCrot2HkyS... 43 Honest Review of IIT Madras Online B.Sc. Degre... ... https://www.youtube.com/channel/UCNliY-e1-7VNt... 29 What&amp;#39;s the best certificate for data analy... ... https://www.youtube.com/channel/UC2UXDak6o7rBm... 14 Why I Quit Data Science ... https://www.youtube.com/channel/UCiT9RITQ9PW6B... 21 Jim Cornette on Stephanie McMahon Talking Abou... ... https://www.youtube.com/channel/UClkZ_CUNwGavQ... 25 Why You Probably Won&amp;#39;t Become a Data Scien... ... https://www.youtube.com/channel/UCiT9RITQ9PW6B... 13 Live 15 hour Session In Implementing End To En... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 22 ÿπŸÑŸÖ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ - data science ... https://www.youtube.com/channel/UC4Y8dVfo_-ayd... 27 How to learn math for data science (the minimi... ... https://www.youtube.com/channel/UC2UXDak6o7rBm... 46 LinkedIn Free Courses With Certificate Python ... ... https://www.youtube.com/channel/UCNr_dWKxRanxF... 42 The 7 Biggest Data Science Beginner Mistakes ... https://www.youtube.com/channel/UCiT9RITQ9PW6B... 33 Complete Road Map Towards Data Science In 2021 ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 16 Feb12- Live Virtual Mock Interview To Real Int... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 3 Data Analysis with Python Course - Numpy, Pand... ... https://www.youtube.com/channel/UC8butISFwT-Wl... 41 iNeuron Full Stack Data Science Course With 1 ... ... https://www.youtube.com/channel/UCb1GdqUqArXMQ... 32 Bank Data Scientist reviews money laundering s... ... https://www.youtube.com/channel/UCNWh9GuY0APdp... 24 What is Data Science and Data Analytics | Data... ... https://www.youtube.com/channel/UC_QpDH_oeU0jX... 36 How to Go From Data Analyst to Data Scientist ... https://www.youtube.com/channel/UCiT9RITQ9PW6B... 37 How to get a data science job ... https://www.youtube.com/channel/UC2UXDak6o7rBm... 11 WhatsApp Pink Virus in circulation can steal u... ... https://www.youtube.com/channel/UCrC8mOqJQpoB7... 23 Python for Data Science Full Course | Data Sci... ... https://www.youtube.com/channel/UCObs0kLIrDjX2... 47 The Art of Learning Data Science (How to learn... ... https://www.youtube.com/channel/UCV8e2g4IWQqK7... 44 Introduction | Mathematics and statistics for ... ... https://www.youtube.com/channel/UCh9nVJoWXmFb7... 15 Data Analytics In Excel Full Course | Data Ana... ... https://www.youtube.com/channel/UCsvqVGtbbyHaM... 38 Feb 2 - Live Virtual Mock Interview Of Fresher... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 19 Breaking Down 2020 Results With Data Scientist... ... https://www.youtube.com/channel/UCaXkIU1QidjPw... 49 Amazing Data Science End To End Project From S... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 18 freeCodeCamp.org Curriculum Expansion: Math + ... ... https://www.youtube.com/channel/UC8butISFwT-Wl... 35 Impfen | Data Science | ARTE ... https://www.youtube.com/channel/UCLLibJTCy3sXj... 50 Successful Transition From Mechanical Engineer... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 45 Feb 3- Live Virtual Mock Interview Of Freshers... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 48 Live Virtual Mock Interview For Data Science Role ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 34 Vaccins : comment juger leur impact ? | Data S... ... https://www.youtube.com/channel/UCwI-JbGNsojun... 39 Data Science with Python | Python For Data Sci... ... https://www.youtube.com/channel/UCCktnahuRFYIB... [50 rows x 8 columns], &#39;top_videos&#39;: Title ... Channel URL 1 Become a DATA ANALYST with NO degree?!? The Go... ... https://www.youtube.com/channel/UCLLw7jmFsvfIV... 6 Work Week in My Life as a Data Scientist ... https://www.youtube.com/channel/UCm4RlxY2d-bFl... 7 Google vs IBM Data Analyst Certificate - BEST ... ... https://www.youtube.com/channel/UCLLw7jmFsvfIV... 40 Luminar Technolab, Data Science, AI, ML, Big D... ... https://www.youtube.com/channel/UCya9kry_lyqBo... 10 Data Science ‚Äì –±—É–¥—É—â–µ–µ —É–∂–µ –∑–¥–µ—Å—å // –ö–∞–∫ –∏—Å–∫—É—Å—Å... ... https://www.youtube.com/channel/UCFu9xJbz-DzBp... 31 Data Science –≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–º –∫–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥–µ ‚Äì –∫–∞... ... https://www.youtube.com/channel/UCFu9xJbz-DzBp... 2 ‡§Ø‡•á Free IBM Courses ‡§ú‡•Ä‡§µ‡§® ‡§¨‡§¶‡§≤ ‡§¶‡•á‡§Ç‡§ó‡•Ä | 6-Month F... ... https://www.youtube.com/channel/UCKVdr_Lro6WDK... 8 Data Analytics for Beginners | Google Data Ana... ... https://www.youtube.com/channel/UC_fyAp919RnkK... 26 –ù–µ–π—Ä–æ—Å–µ—Ç–∏ –ø–æ–º–æ–≥–∞—é—Ç –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –Ω–∞ —Å–≤—è–∑–∏ // Data... ... https://www.youtube.com/channel/UCFu9xJbz-DzBp... 4 Internship that made me rethink my career...(t... ... https://www.youtube.com/channel/UC2UXDak6o7rBm... 20 NJIT MAKES Data Science ... https://www.youtube.com/channel/UC1a7isVSMhccO... 5 Got a new Job as Data scientist in Englandü•∞ | ... ... https://www.youtube.com/channel/UC_r5eKP41yegj... 17 Data Scientist vs Data Analyst (funny!) ... https://www.youtube.com/channel/UCLLw7jmFsvfIV... 30 [FSH SPECIAL TOPIC] Data Science Topics for HI... ... https://www.youtube.com/channel/UCugKa_dTMkVvs... 12 Data Science For Beginners | Career In Data Sc... ... https://www.youtube.com/channel/UCO3pT4ZI8x2RV... 9 Data scientist Rebekah Jones arrested in Florida ... https://www.youtube.com/channel/UCjpzEgbbDUg4Y... 28 Combien gagne un Data Scientist ? Les salaires... ... https://www.youtube.com/channel/UCnEHCrot2HkyS... 43 Honest Review of IIT Madras Online B.Sc. Degre... ... https://www.youtube.com/channel/UCNliY-e1-7VNt... 29 What&amp;#39;s the best certificate for data analy... ... https://www.youtube.com/channel/UC2UXDak6o7rBm... 14 Why I Quit Data Science ... https://www.youtube.com/channel/UCiT9RITQ9PW6B... 21 Jim Cornette on Stephanie McMahon Talking Abou... ... https://www.youtube.com/channel/UClkZ_CUNwGavQ... 25 Why You Probably Won&amp;#39;t Become a Data Scien... ... https://www.youtube.com/channel/UCiT9RITQ9PW6B... 13 Live 15 hour Session In Implementing End To En... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 22 ÿπŸÑŸÖ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ - data science ... https://www.youtube.com/channel/UC4Y8dVfo_-ayd... 27 How to learn math for data science (the minimi... ... https://www.youtube.com/channel/UC2UXDak6o7rBm... 46 LinkedIn Free Courses With Certificate Python ... ... https://www.youtube.com/channel/UCNr_dWKxRanxF... 42 The 7 Biggest Data Science Beginner Mistakes ... https://www.youtube.com/channel/UCiT9RITQ9PW6B... 33 Complete Road Map Towards Data Science In 2021 ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 16 Feb12- Live Virtual Mock Interview To Real Int... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 3 Data Analysis with Python Course - Numpy, Pand... ... https://www.youtube.com/channel/UC8butISFwT-Wl... 41 iNeuron Full Stack Data Science Course With 1 ... ... https://www.youtube.com/channel/UCb1GdqUqArXMQ... 32 Bank Data Scientist reviews money laundering s... ... https://www.youtube.com/channel/UCNWh9GuY0APdp... 24 What is Data Science and Data Analytics | Data... ... https://www.youtube.com/channel/UC_QpDH_oeU0jX... 36 How to Go From Data Analyst to Data Scientist ... https://www.youtube.com/channel/UCiT9RITQ9PW6B... 37 How to get a data science job ... https://www.youtube.com/channel/UC2UXDak6o7rBm... 11 WhatsApp Pink Virus in circulation can steal u... ... https://www.youtube.com/channel/UCrC8mOqJQpoB7... 23 Python for Data Science Full Course | Data Sci... ... https://www.youtube.com/channel/UCObs0kLIrDjX2... 47 The Art of Learning Data Science (How to learn... ... https://www.youtube.com/channel/UCV8e2g4IWQqK7... 44 Introduction | Mathematics and statistics for ... ... https://www.youtube.com/channel/UCh9nVJoWXmFb7... 15 Data Analytics In Excel Full Course | Data Ana... ... https://www.youtube.com/channel/UCsvqVGtbbyHaM... 38 Feb 2 - Live Virtual Mock Interview Of Fresher... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 19 Breaking Down 2020 Results With Data Scientist... ... https://www.youtube.com/channel/UCaXkIU1QidjPw... 49 Amazing Data Science End To End Project From S... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 18 freeCodeCamp.org Curriculum Expansion: Math + ... ... https://www.youtube.com/channel/UC8butISFwT-Wl... 35 Impfen | Data Science | ARTE ... https://www.youtube.com/channel/UCLLibJTCy3sXj... 50 Successful Transition From Mechanical Engineer... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 45 Feb 3- Live Virtual Mock Interview Of Freshers... ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 48 Live Virtual Mock Interview For Data Science Role ... https://www.youtube.com/channel/UCNU_lfiiWBdtU... 34 Vaccins : comment juger leur impact ? | Data S... ... https://www.youtube.com/channel/UCwI-JbGNsojun... 39 Data Science with Python | Python For Data Sci... ... https://www.youtube.com/channel/UCCktnahuRFYIB... [50 rows x 8 columns]} .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/video/2021/06/08/youtube-personalized-recommender-api.html",
            "relUrl": "/video/2021/06/08/youtube-personalized-recommender-api.html",
            "date": " ‚Ä¢ Jun 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Nbdev Github Codespaces A New Literate Programming",
            "content": "Example Markdown Post . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one‚Äôs thoughts during a program‚Äôs creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | ‚Ä¶ and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming¬†&#8617; . | This is not a criticism of Jupyter. Jupyter doesn‚Äôt claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria.¬†&#8617; . |",
            "url": "https://sparsh-ai.github.io/rec-tutorials/samplepost",
            "relUrl": "/samplepost",
            "date": " ‚Ä¢ Jun 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://sparsh-ai.github.io/rec-tutorials/samplepost2",
            "relUrl": "/samplepost2",
            "date": " ‚Ä¢ Jun 2, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Similar Product Recommendations",
            "content": ". Choice of variables . Image Encoder: Any pre-trained image classification model can be selected. These models are commonly known as encoders because their job is to encode an image into a feature vector. In our case, we analyzed four encoders named 1) MobileNet, 2) EfficientNet, 3) ResNet and 4) BiT. After some basic research, we decided to select BiT model because of its performance. I selected the BiT-M-50x3 variant of model which is of size 748 MB. More details about this architecture can be found on the official page here. | Vector Similarity System: Images are represented in a fixed-length feature vector format. For the given input vector, we need to find the TopK most similar vectors, keeping the memory efficiency and real-time retrival objective in mind. We explored the most popular techniques and listed down five of them: Annoy, Cosine distance, L1 distance, Locally Sensitive Hashing (LSH) and Image Deep Ranking. We selected Annoy because of its fast and efficient nature. More details about Annoy can be found on the official page here. | Dataset: This system is able to handle all kind of image dataset. Only the basic preprocessing (in step 1) would need some modifications depending on the dataset. We chose Fashion Product Images (Small). Other examples can be Food-11 image dataset, and Caltech 256 Image Dataset. | . Step 1: Data Acquisition . Download the raw image dataset into a directory. Categorize these images into their respective category directories. Make sure that images are of the same type, JPEG recommended. We will also process the metadata and store it in a serialized file, CSV recommended. . # downloading raw images from kaggle !kaggle datasets download -d paramaggarwal/fashion-product-images-small !unzip fashion-product-images-small.zip . import pandas as pd from shutil import move import os from tqdm import tqdm os.mkdir(&#39;/content/Fashion_data&#39;) os.chdir(&#39;/content/Fashion_data&#39;) df = pd.read_csv(&#39;/content/styles.csv&#39;, usecols=[&#39;id&#39;,&#39;masterCategory&#39;]).reset_index() df[&#39;id&#39;] = df[&#39;id&#39;].astype(&#39;str&#39;) all_images = os.listdir(&#39;/content/images/&#39;) co = 0 os.mkdir(&#39;/content/Fashion_data/categories&#39;) for image in tqdm(all_images): category = df[df[&#39;id&#39;] == image.split(&#39;.&#39;)[0]][&#39;masterCategory&#39;] category = str(list(category)[0]) if not os.path.exists(os.path.join(&#39;/content/Fashion_data/categories&#39;, category)): os.mkdir(os.path.join(&#39;/content/Fashion_data/categories&#39;, category)) path_from = os.path.join(&#39;/content/images&#39;, image) path_to = os.path.join(&#39;/content/Fashion_data/categories&#39;, category, image) move(path_from, path_to) co += 1 print(&#39;Moved {} images.&#39;.format(co)) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44441/44441 [02:08&lt;00:00, 346.93it/s] . Moved 44441 images. . . Step 2: Encoder Fine-tuning [optional] . Download the pre-trained image model and add two additional layers on top of that: the first layer is a feature vector layer and the second layer is the classification layer. We will only train these 2 layers on our data and after training, we will select the feature vector layer as the output of our fine-tuned encoder. After fine-tuning the model, we will save the feature extractor for later use. . import itertools import os import matplotlib.pylab as plt import numpy as np import tensorflow as tf import tensorflow_hub as hub print(&quot;TF version:&quot;, tf.__version__) print(&quot;Hub version:&quot;, hub.__version__) print(&quot;GPU is&quot;, &quot;available&quot; if tf.config.list_physical_devices(&#39;GPU&#39;) else &quot;NOT AVAILABLE&quot;) . TF version: 2.4.1 Hub version: 0.12.0 GPU is available . MODULE_HANDLE = &#39;https://tfhub.dev/google/bit/m-r50x3/1&#39; IMAGE_SIZE = (224, 224) print(&quot;Using {} with input size {}&quot;.format(MODULE_HANDLE, IMAGE_SIZE)) BATCH_SIZE = 32 N_FEATURES = 256 . Using https://tfhub.dev/google/bit/m-r50x3/1 with input size (224, 224) . datagen_kwargs = dict(rescale=1./255, validation_split=.20) dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, interpolation=&quot;bilinear&quot;) valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator( **datagen_kwargs) valid_generator = valid_datagen.flow_from_directory( data_dir, subset=&quot;validation&quot;, shuffle=False, **dataflow_kwargs) do_data_augmentation = False if do_data_augmentation: train_datagen = tf.keras.preprocessing.image.ImageDataGenerator( rotation_range=40, horizontal_flip=True, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, **datagen_kwargs) else: train_datagen = valid_datagen train_generator = train_datagen.flow_from_directory( data_dir, subset=&quot;training&quot;, shuffle=True, **dataflow_kwargs) . Found 8886 images belonging to 7 classes. Found 35555 images belonging to 7 classes. . print(&quot;Building model with&quot;, MODULE_HANDLE) model = tf.keras.Sequential([ tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)), hub.KerasLayer(MODULE_HANDLE, trainable=False), tf.keras.layers.Dropout(rate=0.2), tf.keras.layers.Dense(N_FEATURES, kernel_regularizer=tf.keras.regularizers.l2(0.0001)), tf.keras.layers.Dropout(rate=0.2), tf.keras.layers.Dense(train_generator.num_classes, kernel_regularizer=tf.keras.regularizers.l2(0.0001)) ]) model.build((None,)+IMAGE_SIZE+(3,)) model.summary() . Building model with https://tfhub.dev/google/bit/m-r50x3/1 Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 6144) 211174080 _________________________________________________________________ dropout_2 (Dropout) (None, 6144) 0 _________________________________________________________________ dense_2 (Dense) (None, 256) 1573120 _________________________________________________________________ dropout_3 (Dropout) (None, 256) 0 _________________________________________________________________ dense_3 (Dense) (None, 7) 1799 ================================================================= Total params: 212,748,999 Trainable params: 1,574,919 Non-trainable params: 211,174,080 _________________________________________________________________ . lr = 0.003 * BATCH_SIZE / 512 SCHEDULE_LENGTH = 500 SCHEDULE_BOUNDARIES = [200, 300, 400] # Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES. lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=SCHEDULE_BOUNDARIES, values=[lr, lr*0.1, lr*0.001, lr*0.0001]) optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9) loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True) model.compile(optimizer=optimizer, loss=loss_fn, metrics=[&#39;accuracy&#39;]) . steps_per_epoch = train_generator.samples // train_generator.batch_size validation_steps = valid_generator.samples // valid_generator.batch_size hist = model.fit( train_generator, epochs=5, steps_per_epoch=steps_per_epoch, validation_data=valid_generator, validation_steps=validation_steps).history . Epoch 1/5 1111/1111 [==============================] - 1221s 1s/step - loss: 0.4232 - accuracy: 0.9410 - val_loss: 0.1400 - val_accuracy: 0.9878 Epoch 2/5 1111/1111 [==============================] - 1210s 1s/step - loss: 0.2100 - accuracy: 0.9760 - val_loss: 0.1399 - val_accuracy: 0.9879 Epoch 3/5 1111/1111 [==============================] - 1210s 1s/step - loss: 0.2115 - accuracy: 0.9766 - val_loss: 0.1397 - val_accuracy: 0.9880 Epoch 4/5 1111/1111 [==============================] - 1210s 1s/step - loss: 0.2094 - accuracy: 0.9763 - val_loss: 0.1396 - val_accuracy: 0.9880 Epoch 5/5 1111/1111 [==============================] - 1210s 1s/step - loss: 0.1923 - accuracy: 0.9775 - val_loss: 0.1394 - val_accuracy: 0.9880 . if not os.path.exists(&#39;/content/drive/MyDrive/ImgSim/&#39;): os.mkdir(&#39;/content/drive/MyDrive/ImgSim/&#39;) feature_extractor = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-3].output) feature_extractor.save(&#39;/content/drive/MyDrive/ImgSim/bit_feature_extractor&#39;, save_format=&#39;tf&#39;) saved_model_path = &#39;/content/drive/MyDrive/ImgSim/bit_model&#39; tf.saved_model.save(model, saved_model_path) . INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImgSim/bit_feature_extractor/assets . INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImgSim/bit_feature_extractor/assets . INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImgSim/bit_model/assets . INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImgSim/bit_model/assets . Step 3: Image Vectorization . Now, we will use the encoder (prepared in step 2) to encode the images (prepared in step 1). We will save feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use. . img_paths = [] for path in Path(&#39;/content/Fashion_data/categories&#39;).rglob(&#39;*.jpg&#39;): img_paths.append(path) np.random.shuffle(img_paths) . def load_img(path): img = tf.io.read_file(path) img = tf.io.decode_jpeg(img, channels=3) img = tf.image.resize_with_pad(img, 224, 224) img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...] return img . TRANSFER_LEARNING_FLAG = 1 if TRANSFER_LEARNING_FLAG: module = tf.keras.models.load_model(&#39;/content/drive/MyDrive/ImgSim/bit_feature_extractor&#39;) else: module_handle = &quot;https://tfhub.dev/google/bit/s-r50x3/ilsvrc2012_classification/1&quot; module = hub.load(module_handle) . imgvec_path = &#39;/content/img_vectors/&#39; Path(imgvec_path).mkdir(parents=True, exist_ok=True) . for filename in tqdm(img_paths[:5000]): img = load_img(str(filename)) features = module(img) feature_set = np.squeeze(features) outfile_name = os.path.basename(filename).split(&#39;.&#39;)[0] + &quot;.npz&quot; out_path_file = os.path.join(imgvec_path, outfile_name) np.savetxt(out_path_file, feature_set, delimiter=&#39;,&#39;) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [08:58&lt;00:00, 9.28it/s] . Step 4: Metadata and Indexing . We will assign a unique id to each image and create dictionaries to locate information of this image: 1) Image id to Image name dictionary, 2) Image id to image feature vector dictionary, and 3) (optional) Image id to metadata product id dictionary. We will also create an image id to image feature vector indexing. Then we will save these dictionaries and index object for later use. . test_img = &#39;/content/Fashion_data/categories/Accessories/1941.jpg&#39; dispImage(test_img) . styles = pd.read_csv(&#39;/content/styles.csv&#39;, error_bad_lines=False) styles[&#39;id&#39;] = styles[&#39;id&#39;].astype(&#39;str&#39;) styles.to_csv(root_path+&#39;/styles.csv&#39;, index=False) . def match_id(fname): return styles.index[styles.id==fname].values[0] . file_index_to_file_name = {} file_index_to_file_vector = {} file_index_to_product_id = {} # Configuring annoy parameters dims = 256 n_nearest_neighbors = 20 trees = 10000 # Reads all file names which stores feature vectors allfiles = glob.glob(&#39;/content/img_vectors/*.npz&#39;) t = AnnoyIndex(dims, metric=&#39;angular&#39;) . for findex, fname in tqdm(enumerate(allfiles)): file_vector = np.loadtxt(fname) file_name = os.path.basename(fname).split(&#39;.&#39;)[0] file_index_to_file_name[findex] = file_name file_index_to_file_vector[findex] = file_vector try: file_index_to_product_id[findex] = match_id(file_name) except IndexError: pass t.add_item(findex, file_vector) . 5000it [00:17, 279.67it/s] . t.build(trees) t.save(&#39;t.ann&#39;) . t.save(file_path+&#39;indexer.ann&#39;) pickle.dump(file_index_to_file_name, open(file_path+&quot;file_index_to_file_name.p&quot;, &quot;wb&quot;)) pickle.dump(file_index_to_file_vector, open(file_path+&quot;file_index_to_file_vector.p&quot;, &quot;wb&quot;)) pickle.dump(file_index_to_product_id, open(file_path+&quot;file_index_to_product_id.p&quot;, &quot;wb&quot;)) . Step 5: Local Testing . We will load a random image and find top-K most similar images. . img_addr = &#39;https://images-na.ssl-images-amazon.com/images/I/81%2Bd6eSA0eL._UL1500_.jpg&#39; !wget -q -O img.jpg $img_addr test_img = &#39;img.jpg&#39; topK = 4 test_vec = np.squeeze(module(load_img(test_img))) basewidth = 224 img = Image.open(test_img) wpercent = (basewidth/float(img.size[0])) hsize = int((float(img.size[1])*float(wpercent))) img = img.resize((basewidth,hsize), Image.ANTIALIAS) img . path_dict = {} for path in Path(&#39;/content/Fashion_data/categories&#39;).rglob(&#39;*.jpg&#39;): path_dict[path.name] = path nns = t.get_nns_by_vector(test_vec, n=topK) plt.figure(figsize=(20, 10)) for i in range(topK): x = file_index_to_file_name[nns[i]] x = path_dict[x+&#39;.jpg&#39;] y = file_index_to_product_id[nns[i]] title = &#39; n&#39;.join([str(j) for j in list(styles.loc[y].values[-5:])]) plt.subplot(1, topK, i+1) plt.title(title) plt.imshow(mpimg.imread(x)) plt.axis(&#39;off&#39;) plt.tight_layout() . Step 6: API Call . We will build two kinds of API - 1) UI based API using Streamlit, and 2) REST API using Flask. In both the cases, we will receive an image from user, encode it with our image encoder, find TopK similar vectors using Indexing object, and retrieve the image (and metadata) using dictionaries. We send these images (and metadata) back to the user. . %%writefile utils.py ### -utils.py- ### import os import tensorflow as tf from annoy import AnnoyIndex root_path = &#39;/content/drive/MyDrive/ImgSim&#39; class Encoder: encoder = tf.keras.models.load_model(os.path.join(root_path, &#39;bit_feature_extractor&#39;)) class Indexer: dims = 256 topK = 6 indexer = AnnoyIndex(dims, &#39;angular&#39;) indexer.load(os.path.join(root_path, &#39;indexer.ann&#39;)) encoder = Encoder() indexer = Indexer() . Writing utils.py . Streamlit App . %%writefile app.py ### -app.py- ### import streamlit as st import pandas as pd import numpy as np from PIL import Image from annoy import AnnoyIndex import glob import os import tensorflow as tf import tarfile import pickle from pathlib import Path import time from utils import encoder, indexer root_path = &#39;/content/drive/MyDrive/ImgSim&#39; start_time = time.time() encoder = encoder.encoder print(&quot;Encoder %s seconds &quot; % (time.time() - start_time)) topK = 6 start_time = time.time() t = indexer.indexer print(&quot;Indexer %s seconds &quot; % (time.time() - start_time)) # load the meta data meta_data = pd.read_csv(os.path.join(root_path, &#39;styles.csv&#39;)) # load the mappings file_index_to_file_name = pickle.load(open(os.path.join(root_path ,&#39;file_index_to_file_name.p&#39;), &#39;rb&#39;)) file_index_to_file_vector = pickle.load(open(os.path.join(root_path ,&#39;file_index_to_file_vector.p&#39;), &#39;rb&#39;)) file_index_to_product_id = pickle.load(open(os.path.join(root_path ,&#39;file_index_to_product_id.p&#39;), &#39;rb&#39;)) # load image path mapping path_dict = {} for path in Path(&#39;/content/Fashion_data/categories&#39;).rglob(&#39;*.jpg&#39;): path_dict[path.name] = path def load_img(path): img = tf.io.read_file(path) img = tf.io.decode_jpeg(img, channels=3) img = tf.image.resize_with_pad(img, 224, 224) img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...] return img query_path = &#39;/content/user_query.jpg&#39; st.title(&quot;Image Similarity App&quot;) uploaded_file = st.file_uploader(&quot;Choose an image...&quot;, type=&quot;jpg&quot;) if uploaded_file is not None: image = Image.open(uploaded_file) image.save(query_path) st.image(image, caption=&#39;Uploaded Image.&#39;, use_column_width=True) st.write(&quot;&quot;) st.write(&quot;Top similar images...&quot;) start_time = time.time() test_vec = np.squeeze(encoder(load_img(query_path))) print(&quot;Encoding %s seconds &quot; % (time.time() - start_time)) start_time = time.time() nns = t.get_nns_by_vector(test_vec, n=topK) print(&quot;SimilarityIndex %s seconds &quot; % (time.time() - start_time)) img_files = [] img_captions = [] start_time = time.time() for i in nns: #image files img_path = str(path_dict[file_index_to_file_name[i]+&#39;.jpg&#39;]) img_file = Image.open(img_path) img_files.append(img_file) #image captions item_id = file_index_to_product_id[i] img_caption = &#39; n&#39;.join([str(j) for j in list(meta_data.loc[item_id].values[-5:])]) img_captions.append(img_caption) print(&quot;Output %s seconds &quot; % (time.time() - start_time)) st.image(img_files, caption=img_captions, width=200) . Writing app.py . ! pip install -q pyngrok ! pip install -q streamlit ! pip install -q colab-everything from colab_everything import ColabStreamlit ColabStreamlit(&#39;app.py&#39;) . Flask API . server-side . %%writefile flask_app.py ### -flask_app.py- ### import pandas as pd import numpy as np from PIL import Image from annoy import AnnoyIndex import glob import os import tensorflow as tf import tarfile import pickle from pathlib import Path import time from utils import encoder, indexer import io import base64 from flask import Flask, request, jsonify, send_file _PPATH = &#39;/content/drive/MyDrive/ImgSim/&#39; start_time = time.time() encoder = encoder.encoder print(&quot;Encoder %s seconds &quot; % (time.time() - start_time)) topK = 6 start_time = time.time() t = indexer.indexer print(&quot;Indexer %s seconds &quot; % (time.time() - start_time)) # load the meta data meta_data = pd.read_csv(_PPATH+&#39;styles.csv&#39;) # load the mappings file_index_to_file_name = pickle.load(open(_PPATH+&#39;file_index_to_file_name.p&#39;, &#39;rb&#39;)) file_index_to_file_vector = pickle.load(open(_PPATH+&#39;file_index_to_file_vector.p&#39;, &#39;rb&#39;)) file_index_to_product_id = pickle.load(open(_PPATH+&#39;file_index_to_product_id.p&#39;, &#39;rb&#39;)) # load image path mapping path_dict = {} for path in Path(&#39;/content/Fashion_data/categories&#39;).rglob(&#39;*.jpg&#39;): path_dict[path.name] = path def load_img(path): img = tf.io.read_file(path) img = tf.io.decode_jpeg(img, channels=3) img = tf.image.resize_with_pad(img, 224, 224) img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...] return img query_path = &#39;/content/user_query.jpg&#39; def get_encoded_img(img): img_byte_arr = io.BytesIO() img.save(img_byte_arr, format=&#39;PNG&#39;) my_encoded_img = base64.encodebytes(img_byte_arr.getvalue()).decode(&#39;ascii&#39;) return my_encoded_img app = Flask(__name__) @app.route(&quot;/fashion&quot;, methods=[&quot;POST&quot;]) def home(): file = request.files[&#39;image&#39;] # Read the image via file.stream img = Image.open(file.stream) img.save(query_path) start_time = time.time() test_vec = np.squeeze(encoder(load_img(query_path))) print(&quot;Encoding %s seconds &quot; % (time.time() - start_time)) start_time = time.time() nns = t.get_nns_by_vector(test_vec, n=topK) print(&quot;SimilarityIndex %s seconds &quot; % (time.time() - start_time)) img_files = {} img_captions = {} start_time = time.time() for count, i in enumerate(nns): #image files img_path = str(path_dict[file_index_to_file_name[i]+&#39;.jpg&#39;]) img_file = Image.open(img_path) img_files[count] = get_encoded_img(img_file) #image captions item_id = file_index_to_product_id[i] img_caption = &#39; n&#39;.join([str(j) for j in list(meta_data.loc[item_id].values[-5:])]) img_captions[count] = img_caption print(&quot;Output %s seconds &quot; % (time.time() - start_time)) return jsonify(img_files) app.run(debug=True) . !nohup python3 -u flask_app.py &amp; . nohup: appending output to &#39;nohup.out&#39; . client-side . from PIL import Image from io import BytesIO import base64 import requests !wget -O &#39;img.jpg&#39; -q &#39;https://images-na.ssl-images-amazon.com/images/I/61utX8kBDlL._UL1100_.jpg&#39; url = &#39;http://localhost:5000/fashion&#39; my_img = {&#39;image&#39;: open(&#39;img.jpg&#39;, &#39;rb&#39;)} r = requests.post(url, files=my_img) imgs = [] for i in range(6): img = base64.decodebytes(r.json()[str(i)].encode(&#39;ascii&#39;)) img = Image.open(BytesIO(img)).convert(&#39;RGBA&#39;) imgs.append(img) imgs[1] . Step 7: Deployment on AWS Elastic BeanStalk . The API was deployed on AWS cloud infrastructure using AWS Elastic Beanstalk service. . . application.py . %%writefile ./ebsapp/application.py import os import zipfile import requests from tqdm import tqdm from shutil import move from pandas import read_csv from pathlib import Path import shutil import tensorflow as tf import tensorflow_hub as hub import numpy as np import pandas as pd import glob import json from tqdm import tqdm from annoy import AnnoyIndex from scipy import spatial import pickle import time from PIL import Image import tarfile import io import base64 from flask import Flask, request, jsonify, send_file from flask import redirect, url_for, flash, render_template # path = Path(__file__) # _PPATH = str(path.parents[1])+&#39;/&#39; _PPATH = os.path.join(os.getcwd(), &#39;mytemp&#39;) def load_img(path): img = tf.io.read_file(path) img = tf.io.decode_jpeg(img, channels=3) img = tf.image.resize_with_pad(img, 224, 224) img = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...] return img module_handle = &quot;https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4&quot; module = hub.load(module_handle) def allowed_file(filename): return &#39;.&#39; in filename and filename.rsplit(&#39;.&#39;, 1)[1].lower() in ALLOWED_EXTENSIONS topK = 5 threshold = 0.3 UPLOAD_FOLDER = _PPATH ALLOWED_EXTENSIONS = set([&#39;zip&#39;]) application = Flask(__name__) @application.route(&#39;/&#39;) def hello_world(): return &quot;Hello world!&quot; @application.route(&#39;/original_images&#39;, methods=[&#39;POST&#39;]) def upload_zip1(): os.makedirs(_PPATH, exist_ok=True) shutil.rmtree(_PPATH) os.makedirs(_PPATH, exist_ok=True) os.chdir(_PPATH) file = request.files[&#39;file&#39;] if file and allowed_file(file.filename): file.save(os.path.join(UPLOAD_FOLDER, &#39;zip1.zip&#39;)) zip1_path = os.path.join(_PPATH, &#39;zip1.zip&#39;) zip1_ipath = os.path.join(_PPATH, &#39;zip1&#39;) os.makedirs(zip1_ipath, exist_ok=True) with zipfile.ZipFile(zip1_path, &#39;r&#39;) as zip_ref: zip_ref.extractall(os.path.join(_PPATH, &#39;zip1&#39;)) img_paths = [] for path in Path(zip1_ipath).rglob(&#39;*.jpg&#39;): img_paths.append(path) imgvec_path = os.path.join(_PPATH, &#39;vecs1&#39;) Path(imgvec_path).mkdir(parents=True, exist_ok=True) for filename in tqdm(img_paths): outfile_name = os.path.basename(filename).split(&#39;.&#39;)[0] + &quot;.npz&quot; out_path_file = os.path.join(imgvec_path, outfile_name) if not os.path.exists(out_path_file): img = load_img(str(filename)) features = module(img) feature_set = np.squeeze(features) print(features.shape) np.savetxt(out_path_file, feature_set, delimiter=&#39;,&#39;) # Defining data structures as empty dict file_index_to_file_name = {} file_index_to_file_vector = {} # Configuring annoy parameters dims = 2048 n_nearest_neighbors = 20 trees = 10000 # Reads all file names which stores feature vectors allfiles = glob.glob(os.path.join(_PPATH, &#39;vecs1&#39;, &#39;*.npz&#39;)) t = AnnoyIndex(dims, metric=&#39;angular&#39;) for findex, fname in tqdm(enumerate(allfiles)): file_vector = np.loadtxt(fname) file_name = os.path.basename(fname).split(&#39;.&#39;)[0] file_index_to_file_name[findex] = file_name file_index_to_file_vector[findex] = file_vector t.add_item(findex, file_vector) t.build(trees) file_path = os.path.join(_PPATH,&#39;models/indices/&#39;) Path(file_path).mkdir(parents=True, exist_ok=True) t.save(file_path+&#39;indexer.ann&#39;) pickle.dump(file_index_to_file_name, open(file_path+&quot;file_index_to_file_name.p&quot;, &quot;wb&quot;)) pickle.dump(file_index_to_file_vector, open(file_path+&quot;file_index_to_file_vector.p&quot;, &quot;wb&quot;)) return &#39;File processed on the server status OK!&#39; @application.route(&#39;/test_images&#39;, methods=[&#39;POST&#39;]) def upload_zip2(): os.chdir(_PPATH) file = request.files[&#39;file&#39;] if file and allowed_file(file.filename): file.save(os.path.join(UPLOAD_FOLDER, &#39;zip2.zip&#39;)) zip2_path = os.path.join(_PPATH, &#39;zip2.zip&#39;) zip2_ipath = os.path.join(_PPATH, &#39;zip2&#39;) os.makedirs(zip2_ipath, exist_ok=True) with zipfile.ZipFile(zip2_path, &#39;r&#39;) as zip_ref: zip_ref.extractall(os.path.join(_PPATH, &#39;zip2&#39;)) query_files = [] for path in Path(zip2_ipath).rglob(&#39;*.jpg&#39;): query_files.append(path) dims = 2048 indexer = AnnoyIndex(dims, &#39;angular&#39;) indexer.load(os.path.join(_PPATH,&#39;models/indices/indexer.ann&#39;)) file_index_to_file_name = pickle.load(open(os.path.join(_PPATH,&#39;models/indices/file_index_to_file_name.p&#39;), &#39;rb&#39;)) results = pd.DataFrame(columns=[&#39;qid&#39;,&#39;fname&#39;,&#39;dist&#39;]) for q in query_files: temp_vec = np.squeeze(module(load_img(str(q)))) nns = indexer.get_nns_by_vector(temp_vec, n=topK, include_distances=True) col1 = [q.stem]*topK col2 = [file_index_to_file_name[x] for x in nns[0]] col3 = nns[1] results = results.append(pd.DataFrame({&#39;qid&#39;:col1,&#39;fname&#39;:col2,&#39;dist&#39;:col3})) results = results[results.dist&lt;=threshold] results = results.reset_index(drop=True).T.to_json() return results # run the app. if __name__ == &quot;__main__&quot;: # Setting debug to True enables debug output. This line should be # removed before deploying a production app. application.debug = True application.run() . . requirements.txt . %%writefile ./ebsapp/requirements.txt annoy==1.16.3 Pillow==2.2.2 click==7.1.2 Flask==1.1.2 itsdangerous==1.1.0 Jinja2==2.11.2 MarkupSafe==1.1.1 Werkzeug==1.0.1 numpy==1.18.5 pandas==1.0.5 pathlib==1.0.1 pip-tools==4.5.1 requests==2.23.0 scipy==1.4.1 tensorflow==2.0.0b1 tensorflow-hub==0.8.0 tqdm==4.41.1 urllib3==1.24.3 zipfile36==0.1.3 . . packages.config . %%writefile ./ebsapp/.ebextensions/01_packages.config packages: yum: gcc-c++: [] unixODBC-devel: [] files: &quot;/etc/httpd/conf.d/wsgi_custom.conf&quot;: mode: &quot;000644&quot; owner: root group: root content: | WSGIApplicationGroup %{GLOBAL} . . extras . some handy EBS commands . # !pip install aws.sam.cli # !sam init # %cd my-app # !git config --global user.email &quot;&lt;email&gt;&quot; # !git config --global user.name &quot;sparsh-ai&quot; # !git init # !git status # !git add . # !git commit -m &#39;commit&#39; # !pip install awscli # !aws configure # !sam build &amp;&amp; sam deploy # !pip install flask-lambda-python36 # !aws ec2 create-key-pair --key-name MyKeyPair --query &#39;KeyMaterial&#39; --output text &gt; MyKeyPair.pem # !chmod 400 MyKeyPair.pem # !aws ec2 describe-instances --filters &quot;Name=instance-type,Values=t2.micro&quot; --query &quot;Reservations[].Instances[].InstanceId&quot; # !ssh -i MyKeyPair.pem ec2-user@ec2-50-xx-xx-xxx.compute-1.amazonaws.com # !git clone https://github.com/aws/aws-elastic-beanstalk-cli-setup.git # build-essential zlib1g-dev libssl-dev libncurses-dev libffi-dev libsqlite3-dev libreadline-dev libbz2-dev # !pip install awscli # !pip install awsebcli # !aws configure # !mkdir eb-flask # %cd eb-flask # python --version # pip install awsebcli --upgrade --user # eb --version # mkdir eb-flask # cd eb-flask # pip install virtualenv # virtualenv virt # source virt/bin/activate # pip install flask # pip freeze # pip freeze &gt; requirements.txt # python application.py # eb init -p python-3.6 my-app --region us-east-1 # pip install zip-files . . .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/similarity%20vision/2021/04/27/image-similarity-recommendations.html",
            "relUrl": "/similarity%20vision/2021/04/27/image-similarity-recommendations.html",
            "date": " ‚Ä¢ Apr 27, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Sequence Aware Recommender Systems",
            "content": "Data loading . !mkdir datasets &amp;&amp; cd datasets &amp;&amp; wget https://raw.githubusercontent.com/mquad/sars_tutorial/master/datasets/sessions.zip &amp;&amp; unzip sessions.zip . dataset = create_seq_db_filter_top_k(path=dataset_path, topk=1000, last_months=1) . dataset.head() . session_id sequence ts user_id . 0 357 | [793, 3489] | 1421003874 | 4296 | . 1 359 | [1762] | 1421018535 | 4296 | . 2 394 | [1256] | 1421007470 | 30980 | . 3 4127 | [1948, 1364, 2060, 1115, 6488, 2060] | 1421416896 | 28117 | . 4 6400 | [687, 1394] | 1420807778 | 35247 | . Data statistics . Number of items: 1000 Number of users: 4165 Number of sessions: 6765 Session length: Average: 4.29 Median: 3.0 Min: 1 Max: 148 Sessions per user: Average: 1.62 Median: 1.0 Min: 1 Max: 13 Most popular items: [(&#39;443&#39;, 207), (&#39;1065&#39;, 155), (&#39;67&#39;, 146), (&#39;2308&#39;, 138), (&#39;658&#39;, 131)] . Split the dataset . def random_holdout(dataset, perc=0.8, seed=1234): &quot;&quot;&quot; Split sequence dataset randomly :param dataset: the sequence dataset :param perc: the training percentange :param seed: the random seed :return: the training and test splits &quot;&quot;&quot; dataset = dataset.sample(frac=1, random_state=seed) nseqs = len(dataset) train_size = int(nseqs * perc) # split data according to the shuffled index and the holdout size train_split = dataset[:train_size] test_split = dataset[train_size:] return train_split, test_split def temporal_holdout(dataset, ts_threshold): &quot;&quot;&quot; Split sequence dataset using timestamps :param dataset: the sequence dataset :param ts_threshold: the timestamp from which test sequences will start :return: the training and test splits &quot;&quot;&quot; train = dataset.loc[dataset[&#39;ts&#39;] &lt; ts_threshold] test = dataset.loc[dataset[&#39;ts&#39;] &gt;= ts_threshold] train, test = clean_split(train, test) return train, test def last_session_out_split(data, user_key=&#39;user_id&#39;, session_key=&#39;session_id&#39;, time_key=&#39;ts&#39;): &quot;&quot;&quot; Assign the last session of every user to the test set and the remaining ones to the training set &quot;&quot;&quot; sessions = data.sort_values(by=[user_key, time_key]).groupby(user_key)[session_key] last_session = sessions.last() train = data[~data.session_id.isin(last_session.values)].copy() test = data[data.session_id.isin(last_session.values)].copy() train, test = clean_split(train, test) return train, test def clean_split(train, test): &quot;&quot;&quot; Remove new items from the test set. :param train: The training set. :param test: The test set. :return: The cleaned training and test sets. &quot;&quot;&quot; train_items = set() train[&#39;sequence&#39;].apply(lambda seq: train_items.update(set(seq))) test[&#39;sequence&#39;] = test[&#39;sequence&#39;].apply(lambda seq: [it for it in seq if it in train_items]) return train, test def balance_dataset(x, y): number_of_elements = y.shape[0] nnz = set(find(y)[0]) zero = set(range(number_of_elements)).difference(nnz) max_samples = min(len(zero), len(nnz)) nnz_indices = random.sample(nnz, max_samples) zero_indeces = random.sample(zero, max_samples) indeces = nnz_indices + zero_indeces return x[indeces, :], y[indeces, :] . . For simplicity, let&#39;s split the dataset by assigning the last session of every user to the test set, and all the previous ones to the training set. . train_data, test_data = last_session_out_split(dataset) print(&quot;Train sessions: {} - Test sessions: {}&quot;.format(len(train_data), len(test_data))) . Train sessions: 2600 - Test sessions: 4165 . Fitting the recommender . Algorithm summary . . class ISeqRecommender(object): &quot;&quot;&quot;Abstract Recommender class&quot;&quot;&quot; logging.basicConfig(level=logging.INFO, format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39;) logger = logging.getLogger() def __init__(self): super(ISeqRecommender, self).__init__() def fit(self, train_data): pass def recommend(self, user_profile, user_id=None): &quot;&quot;&quot; Given the user profile return a list of recommendation :param user_profile: the user profile as a list of item identifiers :param user_id: (optional) the user id :return: list of recommendations e.g. [([2], 0.875), ([6], 1.0)] &quot;&quot;&quot; pass @staticmethod def get_recommendation_list(recommendation): return list(map(lambda x: x[0], recommendation)) @staticmethod def get_recommendation_confidence_list(recommendation): return list(map(lambda x: x[1], recommendation)) def activate_debug_print(self): self.logger.setLevel(logging.DEBUG) def deactivate_debug_print(self): self.logger.setLevel(logging.INFO) . . Popularity recommender . PopularityRecommender simply recommends items ordered by their popularity in the training set. . It doesn&#39;t have any hyper-parameter, so we can move on! . class PopularityRecommender(ISeqRecommender): def __init__(self): super(PopularityRecommender, self).__init__() def fit(self, train_data): sequences = train_data[&#39;sequence&#39;].values count_dict = {} for s in sequences: for item in s: if item not in count_dict: count_dict[item] = 1 else: count_dict[item] += 1 self.top = sorted(count_dict.items(), key=operator.itemgetter(1), reverse=True) self.top = [([x[0]], x[1]) for x in self.top] def recommend(self, user_profile, user_id=None): return self.top def get_popular_list(self): return self.top . . poprecommender = PopularityRecommender() poprecommender.fit(train_data) . Frequent Sequential Patterns . . This algorithm extract Frequent Sequential Patterns from all the training sequences. Patterns are having support lower than minsup are discarded (support = # occurrences of a pattern in the traning data). Recommendations are then generated by looking for patterns having a prefix corresponding to the last [max_context, min_context] elements in the user profile, taken in order. Matches are then sorted by decreasing confidence score (ratio between the support of the matched rule and the support of the context). Matches having confidence below minconf are discarded. . The class FSMRecommender has the following initialization hyper-parameters: . minsup: the minimum support threshold. It is interpreted as relative count if in [0-1], otherwise as an absolute count. NOTE: Relative count required for training with SPFM (faster). | minconf: the minimum confidence threshold. Use to filter irrelevent recommendations. | max_context: the maximum number of items in the user profile (starting from the last) that will be used for lookup in the database of frequent sequences. | min_context: the minimum number of items in the user profile (starting from the last) that will be used for lookup in the database of frequent sequences. | spmf_path: path to SPMF jar file. If provided, SPFM library will be used for pattern extraction (algorithm: Prefix Span). Otherwise, use pymining, which can be significantly slower depending on the sequence database size. | db_path: path to the sequence database file | . class FSMRecommender(ISeqRecommender): &quot;&quot;&quot;Frequent Sequence Mining recommender&quot;&quot;&quot; def __init__(self, minsup, minconf, max_context=1, min_context=1, spmf_path=None, db_path=None): &quot;&quot;&quot; :param minsup: the minimum support threshold. It is interpreted as relative count if in [0-1], otherwise as an absolute count. NOTE: Relative count required for training with SPFM (faster). :param minconf: the minimum confidence threshold. :param max_context: (optional) the maximum number of items in the user profile (starting from the last) that will be used for lookup in the database of frequent sequences. :param min_context: (optional) the minimum number of items in the user profile (starting from the last) that will be used for lookup in the database of frequent sequences. :param spmf_path: (optional) path to SPMF jar file. If provided, SPFM library will be used for pattern extraction (algorithm: Prefix Span). Otherwise, use pymining, which can be significantly slower depending on the sequence database size. :param db_path: (optional) path to the sequence database file &quot;&quot;&quot; super(FSMRecommender, self).__init__() self.minsup = minsup self.minconf = minconf self.max_context = max_context self.min_context = min_context self.recommendation_length = 1 self.db_path = db_path self.spmf_path = spmf_path self.spmf_algorithm = &quot;PrefixSpan&quot; self.output_path = &quot;tmp/tmp_output.txt&quot; def __str__(self): return &#39;FreqSeqMiningRecommender: &#39; &#39;minsup={minsup}, &#39; &#39;minconf={minconf}, &#39; &#39;max_context={max_context}, &#39; &#39;min_context={min_context}, &#39; &#39;spmf_path={spmf_path}, &#39; &#39;db_path={db_path}&#39;.format(**self.__dict__) def fit(self, train_data=None): &quot;&quot;&quot; Fit the model :param train_data: (optional) DataFrame with the training sequences, which must be assigned to column &quot;sequence&quot;. If None, run FSM using SPFM over the sequence database stored in `self.db_path`. Otherwise, run FSM using `pymining.seqmining` (slower). &quot;&quot;&quot; if train_data is None: if self.spmf_path is None or self.db_path is None: raise ValueError(&quot;You should set db_path and spfm_path before calling fit() without arguments.&quot;) self.logger.info(&#39;Using SPFM (Java) for Frequent Sequence Mining&#39;) if 0 &lt;= self.minsup &lt;= 1: percentage_min_sup = self.minsup * 100 else: raise NameError(&quot;SPMF only accepts 0&lt;=minsup&lt;=1&quot;) # call spmf command = &#39; &#39;.join([self.spmf_algorithm, self.db_path, self.output_path, str(percentage_min_sup) + &#39;%&#39;]) callSPMF(self.spmf_path, command) # parse back output from text file self._parse_spfm_output() else: # use pymining self.logger.info(&#39;Using pymining.seqmining (python) for Frequent Sequence Mining&#39;) sequences = train_data[&#39;sequence&#39;].values msup = int(self.minsup * len(sequences)) if 0 &lt;= self.minsup &lt;= 1 else self.minsup self.logger.info(&#39;Mining frequent sequences (minsup={})&#39;.format(msup)) self.freq_seqs = seqmining.freq_seq_enum(sequences, msup) self.logger.info(&#39;{} frequent sequences found&#39;.format(len(self.freq_seqs))) self.logger.info(&#39;Building the prefix tree&#39;) self.tree = SmartTree() self.root_node = self.tree.set_root() for pattern, support in self.freq_seqs: if len(pattern) == 1: # add node to root self.tree.create_node(pattern[0], parent=self.root_node, data={&quot;support&quot;: support}) elif len(pattern) &gt; 1: # add entire path starting from root self.tree.add_path(self.root_node, pattern, support) else: raise ValueError(&#39;Frequent sequence of length 0&#39;) self.logger.info(&#39;Training completed&#39;) def recommend(self, user_profile, user_id=None): n = len(user_profile) c = min(n, self.max_context) match = [] # iterate over decreasing context lengths until a match with sufficient confidence is found while not match and c &gt;= self.min_context: q = user_profile[n - c:n] match = self._find_match(q, self.recommendation_length) c -= 1 return match def _find_match(self, context, recommendation_length): # search context lastNode = self.tree.find_path(self.root_node, context) if lastNode == -1: return [] else: # context matched context_support = self.tree[lastNode].data[&#39;support&#39;] children = self.tree[lastNode].fpointer if not children: return [] # find all path of length recommendation_length from match paths = self.tree.find_n_length_paths(lastNode, recommendation_length) return sorted(self._filter_confidence(context_support, paths), key=lambda x: x[1], reverse=True) def _filter_confidence(self, context_support, path_list): goodPaths = [] for p in path_list: confidence = self.tree[p[len(p) - 1]].data[&#39;support&#39;] / float(context_support) if confidence &gt;= self.minconf: goodPaths.append((self.tree.get_nodes_tag(p), confidence)) return goodPaths def _set_tree_debug_only(self, tree): self.tree = tree self.root_node = tree.get_root() def get_freq_seqs(self): return self.freq_seqs def get_sequence_tree(self): return self.tree def show_tree(self): self.tree.show() def get_confidence_list(self, recommendation): return list(map(lambda x: x[1], recommendation)) def _parse_spfm_output(self): with open(self.output_path, &#39;r&#39;) as fin: self.freq_seqs = [] for line in fin: pieces = line.split(&#39;#SUP: &#39;) support = pieces[1].strip() items = pieces[0].split(&#39; &#39;) seq = tuple(x for x in items if x != &#39;&#39; and x != &#39;-1&#39;) seq_and_support = (seq, int(support)) self.freq_seqs.append(seq_and_support) . . db_path = &#39;tmp/sequences.txt&#39; sequences_to_spfm_format(train_data[&#39;sequence&#39;], tmp_path=db_path) # then we instantiate and fit the recommender fsmrecommender = FSMRecommender(minsup=0.002, minconf=0.1, min_context=1, max_context=10, spmf_path=&#39;spmf/spmf.jar&#39;, db_path=db_path) # calling fit() without arguments to use SPFM and the sequences stored in db_path fsmrecommender.fit() . 2021-04-25 13:57:15,144 - INFO - Using SPFM (Java) for Frequent Sequence Mining . java -jar spmf/spmf.jar run PrefixSpan tmp/sequences.txt tmp/tmp_output.txt 0.2% . 2021-04-25 13:57:16,823 - INFO - 66730 frequent sequences found 2021-04-25 13:57:16,827 - INFO - Building the prefix tree 2021-04-25 13:57:29,086 - INFO - Training completed . Markov Chains . . Here we fit the recommedation algorithm over the sessions in the training set. This recommender is based on the MarkovChainRecommender implemented from: . Shani, Guy, David Heckerman, and Ronen I. Brafman. &quot;An MDP-based recommender system.&quot; Journal of Machine Learning Research 6, no. Sep (2005): 1265-1295. Chapter 3-4 . This recommender computes the item transition matrices for any Markov Chain having order in [min_order, max_order]. Each individual Markov Chain model employes some heristics like skipping or clustering to deal better with data sparsity. Recommendations are generated by sorting items by their transition probability to being next, given the user profile. The scores coming from different MC are weighted inversely wrt to their order. . The class MixedMarkovChainRecommender has the following initialization hyper-parameters: . min_order: the minimum order of the Mixed Markov Chain | max_order: the maximum order of the Mixed Markov Chain | . class MarkovChainRecommender(ISeqRecommender): &quot;&quot;&quot; Implementation from Shani, Guy, David Heckerman, and Ronen I. Brafman. &quot;An MDP-based recommender system.&quot; Journal of Machine Learning Research 6, no. Sep (2005): 1265-1295. Chapter 3-4 &quot;&quot;&quot; logging.basicConfig(level=logging.INFO, format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39;) def __init__(self, order): &quot;&quot;&quot; :param order: the order of the Markov Chain &quot;&quot;&quot; super(MarkovChainRecommender, self).__init__() self.order = order def fit(self, train_data): sequences = train_data[&#39;sequence&#39;].values logging.info(&#39;Building Markov Chain model with k = &#39; + str(self.order)) logging.info(&#39;Adding nodes&#39;) self.tree, self.count_dict, self.G = add_nodes_to_graph(sequences, self.order) logging.info(&#39;Adding edges&#39;) self.G = add_edges(self.tree, self.count_dict, self.G, self.order) logging.info(&#39;Applying skipping&#39;) self.G = apply_skipping(self.G, self.order, sequences) logging.info(&#39;Applying clustering&#39;) logging.info(&#39;{} states in the graph&#39;.format(len(self.G.nodes()))) self.G, _, _ = apply_clustering(self.G) # drop not useful resources self.tree = None self.count_dict = None gc.collect() def recommend(self, user_profile, user_id=None): # if the user profile is longer than the markov order, chop it keeping recent history state = tuple(user_profile[-self.order:]) # see if graph has that state recommendations = [] if self.G.has_node(state): # search for recommendations in the forward star rec_dict = {} for u, v in self.G.out_edges_iter([state]): lastElement = tuple(v[-1:]) if lastElement in rec_dict: rec_dict[lastElement] += self.G[u][v][&#39;count&#39;] else: rec_dict[lastElement] = self.G[u][v][&#39;count&#39;] for k, v in rec_dict.items(): recommendations.append((list(k), v)) return recommendations def _set_graph_debug(self, G): self.G = G . . class MixedMarkovChainRecommender(ISeqRecommender): &quot;&quot;&quot; Creates markov models with different values of k, and return recommendation by weighting the list of recommendation of each model. Reference: Shani, Guy, David Heckerman, and Ronen I. Brafman. &quot;An MDP-based recommender system.&quot; Journal of Machine Learning Research 6, no. Sep (2005): 1265-1295. Chapter 3-4 &quot;&quot;&quot; logging.basicConfig(level=logging.INFO, format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39;) recommenders = {} def __init__(self, min_order=1, max_order=1): &quot;&quot;&quot; :param min_order: the minimum order of the Mixed Markov Chain :param max_order: the maximum order of the Mixed Markov Chain &quot;&quot;&quot; super(MixedMarkovChainRecommender, self).__init__() self.min_order = min_order self.max_order = max_order # define the models for i in range(self.min_order, self.max_order + 1): self.recommenders[i] = MarkovChainRecommender(i) def fit(self, user_profile): for order in self.recommenders: self.recommenders[order].fit(user_profile) def recommend(self, user_profile, user_id=None): rec_dict = {} recommendations = [] sum_of_weights = 0 for order, r in self.recommenders.items(): rec_list = r.recommend(user_profile) sum_of_weights += 1 / order for i in rec_list: if tuple(i[0]) in rec_dict: rec_dict[tuple(i[0])] += 1 / order * i[1] else: rec_dict[tuple(i[0])] = 1 / order * i[1] for k, v in rec_dict.items(): recommendations.append((list(k), v / sum_of_weights)) return recommendations def _set_model_debug(self, recommender, order): self.recommenders[order] = recommender . . mmcrecommender = MixedMarkovChainRecommender(min_order=1, max_order=1) mmcrecommender.fit(train_data) . 2021-04-25 13:57:40,210 - INFO - Building Markov Chain model with k = 1 2021-04-25 13:57:40,213 - INFO - Adding nodes 2021-04-25 13:57:40,499 - INFO - Adding edges 2021-04-25 13:57:58,752 - INFO - Applying skipping 2021-04-25 13:57:58,973 - INFO - Applying clustering 2021-04-25 13:57:58,974 - INFO - 999 states in the graph . FPMC . Here we fit the recommedation algorithm over the sessions in the training set. This recommender is based on the following paper: . Rendle, S., Freudenthaler, C., &amp; Schmidt-Thieme, L. (2010). Factorizing personalized Markov chains for next-basket recommendation. Proceedings of the 19th International Conference on World Wide Web - WWW ‚Äô10, 811 . In short, FPMC factorizes a personalized order-1 transition tensor using Tensor Factorization with pairwise loss function akin to BPR (Bayesian Pairwise Ranking). . TF allows to impute values for the missing transitions between items for each user. For this reason, FPMC can be used for generating personalized recommendations in session-aware recommenders as well. . In this notebook, you will be able to change the number of latent factors and a few other learning hyper-parameters and see the impact on the recommendation quality. . The class FPMCRecommender has the following initialization hyper-parameters: . n_factor: (optional) the number of latent factors | learn_rate: (optional) the learning rate | regular: (optional) the L2 regularization coefficient | n_epoch: (optional) the number of training epochs | n_neg: (optional) the number of negative samples used in BPR learning | . class FPMCRecommender(ISeqRecommender): &quot;&quot;&quot; Implementation of Rendle, S., Freudenthaler, C., &amp; Schmidt-Thieme, L. (2010). Factorizing personalized Markov chains for next-basket recommendation. Proceedings of the 19th International Conference on World Wide Web - WWW ‚Äô10, 811 Based on the implementation available at https://github.com/khesui/FPMC &quot;&quot;&quot; def __init__(self, n_factor=32, learn_rate=0.01, regular=0.001, n_epoch=15, n_neg=10): &quot;&quot;&quot; :param n_factor: (optional) the number of latent factors :param learn_rate: (optional) the learning rate :param regular: (optional) the L2 regularization coefficient :param n_epoch: (optional) the number of training epochs :param n_neg: (optional) the number of negative samples used in BPR learning &quot;&quot;&quot; super(FPMCRecommender, self).__init__() self.n_epoch = n_epoch self.n_neg = n_neg self.n_factor = n_factor self.learn_rate = learn_rate self.regular = regular def __str__(self): return &#39;FPMCRecommender(n_epoch={n_epoch}, &#39; &#39;n_neg={n_neg}, &#39; &#39;n_factor={n_factor}, &#39; &#39;learn_rate={learn_rate}, &#39; &#39;regular={regular})&#39;.format(**self.__dict__) def fit(self, train_data): self._declare(train_data) train_data_supervised = [] for i, row in train_data.iterrows(): u = self.user_mapping[row[&#39;user_id&#39;]] seq = [] if len(row[&#39;sequence&#39;]) &gt; 1: # cannot use sequences with length 1 for supervised learning for item in row[&#39;sequence&#39;]: i = self.item_mapping[item] seq.append(i) train_data_supervised.append((u, seq[len(seq) - 1], seq[:len(seq) - 1])) self.fpmc = FPMC(n_user=len(self.user_mapping), n_item=len(self.item_mapping), n_factor=self.n_factor, learn_rate=self.learn_rate, regular=self.regular) self.fpmc.user_set = set(self.user_mapping.values()) self.fpmc.item_set = set(self.item_mapping.values()) self.fpmc.init_model() self.fpmc.learnSBPR_FPMC(train_data_supervised, n_epoch=self.n_epoch, neg_batch_size=self.n_neg) def recommend(self, user_profile, user_id=None): context = [] for item in user_profile: context.append(self.item_mapping[item]) items, scores = self.fpmc.evaluation_recommender(self.user_mapping[user_id], context) recommendations = [] for i, it in enumerate(items): recommendations.append(([self.reverse_item_mapping[it]], scores[i])) return recommendations def _declare(self, data): self.user_mapping = {} self.item_mapping = {} self.reverse_item_mapping = {} user_counter = 0 item_counter = 0 for i, row in data.iterrows(): if row[&#39;user_id&#39;] not in self.user_mapping: self.user_mapping[row[&#39;user_id&#39;]] = user_counter user_counter += 1 for item in row[&#39;sequence&#39;]: if item not in self.item_mapping: self.item_mapping[item] = item_counter self.reverse_item_mapping[item_counter] = item item_counter += 1 . . fpmcrecommender = FPMCRecommender(n_factor=16, n_epoch=5) fpmcrecommender.fit(train_data) . 2021-04-25 13:59:28,886 - INFO - epoch 0 done 2021-04-25 13:59:28,992 - INFO - epoch 1 done 2021-04-25 13:59:29,107 - INFO - epoch 2 done 2021-04-25 13:59:29,217 - INFO - epoch 3 done 2021-04-25 13:59:29,330 - INFO - epoch 4 done . Prod2vec . . Here we fit the recommedation algorithm over the sessions in the training set. . This is simplified implementation of the following: . Grbovic, Mihajlo, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati, Jaikit Savla, Varun Bhagwan, and Doug Sharp. &quot;E-commerce in your inbox: Product recommendations at scale.&quot; In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1809-1818. ACM, 2015. . This implementation uses the gensim implementation of Word2Vec to compute item embeddings using the skip-gram model. . Recommendations are generated by returning the k-nearest neighbors of the last items in the user profile, whose relevance is weighted using a simple exponential decay (the last item in the user profile is the most relevant one, and the first item the least relevant). . The original paper contains other variants of this algorithm (namely bagged-prod2vec and prod2vec-cluster) which are not subject of this tutorial. . The class Prod2VecRecommender has the following initialization hyper-parameters: . min_count: the minimum item frequency. Items less frequent that min_count will be pruned | size: the size of the embeddings | window: the size of the context window | decay_alpha: the exponential decay factor used to discount the similarity scores for items back in the user profile. Lower values mean higher discounting of past user interactions. Allows values in [0-1] | workers: the number of threads used for training | . class Prod2VecRecommender(ISeqRecommender): &quot;&quot;&quot; Implementation of the Prod2Vec skipgram model from Grbovic Mihajlo, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati, Jaikit Savla, Varun Bhagwan, and Doug Sharp. &quot;E-commerce in your inbox: Product recommendations at scale.&quot; In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1809-1818. ACM, 2015. &quot;&quot;&quot; logging.basicConfig(level=logging.INFO, format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39;) def __init__(self, min_count=2, size=100, window=5, decay_alpha=0.9, workers=4): &quot;&quot;&quot; :param min_count: (optional) the minimum item frequency. Items less frequent that min_count will be pruned :param size: (optional) the size of the embeddings :param window: (optional) the size of the context window :param decay_alpha: (optional) the exponential decay factor used to discount the similarity scores for items back in the user profile. Lower values mean higher discounting of past user interactions. Allows values in [0-1]. :param workers: (optional) the number of threads used for training &quot;&quot;&quot; super(Prod2VecRecommender, self).__init__() self.min_count = min_count self.size = size self.window = window self.decay_alpha = decay_alpha self.workers = workers def __str__(self): return &#39;Prod2VecRecommender(min_count={min_count}, &#39; &#39;size={size}, &#39; &#39;window={window}, &#39; &#39;decay_alpha={decay_alpha}, &#39; &#39;workers={workers})&#39;.format(**self.__dict__) def fit(self, train_data): sequences = train_data[&#39;sequence&#39;].values self.model = gensim.models.Word2Vec(sequences, min_count=self.min_count, window=self.window, hs=1, size=self.size, sg=1, workers=self.workers) def recommend(self, user_profile, user_id=None): user_profile = list(map(str, user_profile)) rec = [] try: # iterate the user profile backwards for i, item in enumerate(user_profile[::-1]): ms = self.model.most_similar(positive=item) # apply exponential decay to the similarity scores decay = self.decay_alpha ** i ms = [(x[0], decay * x[1]) for x in ms] rec.extend(ms) # sort items by similarity score rec = sorted(rec, key=lambda x: -x[1]) except KeyError: rec = [] return [([x[0]], x[1]) for x in rec] . . p2vrecommender = Prod2VecRecommender(min_count=2, size=50, window=5, decay_alpha=0.9, workers=4) p2vrecommender.fit(train_data) . Session based RNN . Here we fit the recommedation algorithm over the sessions in the training set. . This is a simplified interface to Recurrent Neural Network models for Session-based recommendation. Based on the following two papers: . Recurrent Neural Networks with Top-k Gains for Session-based Recommendations, Hidasi and Karatzoglou, CIKM 2018 | Personalizing Session-based Recommendation with Hierarchical Recurrent Neural Networks, Quadrana et al, Recsys 2017 | . In this notebook, we will consider the session-based (non-personalized) version of the algorithm. Here&#39;s a schematic representation of the model: . . Each item in the current user session is first encoded either using 1-hot encoding or a dense embedding vector. The item representation is then forwarded to one or more Gated Reucurrent Unit (GRU) layers, which &quot;mix&quot; the information coming from the past steps of the sequence with the representation of the current item. The last hidden state of the network is finally use to compute the likelihood scores for the next items by using one out of several loss functions (e.g. cross-entropy, BPR, TOP1, BPR-max, TOP1-max, etc.). . For simplicity, we only support 1-hot encoded inputs and the BPR-max loss function here. . The hyper-parameters of the model are: . session_layers: number of units per layer used at session level. It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks. | user_layers: number of units per layer used at user level. Required only by personalized models. (None in this case) | batch_size: the mini-batch size used in training | learning_rate: the learning rate used in training (Adagrad optimized) | momentum: the momentum coefficient used in training | dropout: it&#39;s a float value for the hidden-layer(s) dropout. | epochs: number of training epochs | personalized: whether to train a personalized model using the HRNN model (False in this case). | . NOTE: GRU4Rec originally has many more hyper-parameters. Going through all of them is out from the scope of this tutorial, but we suggest to check-out the original source code here if you are interested. . class GRU4Rec: &#39;&#39;&#39; GRU4Rec(loss=&#39;bpr-max&#39;, final_act=&#39;elu-1&#39;, hidden_act=&#39;tanh&#39;, layers=[100], n_epochs=10, batch_size=32, dropout_p_hidden=0.0, dropout_p_embed=0.0, learning_rate=0.1, momentum=0.0, lmbd=0.0, embedding=0, n_sample=2048, sample_alpha=0.75, smoothing=0.0, constrained_embedding=False, adapt=&#39;adagrad&#39;, adapt_params=[], grad_cap=0.0, bpreg=1.0, sigma=0.0, init_as_normal=False, train_random_order=False, time_sort=True, session_key=&#39;SessionId&#39;, item_key=&#39;ItemId&#39;, time_key=&#39;Time&#39;) Initializes the network. Parameters -- loss : &#39;top1&#39;, &#39;bpr&#39;, &#39;cross-entropy&#39;, &#39;xe_logit&#39;, &#39;top1-max&#39;, &#39;bpr-max&#39; selects the loss function (default : &#39;bpr-max&#39;) final_act : &#39;softmax&#39;, &#39;linear&#39;, &#39;relu&#39;, &#39;tanh&#39;, &#39;softmax_logit&#39;, &#39;leaky-&lt;X&gt;&#39;, &#39;elu-&lt;X&gt;&#39;, &#39;selu-&lt;X&gt;-&lt;Y&gt;&#39; selects the activation function of the final layer, &lt;X&gt; and &lt;Y&gt; are the parameters of the activation function (default : &#39;elu-1&#39;) hidden_act : &#39;linear&#39;, &#39;relu&#39;, &#39;tanh&#39;, &#39;leaky-&lt;X&gt;&#39;, &#39;elu-&lt;X&gt;&#39;, &#39;selu-&lt;X&gt;-&lt;Y&gt;&#39; selects the activation function on the hidden states, &lt;X&gt; and &lt;Y&gt; are the parameters of the activation function (default : &#39;tanh&#39;) layers : list of int values list of the number of GRU units in the layers (default : [100]) n_epochs : int number of training epochs (default: 10) batch_size : int size of the minibacth, also effect the number of negative samples through minibatch based sampling (default: 32) dropout_p_hidden : float probability of dropout of hidden units (default: 0.0) dropout_p_embed : float probability of dropout of the input units, applicable only if embeddings are used (default: 0.0) learning_rate : float learning rate (default: 0.05) momentum : float if not zero, Nesterov momentum will be applied during training with the given strength (default: 0.0) lmbd : float coefficient of the L2 regularization (default: 0.0) embedding : int size of the embedding used, 0 means not to use embedding (default: 0) n_sample : int number of additional negative samples to be used (besides the other examples of the minibatch) (default: 2048) sample_alpha : float the probability of an item used as an additional negative sample is supp^sample_alpha (default: 0.75) (e.g.: sample_alpha=1 --&gt; popularity based sampling; sample_alpha=0 --&gt; uniform sampling) smoothing : float (only works with cross-entropy and xe_logit losses) if set to non-zero class labels are smoothed with this value, i.e. the expected utput is (e/N, ..., e/N, 1-e+e/N, e/N, ..., e/N) instead of (0, ..., 0, 1, 0, ..., 0), where N is the number of outputs and e is the smoothing value (default: 0.0) constrained_embedding : bool if True, the output weight matrix is also used as input embedding (default: False) adapt : None, &#39;adagrad&#39;, &#39;rmsprop&#39;, &#39;adam&#39;, &#39;adadelta&#39; sets the appropriate learning rate adaptation strategy, use None for standard SGD (default: &#39;adagrad&#39;) adapt_params : list parameters for the adaptive learning methods (default: []) grad_cap : float clip gradients that exceede this value to this value, 0 means no clipping (default: 0.0) bpreg : float score regularization coefficient for the BPR-max loss function (default: 1.0) sigma : float &quot;width&quot; of initialization; either the standard deviation or the min/max of the init interval (with normal and uniform initializations respectively); 0 means adaptive normalization (sigma depends on the size of the weight matrix); (default: 0.0) init_as_normal : boolean False: init from uniform distribution on [-sigma,sigma]; True: init from normal distribution N(0,sigma); (default: False) train_random_order : boolean whether to randomize the order of sessions in each epoch (default: False) time_sort : boolean whether to ensure the the order of sessions is chronological (default: True) session_key : string header of the session ID column in the input file (default: &#39;SessionId&#39;) item_key : string header of the item ID column in the input file (default: &#39;ItemId&#39;) time_key : string header of the timestamp column in the input file (default: &#39;Time&#39;) &#39;&#39;&#39; def __init__(self, loss=&#39;bpr-max&#39;, final_act=&#39;linear&#39;, hidden_act=&#39;tanh&#39;, layers=[100], n_epochs=10, batch_size=32, dropout_p_hidden=0.0, dropout_p_embed=0.0, learning_rate=0.1, momentum=0.0, lmbd=0.0, embedding=0, n_sample=2048, sample_alpha=0.75, smoothing=0.0, constrained_embedding=False, adapt=&#39;adagrad&#39;, adapt_params=[], grad_cap=0.0, bpreg=1.0, sigma=0.0, init_as_normal=False, train_random_order=False, time_sort=True, session_key=&#39;SessionId&#39;, item_key=&#39;ItemId&#39;, time_key=&#39;Time&#39;): self.layers = layers self.n_epochs = n_epochs self.batch_size = batch_size self.dropout_p_hidden = dropout_p_hidden self.dropout_p_embed = dropout_p_embed self.learning_rate = learning_rate self.adapt_params = adapt_params self.momentum = momentum self.sigma = sigma self.init_as_normal = init_as_normal self.session_key = session_key self.item_key = item_key self.time_key = time_key self.grad_cap = grad_cap self.bpreg = bpreg self.train_random_order = train_random_order self.lmbd = lmbd self.embedding = embedding self.constrained_embedding = constrained_embedding self.time_sort = time_sort self.adapt = adapt self.loss = loss self.set_loss_function(self.loss) self.final_act = final_act self.set_final_activation(self.final_act) self.hidden_act = hidden_act self.set_hidden_activation(self.hidden_act) self.n_sample = n_sample self.sample_alpha = sample_alpha self.smoothing = smoothing def set_loss_function(self, loss): if loss == &#39;cross-entropy&#39;: self.loss_function = self.cross_entropy elif loss == &#39;bpr&#39;: self.loss_function = self.bpr elif loss == &#39;bpr-max&#39;: self.loss_function = self.bpr_max elif loss == &#39;top1&#39;: self.loss_function = self.top1 elif loss == &#39;top1-max&#39;: self.loss_function = self.top1_max elif loss == &#39;xe_logit&#39;: self.loss_function = self.cross_entropy_logits else: raise NotImplementedError def set_final_activation(self, final_act): if final_act == &#39;linear&#39;: self.final_activation = self.linear elif final_act == &#39;relu&#39;: self.final_activation = self.relu elif final_act == &#39;softmax&#39;: self.final_activation = self.softmax elif final_act == &#39;tanh&#39;: self.final_activation = self.tanh elif final_act == &#39;softmax_logit&#39;: self.final_activation = self.softmax_logit elif final_act.startswith(&#39;leaky-&#39;): self.final_activation = self.LeakyReLU(float(final_act.split(&#39;-&#39;)[1])).execute elif final_act.startswith(&#39;elu-&#39;): self.final_activation = self.Elu(float(final_act.split(&#39;-&#39;)[1])).execute elif final_act.startswith(&#39;selu-&#39;): self.final_activation = self.Selu(*[float(x) for x in final_act.split(&#39;-&#39;)[1:]]).execute else: raise NotImplementedError def set_hidden_activation(self, hidden_act): if hidden_act == &#39;relu&#39;: self.hidden_activation = self.relu elif hidden_act == &#39;tanh&#39;: self.hidden_activation = self.tanh elif hidden_act == &#39;linear&#39;: self.hidden_activation = self.linear elif hidden_act.startswith(&#39;leaky-&#39;): self.hidden_activation = self.LeakyReLU(float(hidden_act.split(&#39;-&#39;)[1])).execute elif hidden_act.startswith(&#39;elu-&#39;): self.hidden_activation = self.Elu(float(hidden_act.split(&#39;-&#39;)[1])).execute elif hidden_act.startswith(&#39;selu-&#39;): self.hidden_activation = self.Selu(*[float(x) for x in hidden_act.split(&#39;-&#39;)[1:]]).execute else: raise NotImplementedError def set_params(self, **kvargs): maxk_len = np.max([len(x) for x in kvargs.keys()]) maxv_len = np.max([len(x) for x in kvargs.values()]) for k, v in kvargs.items(): if not hasattr(self, k): print(&#39;Unkown attribute: {}&#39;.format(k)) raise NotImplementedError else: if k == &#39;adapt_params&#39;: v = [float(l) for l in v.split(&#39;/&#39;)] elif type(getattr(self, k)) == list: v = [int(l) for l in v.split(&#39;/&#39;)] if type(getattr(self, k)) == bool: if v == &#39;True&#39; or v == &#39;1&#39;: v = True elif v == &#39;False&#39; or v == &#39;0&#39;: v = False else: print(&#39;Invalid value for boolean parameter: {}&#39;.format(v)) raise NotImplementedError setattr(self, k, type(getattr(self, k))(v)) if k == &#39;loss&#39;: self.set_loss_function(self.loss) if k == &#39;final_act&#39;: self.set_final_activation(self.final_act) if k == &#39;hidden_act&#39;: self.set_hidden_activation(self.hidden_act) print(&#39;SET {}{}TO {}{}(type: {})&#39;.format(k, &#39; &#39; * (maxk_len - len(k) + 3), getattr(self, k), &#39; &#39; * (maxv_len - len(str(getattr(self, k))) + 3), type(getattr(self, k)))) ######################ACTIVATION FUNCTIONS##################### def linear(self, X): return X def tanh(self, X): return T.tanh(X) def softmax(self, X): e_x = T.exp(X - X.max(axis=1).dimshuffle(0, &#39;x&#39;)) return e_x / e_x.sum(axis=1).dimshuffle(0, &#39;x&#39;) def softmax_logit(self, X): X = X - X.max(axis=1).dimshuffle(0, &#39;x&#39;) return T.log(T.exp(X).sum(axis=1).dimshuffle(0, &#39;x&#39;)) - X def softmax_neg(self, X): hm = 1.0 - T.eye(*X.shape) X = X * hm e_x = T.exp(X - X.max(axis=1).dimshuffle(0, &#39;x&#39;)) * hm return e_x / e_x.sum(axis=1).dimshuffle(0, &#39;x&#39;) def relu(self, X): return T.maximum(X, 0) def sigmoid(self, X): return T.nnet.sigmoid(X) class Selu: def __init__(self, lmbd, alpha): self.lmbd = lmbd self.alpha = alpha def execute(self, X): return self.lmbd * T.switch(T.ge(X, 0), X, self.alpha * (T.exp(X) - 1)) class Elu: def __init__(self, alpha): self.alpha = alpha def execute(self, X): return T.switch(T.ge(X, 0), X, self.alpha * (T.exp(X) - 1)) class LeakyReLU: def __init__(self, leak): self.leak = leak def execute(self, X): return T.switch(T.ge(X, 0), X, self.leak * X) #################################LOSS FUNCTIONS################################ def cross_entropy(self, yhat, M): if self.smoothing: n_out = M + self.n_sample return T.cast(T.mean( (1.0 - (n_out / (n_out - 1)) * self.smoothing) * (-T.log(gpu_diag_wide(yhat) + 1e-24)) + ( self.smoothing / (n_out - 1)) * T.sum(-T.log(yhat + 1e-24), axis=1)), theano.config.floatX) else: return T.cast(T.mean(-T.log(gpu_diag_wide(yhat) + 1e-24)), theano.config.floatX) def cross_entropy_logits(self, yhat, M): if self.smoothing: n_out = M + self.n_sample return T.cast(T.mean((1.0 - (n_out / (n_out - 1)) * self.smoothing) * gpu_diag_wide(yhat) + ( self.smoothing / (n_out - 1)) * T.sum(yhat, axis=1)), theano.config.floatX) else: return T.cast(T.mean(gpu_diag_wide(yhat)), theano.config.floatX) def bpr(self, yhat, M): return T.cast(T.mean(-T.log(T.nnet.sigmoid(gpu_diag_wide(yhat).dimshuffle((0, &#39;x&#39;)) - yhat))), theano.config.floatX) def bpr_max(self, yhat, M): softmax_scores = self.softmax_neg(yhat) return T.cast(T.mean(-T.log( T.sum(T.nnet.sigmoid(gpu_diag_wide(yhat).dimshuffle((0, &#39;x&#39;)) - yhat) * softmax_scores, axis=1) + 1e-24) + self.bpreg * T.sum((yhat ** 2) * softmax_scores, axis=1)), theano.config.floatX) def top1(self, yhat, M): ydiag = gpu_diag_wide(yhat).dimshuffle((0, &#39;x&#39;)) return T.cast(T.mean( T.mean(T.nnet.sigmoid(-ydiag + yhat) + T.nnet.sigmoid(yhat ** 2), axis=1) - T.nnet.sigmoid(ydiag ** 2) / ( M + self.n_sample)), theano.config.floatX) def top1_max(self, yhat, M): softmax_scores = self.softmax_neg(yhat) y = softmax_scores * ( T.nnet.sigmoid(-gpu_diag_wide(yhat).dimshuffle((0, &#39;x&#39;)) + yhat) + T.nnet.sigmoid(yhat ** 2)) return T.cast(T.mean(T.sum(y, axis=1)), theano.config.floatX) ############################################################################### def floatX(self, X): return np.asarray(X, dtype=theano.config.floatX) def init_weights(self, shape, name=None): return theano.shared(self.init_matrix(shape), borrow=True, name=name) def init_matrix(self, shape): if self.sigma != 0: sigma = self.sigma else: sigma = np.sqrt(6.0 / (shape[0] + shape[1])) if self.init_as_normal: return self.floatX(np.random.randn(*shape) * sigma) else: return self.floatX(np.random.rand(*shape) * sigma * 2 - sigma) def extend_weights(self, W, n_new): matrix = W.get_value() sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (matrix.shape[0] + matrix.shape[1] + n_new)) if self.init_as_normal: new_rows = self.floatX(np.random.randn(n_new, matrix.shape[1]) * sigma) else: new_rows = self.floatX(np.random.rand(n_new, matrix.shape[1]) * sigma * 2 - sigma) W.set_value(np.vstack([matrix, new_rows])) def init(self, data): data.sort_values([self.session_key, self.time_key], inplace=True) offset_sessions = np.zeros(data[self.session_key].nunique() + 1, dtype=np.int32) offset_sessions[1:] = data.groupby(self.session_key).size().cumsum() np.random.seed(42) self.Wx, self.Wh, self.Wrz, self.Bh, self.H = [], [], [], [], [] if self.constrained_embedding: n_features = self.layers[-1] elif self.embedding: self.E = self.init_weights((self.n_items, self.embedding), name=&#39;E&#39;) n_features = self.embedding else: n_features = self.n_items for i in range(len(self.layers)): m = [] m.append(self.init_matrix((self.layers[i - 1] if i &gt; 0 else n_features, self.layers[i]))) m.append(self.init_matrix((self.layers[i - 1] if i &gt; 0 else n_features, self.layers[i]))) m.append(self.init_matrix((self.layers[i - 1] if i &gt; 0 else n_features, self.layers[i]))) self.Wx.append( theano.shared(value=np.hstack(m), borrow=True, name=&#39;Wx{}&#39;.format(i))) # For compatibility&#39;s sake self.Wh.append(self.init_weights((self.layers[i], self.layers[i]), name=&#39;Wh{}&#39;.format(i))) m2 = [] m2.append(self.init_matrix((self.layers[i], self.layers[i]))) m2.append(self.init_matrix((self.layers[i], self.layers[i]))) self.Wrz.append( theano.shared(value=np.hstack(m2), borrow=True, name=&#39;Wrz{}&#39;.format(i))) # For compatibility&#39;s sake self.Bh.append(theano.shared(value=np.zeros((self.layers[i] * 3,), dtype=theano.config.floatX), borrow=True, name=&#39;Bh{}&#39;.format(i))) self.H.append(theano.shared(value=np.zeros((self.batch_size, self.layers[i]), dtype=theano.config.floatX), borrow=True, name=&#39;H{}&#39;.format(i))) self.Wy = self.init_weights((self.n_items, self.layers[-1]), name=&#39;Wy&#39;) self.By = theano.shared(value=np.zeros((self.n_items, 1), dtype=theano.config.floatX), borrow=True, name=&#39;By&#39;) return offset_sessions def dropout(self, X, drop_p): if drop_p &gt; 0: retain_prob = 1 - drop_p X *= mrng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX) / retain_prob return X def adam(self, param, grad, updates, sample_idx=None, epsilon=1e-6): v1 = np.float32(self.adapt_params[0]) v2 = np.float32(1.0 - self.adapt_params[0]) v3 = np.float32(self.adapt_params[1]) v4 = np.float32(1.0 - self.adapt_params[1]) acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True) meang = theano.shared(param.get_value(borrow=False) * 0., borrow=True) countt = theano.shared(param.get_value(borrow=False) * 0., borrow=True) if sample_idx is None: acc_new = v3 * acc + v4 * (grad ** 2) meang_new = v1 * meang + v2 * grad countt_new = countt + 1 updates[acc] = acc_new updates[meang] = meang_new updates[countt] = countt_new else: acc_s = acc[sample_idx] meang_s = meang[sample_idx] countt_s = countt[sample_idx] # acc_new = v3 * acc_s + v4 * (grad**2) #Faster, but inaccurate when an index occurs multiple times # updates[acc] = T.set_subtensor(acc_s, acc_new) #Faster, but inaccurate when an index occurs multiple times updates[acc] = T.inc_subtensor(T.set_subtensor(acc_s, acc_s * v3)[sample_idx], v4 * (grad ** 2)) # Slower, but accurate when an index occurs multiple times acc_new = updates[acc][sample_idx] # Slower, but accurate when an index occurs multiple times # meang_new = v1 * meang_s + v2 * grad # updates[meang] = T.set_subtensor(meang_s, meang_new) #Faster, but inaccurate when an index occurs multiple times updates[meang] = T.inc_subtensor(T.set_subtensor(meang_s, meang_s * v1)[sample_idx], v2 * ( grad ** 2)) # Slower, but accurate when an index occurs multiple times meang_new = updates[meang][sample_idx] # Slower, but accurate when an index occurs multiple times countt_new = countt_s + 1.0 updates[countt] = T.set_subtensor(countt_s, countt_new) return (meang_new / (1 - v1 ** countt_new)) / (T.sqrt(acc_new / (1 - v1 ** countt_new)) + epsilon) def adagrad(self, param, grad, updates, sample_idx=None, epsilon=1e-6): acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True) if sample_idx is None: acc_new = acc + grad ** 2 updates[acc] = acc_new else: acc_s = acc[sample_idx] acc_new = acc_s + grad ** 2 updates[acc] = T.set_subtensor(acc_s, acc_new) gradient_scaling = T.cast(T.sqrt(acc_new + epsilon), theano.config.floatX) return grad / gradient_scaling def adadelta(self, param, grad, updates, sample_idx=None, epsilon=1e-6): v1 = np.float32(self.adapt_params[0]) v2 = np.float32(1.0 - self.adapt_params[0]) acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True) upd = theano.shared(param.get_value(borrow=False) * 0., borrow=True) if sample_idx is None: acc_new = v1 * acc + v2 * (grad ** 2) updates[acc] = acc_new grad_scaling = (upd + epsilon) / (acc_new + epsilon) upd_new = v1 * upd + v2 * grad_scaling * (grad ** 2) updates[upd] = upd_new else: acc_s = acc[sample_idx] # acc_new = v1 * acc_s + v2 * (grad**2) #Faster, but inaccurate when an index occurs multiple times # updates[acc] = T.set_subtensor(acc_s, acc_new) #Faster, but inaccurate when an index occurs multiple times updates[acc] = T.inc_subtensor(T.set_subtensor(acc_s, acc_s * v1)[sample_idx], v2 * (grad ** 2)) # Slower, but accurate when an index occurs multiple times acc_new = updates[acc][sample_idx] # Slower, but accurate when an index occurs multiple times upd_s = upd[sample_idx] grad_scaling = (upd_s + epsilon) / (acc_new + epsilon) # updates[upd] = T.set_subtensor(upd_s, v1 * upd_s + v2 * grad_scaling * (grad**2)) #Faster, but inaccurate when an index occurs multiple times updates[upd] = T.inc_subtensor(T.set_subtensor(upd_s, upd_s * v1)[sample_idx], v2 * grad_scaling * ( grad ** 2)) # Slower, but accurate when an index occurs multiple times gradient_scaling = T.cast(T.sqrt(grad_scaling), theano.config.floatX) if self.learning_rate != 1.0: print(&#39;Warn: learning_rate is not 1.0 while using adadelta. Setting learning_rate to 1.0&#39;) self.learning_rate = 1.0 return grad * gradient_scaling # Ok, checked def rmsprop(self, param, grad, updates, sample_idx=None, epsilon=1e-6): v1 = np.float32(self.adapt_params[0]) v2 = np.float32(1.0 - self.adapt_params[0]) acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True) if sample_idx is None: acc_new = v1 * acc + v2 * grad ** 2 updates[acc] = acc_new else: acc_s = acc[sample_idx] # acc_new = v1 * acc_s + v2 * grad ** 2 #Faster, but inaccurate when an index occurs multiple times # updates[acc] = T.set_subtensor(acc_s, acc_new) #Faster, but inaccurate when an index occurs multiple times updates[acc] = T.inc_subtensor(T.set_subtensor(acc_s, acc_s * v1)[sample_idx], v2 * grad ** 2) # Slower, but accurate when an index occurs multiple times acc_new = updates[acc][sample_idx] # Slower, but accurate when an index occurs multiple times gradient_scaling = T.cast(T.sqrt(acc_new + epsilon), theano.config.floatX) return grad / gradient_scaling def RMSprop(self, cost, params, full_params, sampled_params, sidxs, epsilon=1e-6): grads = [T.grad(cost=cost, wrt=param) for param in params] sgrads = [T.grad(cost=cost, wrt=sparam) for sparam in sampled_params] updates = OrderedDict() if self.grad_cap &gt; 0: norm = T.cast(T.sqrt(T.sum([T.sum([T.sum(g ** 2) for g in g_list]) for g_list in grads]) + T.sum( [T.sum(g ** 2) for g in sgrads])), theano.config.floatX) grads = [[T.switch(T.ge(norm, self.grad_cap), g * self.grad_cap / norm, g) for g in g_list] for g_list in grads] sgrads = [T.switch(T.ge(norm, self.grad_cap), g * self.grad_cap / norm, g) for g in sgrads] for p_list, g_list in zip(params, grads): for p, g in zip(p_list, g_list): if self.adapt == &#39;adagrad&#39;: g = self.adagrad(p, g, updates) elif self.adapt == &#39;rmsprop&#39;: g = self.rmsprop(p, g, updates) elif self.adapt == &#39;adadelta&#39;: g = self.adadelta(p, g, updates) elif self.adapt == &#39;adam&#39;: g = self.adam(p, g, updates) if self.momentum &gt; 0: velocity = theano.shared(p.get_value(borrow=False) * 0., borrow=True) velocity2 = self.momentum * velocity - np.float32(self.learning_rate) * (g + self.lmbd * p) updates[velocity] = velocity2 updates[p] = p + velocity2 else: updates[p] = p * np.float32(1.0 - self.learning_rate * self.lmbd) - np.float32( self.learning_rate) * g for i in range(len(sgrads)): g = sgrads[i] fullP = full_params[i] sample_idx = sidxs[i] sparam = sampled_params[i] if self.adapt == &#39;adagrad&#39;: g = self.adagrad(fullP, g, updates, sample_idx) elif self.adapt == &#39;rmsprop&#39;: g = self.rmsprop(fullP, g, updates, sample_idx) elif self.adapt == &#39;adadelta&#39;: g = self.adadelta(fullP, g, updates, sample_idx) elif self.adapt == &#39;adam&#39;: g = self.adam(fullP, g, updates, sample_idx) if self.lmbd &gt; 0: delta = np.float32(self.learning_rate) * (g + self.lmbd * sparam) else: delta = np.float32(self.learning_rate) * g if self.momentum &gt; 0: velocity = theano.shared(fullP.get_value(borrow=False) * 0., borrow=True) vs = velocity[sample_idx] velocity2 = self.momentum * vs - delta updates[velocity] = T.set_subtensor(vs, velocity2) updates[fullP] = T.inc_subtensor(sparam, velocity2) else: updates[fullP] = T.inc_subtensor(sparam, - delta) return updates def model(self, X, H, M, R=None, Y=None, drop_p_hidden=0.0, drop_p_embed=0.0, predict=False): sparams, full_params, sidxs = [], [], [] if self.constrained_embedding: if Y is not None: X = T.concatenate([X, Y], axis=0) S = self.Wy[X] Sx = S[:M] Sy = S[M:] y = self.dropout(Sx, drop_p_embed) H_new = [] start = 0 sparams.append(S) full_params.append(self.Wy) sidxs.append(X) elif self.embedding: Sx = self.E[X] y = self.dropout(Sx, drop_p_embed) H_new = [] start = 0 sparams.append(Sx) full_params.append(self.E) sidxs.append(X) else: Sx = self.Wx[0][X] vec = Sx + self.Bh[0] rz = T.nnet.sigmoid(vec[:, self.layers[0]:] + T.dot(H[0], self.Wrz[0])) h = self.hidden_activation(T.dot(H[0] * rz[:, :self.layers[0]], self.Wh[0]) + vec[:, :self.layers[0]]) z = rz[:, self.layers[0]:] h = (1.0 - z) * H[0] + z * h h = self.dropout(h, drop_p_hidden) y = h H_new = [T.switch(R.dimshuffle((0, &#39;x&#39;)), 0, h) if not predict else h] start = 1 sparams.append(Sx) full_params.append(self.Wx[0]) sidxs.append(X) for i in range(start, len(self.layers)): vec = T.dot(y, self.Wx[i]) + self.Bh[i] rz = T.nnet.sigmoid(vec[:, self.layers[i]:] + T.dot(H[i], self.Wrz[i])) h = self.hidden_activation(T.dot(H[i] * rz[:, :self.layers[i]], self.Wh[i]) + vec[:, :self.layers[i]]) z = rz[:, self.layers[i]:] h = (1.0 - z) * H[i] + z * h h = self.dropout(h, drop_p_hidden) y = h H_new.append(T.switch(R.dimshuffle((0, &#39;x&#39;)), 0, h) if not predict else h) if Y is not None: if (not self.constrained_embedding) or predict: Sy = self.Wy[Y] sparams.append(Sy) full_params.append(self.Wy) sidxs.append(Y) SBy = self.By[Y] sparams.append(SBy) full_params.append(self.By) sidxs.append(Y) if predict and self.final_act == &#39;softmax_logit&#39;: y = self.softmax(T.dot(y, Sy.T) + SBy.flatten()) else: y = self.final_activation(T.dot(y, Sy.T) + SBy.flatten()) return H_new, y, sparams, full_params, sidxs else: if predict and self.final_act == &#39;softmax_logit&#39;: y = self.softmax(T.dot(y, self.Wy.T) + self.By.flatten()) else: y = self.final_activation(T.dot(y, self.Wy.T) + self.By.flatten()) return H_new, y, sparams, full_params, sidxs def generate_neg_samples(self, pop, length): if self.sample_alpha: sample = np.searchsorted(pop, np.random.rand(self.n_sample * length)) else: sample = np.random.choice(self.n_items, size=self.n_sample * length) if length &gt; 1: sample = sample.reshape((length, self.n_sample)) return sample def fit(self, data, sample_store=10000000): &#39;&#39;&#39; Trains the network. Parameters -- data : pandas.DataFrame Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps). It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties). sample_store : int If additional negative samples are used (n_sample &gt; 0), the efficiency of GPU utilization can be sped up, by precomputing a large batch of negative samples (and recomputing when necessary). This parameter regulizes the size of this precomputed ID set. Its value is the maximum number of int values (IDs) to be stored. Precomputed IDs are stored in the RAM. For the most efficient computation, a balance must be found between storing few examples and constantly interrupting GPU computations for a short time vs. computing many examples and interrupting GPU computations for a long time (but rarely). &#39;&#39;&#39; self.predict = None self.error_during_train = False itemids = data[self.item_key].unique() self.n_items = len(itemids) self.itemidmap = pd.Series(data=np.arange(self.n_items), index=itemids) data = pd.merge(data, pd.DataFrame({self.item_key: itemids, &#39;ItemIdx&#39;: self.itemidmap[itemids].values}), on=self.item_key, how=&#39;inner&#39;) offset_sessions = self.init(data) if self.n_sample: pop = data.groupby(self.item_key).size() pop = pop[self.itemidmap.index.values].values ** self.sample_alpha pop = pop.cumsum() / pop.sum() pop[-1] = 1 if sample_store: generate_length = sample_store // self.n_sample if generate_length &lt;= 1: sample_store = 0 print(&#39;No example store was used&#39;) else: neg_samples = self.generate_neg_samples(pop, generate_length) sample_pointer = 0 else: print(&#39;No example store was used&#39;) X = T.ivector() Y = T.ivector() M = T.iscalar() R = T.bvector() H_new, Y_pred, sparams, full_params, sidxs = self.model(X, self.H, M, R, Y, self.dropout_p_hidden, self.dropout_p_embed) cost = (M / self.batch_size) * self.loss_function(Y_pred, M) params = [self.Wx if self.embedding or self.constrained_embedding else self.Wx[1:], self.Wh, self.Wrz, self.Bh] updates = self.RMSprop(cost, params, full_params, sparams, sidxs) for i in range(len(self.H)): updates[self.H[i]] = H_new[i] train_function = function(inputs=[X, Y, M, R], outputs=cost, updates=updates, allow_input_downcast=True) base_order = np.argsort( data.groupby(self.session_key)[self.time_key].min().values) if self.time_sort else np.arange( len(offset_sessions) - 1) data_items = data.ItemIdx.values for epoch in range(self.n_epochs): for i in range(len(self.layers)): self.H[i].set_value(np.zeros((self.batch_size, self.layers[i]), dtype=theano.config.floatX), borrow=True) c = [] cc = [] session_idx_arr = np.random.permutation(len(offset_sessions) - 1) if self.train_random_order else base_order iters = np.arange(self.batch_size) maxiter = iters.max() start = offset_sessions[session_idx_arr[iters]] end = offset_sessions[session_idx_arr[iters] + 1] finished = False while not finished: minlen = (end - start).min() out_idx = data_items[start] for i in range(minlen - 1): in_idx = out_idx out_idx = data_items[start + i + 1] if self.n_sample: if sample_store: if sample_pointer == generate_length: neg_samples = self.generate_neg_samples(pop, generate_length) sample_pointer = 0 sample = neg_samples[sample_pointer] sample_pointer += 1 else: sample = self.generate_neg_samples(pop, 1) y = np.hstack([out_idx, sample]) else: y = out_idx if self.n_sample: if sample_pointer == generate_length: generate_samples() sample_pointer = 0 sample_pointer += 1 reset = (start + i + 1 == end - 1) cost = train_function(in_idx, y, len(iters), reset) c.append(cost) cc.append(len(iters)) if np.isnan(cost): print(str(epoch) + &#39;: NaN error!&#39;) self.error_during_train = True return start = start + minlen - 1 finished_mask = (end - start &lt;= 1) n_finished = finished_mask.sum() iters[finished_mask] = maxiter + np.arange(1, n_finished + 1) maxiter += n_finished valid_mask = (iters &lt; len(offset_sessions) - 1) n_valid = valid_mask.sum() if (n_valid == 0) or (n_valid &lt; 2 and self.n_sample == 0): finished = True break mask = finished_mask &amp; valid_mask sessions = session_idx_arr[iters[mask]] start[mask] = offset_sessions[sessions] end[mask] = offset_sessions[sessions + 1] iters = iters[valid_mask] start = start[valid_mask] end = end[valid_mask] if n_valid &lt; len(valid_mask): for i in range(len(self.H)): tmp = self.H[i].get_value(borrow=True) tmp = tmp[valid_mask] self.H[i].set_value(tmp, borrow=True) c = np.array(c) cc = np.array(cc) avgc = np.sum(c * cc) / np.sum(cc) if np.isnan(avgc): print(&#39;Epoch {}: NaN error!&#39;.format(str(epoch))) self.error_during_train = True return print(&#39;Epoch{} tloss: {:.6f}&#39;.format(epoch, avgc)) def predict_next_batch(self, session_ids, input_item_ids, predict_for_item_ids=None, batch=100): &#39;&#39;&#39; Gives predicton scores for a selected set of items. Can be used in batch mode to predict for multiple independent events (i.e. events of different sessions) at once and thus speed up evaluation. If the session ID at a given coordinate of the session_ids parameter remains the same during subsequent calls of the function, the corresponding hidden state of the network will be kept intact (i.e. that&#39;s how one can predict an item to a session). If it changes, the hidden state of the network is reset to zeros. Parameters -- session_ids : 1D array Contains the session IDs of the events of the batch. Its length must equal to the prediction batch size (batch param). input_item_ids : 1D array Contains the item IDs of the events of the batch. Every item ID must be must be in the training data of the network. Its length must equal to the prediction batch size (batch param). predict_for_item_ids : 1D array (optional) IDs of items for which the network should give prediction scores. Every ID must be in the training set. The default value is None, which means that the network gives prediction on its every output (i.e. for all items in the training set). batch : int Prediction batch size. Returns -- out : pandas.DataFrame Prediction scores for selected items for every event of the batch. Columns: events of the batch; rows: items. Rows are indexed by the item IDs. &#39;&#39;&#39; if self.error_during_train: raise Exception if self.predict is None or self.predict_batch != batch: self.predict_batch = batch X = T.ivector() Y = T.ivector() M = T.iscalar() if self.constrained_embedding or (predict_for_item_ids is not None) else None for i in range(len(self.layers)): self.H[i].set_value(np.zeros((batch, self.layers[i]), dtype=theano.config.floatX), borrow=True) if predict_for_item_ids is not None: H_new, yhat, _, _, _ = self.model(X, self.H, M, Y=Y, predict=True) else: H_new, yhat, _, _, _ = self.model(X, self.H, M, predict=True) updatesH = OrderedDict() for i in range(len(self.H)): updatesH[self.H[i]] = H_new[i] if predict_for_item_ids is not None: if self.constrained_embedding: self.predict = function(inputs=[X, Y, M], outputs=yhat, updates=updatesH, allow_input_downcast=True) else: self.predict = function(inputs=[X, Y], outputs=yhat, updates=updatesH, allow_input_downcast=True) else: if self.constrained_embedding: self.predict = function(inputs=[X, M], outputs=yhat, updates=updatesH, allow_input_downcast=True) else: self.predict = function(inputs=[X], outputs=yhat, updates=updatesH, allow_input_downcast=True) self.current_session = np.ones(batch) * -1 session_change = np.arange(batch)[session_ids != self.current_session] if len(session_change) &gt; 0: for i in range(len(self.H)): tmp = self.H[i].get_value(borrow=True) tmp[session_change] = 0 self.H[i].set_value(tmp, borrow=True) self.current_session = session_ids.copy() in_idxs = self.itemidmap[input_item_ids] if np.any(np.isnan(in_idxs)): preds = np.random.randn(len(self.itemidmap), len(in_idxs)) return pd.DataFrame(data=preds, index=self.itemidmap.index) if predict_for_item_ids is not None: iIdxs = self.itemidmap[predict_for_item_ids] if self.constrained_embedding: preds = np.asarray(self.predict(in_idxs, iIdxs, batch)).T else: preds = np.asarray(self.predict(in_idxs, iIdxs)).T return pd.DataFrame(data=preds, index=predict_for_item_ids) else: if self.constrained_embedding: preds = np.asarray(self.predict(in_idxs, batch)).T else: preds = np.asarray(self.predict(in_idxs)).T return pd.DataFrame(data=preds, index=self.itemidmap.index) def symbolic_predict(self, X, Y, M, items, batch_size): if not self.constrained_embedding: M = None H = [] for i in range(len(self.layers)): H.append(theano.shared(np.zeros((batch_size, self.layers[i]), dtype=theano.config.floatX))) if items is not None: H_new, yhat, _, _, _ = self.model(X, H, M, Y=Y, predict=True) else: H_new, yhat, _, _, _ = self.model(X, H, M, predict=True) updatesH = OrderedDict() for i in range(len(H)): updatesH[H[i]] = H_new[i] return yhat, H, updatesH . . class HGRU4Rec: &quot;&quot;&quot; HGRU4Rec(session_layers, user_layers, n_epochs=10, batch_size=50, learning_rate=0.05, momentum=0.0, adapt=&#39;adagrad&#39;, decay=0.9, grad_cap=0, sigma=0, dropout_p_hidden_usr=0.0, dropout_p_hidden_ses=0.0, dropout_p_init=0.0, init_as_normal=False, reset_after_session=True, loss=&#39;top1&#39;, hidden_act=&#39;tanh&#39;, final_act=None, train_random_order=False, lmbd=0.0, session_key=&#39;SessionId&#39;, item_key=&#39;ItemId&#39;, time_key=&#39;Time&#39;, user_key=&#39;UserId&#39;, n_sample=0, sample_alpha=0.75, item_embedding=None, init_item_embeddings=None, user_hidden_bias_mode=&#39;init&#39;, user_output_bias=False, user_to_session_act=&#39;tanh&#39;, seed=42) Initializes the network. Parameters -- session_layers : 1D array list of the number of GRU units in the session layers user_layers : 1D array list of the number of GRU units in the user layers n_epochs : int number of training epochs (default: 10) batch_size : int size of the minibatch, also effect the number of negative samples through minibatch based sampling (default: 50) dropout_p_hidden_usr : float probability of dropout of hidden units for the user layers (default: 0.0) dropout_p_hidden_ses : float probability of dropout of hidden units for the session layers (default: 0.0) dropout_p_init : float probability of dropout of the session-level initialization (default: 0.0) learning_rate : float learning rate (default: 0.05) momentum : float if not zero, Nesterov momentum will be applied during training with the given strength (default: 0.0) adapt : None, &#39;adagrad&#39;, &#39;rmsprop&#39;, &#39;adam&#39;, &#39;adadelta&#39; sets the appropriate learning rate adaptation strategy, use None for standard SGD (default: &#39;adagrad&#39;) decay : float decay parameter for RMSProp, has no effect in other modes (default: 0.9) grad_cap : float clip gradients that exceede this value to this value, 0 means no clipping (default: 0.0) sigma : float &quot;width&quot; of initialization; either the standard deviation or the min/max of the init interval (with normal and uniform initializations respectively); 0 means adaptive normalization (sigma depends on the size of the weight matrix); (default: 0) init_as_normal : boolean False: init from uniform distribution on [-sigma,sigma]; True: init from normal distribution N(0,sigma); (default: False) reset_after_session : boolean whether the hidden state is set to zero after a session finished (default: True) loss : &#39;top1&#39;, &#39;bpr&#39; or &#39;cross-entropy&#39; selects the loss function (default: &#39;top1&#39;) hidden_act : &#39;tanh&#39; or &#39;relu&#39; selects the activation function on the hidden states (default: &#39;tanh&#39;) final_act : None, &#39;linear&#39;, &#39;relu&#39; or &#39;tanh&#39; selects the activation function of the final layer where appropriate, None means default (tanh if the loss is brp or top1; softmax for cross-entropy), cross-entropy is only affeted by &#39;tanh&#39; where the softmax layers is preceeded by a tanh nonlinearity (default: None) train_random_order : boolean whether to randomize the order of sessions in each epoch (default: False) lmbd : float coefficient of the L2 regularization (default: 0.0) session_key : string header of the session ID column in the input file (default: &#39;SessionId&#39;) item_key : string header of the item ID column in the input file (default: &#39;ItemId&#39;) time_key : string header of the timestamp column in the input file (default: &#39;Time&#39;) user_key : string header of the user column in the input file (default: &#39;UserId&#39;) n_sample : int number of additional negative samples to be used (besides the other examples of the minibatch) (default: 0) sample_alpha : float the probability of an item used as an additional negative sample is supp^sample_alpha (default: 0.75) (e.g.: sample_alpha=1 --&gt; popularity based sampling; sample_alpha=0 --&gt; uniform sampling) item_embedding: int size of the item embedding vector (default: None) init_item_embeddings: 2D array or dict array with the initial values of the embeddings vector of every item, or dict that maps each item id to its embedding vector (default: None) user_propagation_mode: string &#39;init&#39; to use the (last) user hidden state to initialize the (first) session hidden state; &#39;all&#39; to propagate the user hidden also in input the the (first) session layers. (default: &#39;init&#39;) user_to_output: boolean True to propagate the (last) user hidden state in input to the final output layer, False otherwise (default: False) user_to_session_act: string activation of the user-to-session initialization network (default: &#39;tanh&#39;) seed: int random seed (default: 42) &quot;&quot;&quot; def __init__(self, session_layers, user_layers, n_epochs=10, batch_size=50, learning_rate=0.05, momentum=0.0, adapt=&#39;adagrad&#39;, decay=0.9, grad_cap=0, sigma=0, dropout_p_hidden_usr=0.0, dropout_p_hidden_ses=0.0, dropout_p_init=0.0, init_as_normal=False, reset_after_session=True, loss=&#39;top1&#39;, hidden_act=&#39;tanh&#39;, final_act=None, train_random_order=False, lmbd=0.0, session_key=&#39;SessionId&#39;, item_key=&#39;ItemId&#39;, time_key=&#39;Time&#39;, user_key=&#39;UserId&#39;, n_sample=0, sample_alpha=0.75, item_embedding=None, init_item_embeddings=None, user_propagation_mode=&#39;init&#39;, user_to_output=False, user_to_session_act=&#39;tanh&#39;, seed=42): self.session_layers = session_layers self.user_layers = user_layers self.n_epochs = n_epochs self.batch_size = batch_size self.dropout_p_hidden_usr = dropout_p_hidden_usr self.dropout_p_hidden_ses = dropout_p_hidden_ses self.dropout_p_init = dropout_p_init self.learning_rate = learning_rate self.decay = decay self.momentum = momentum self.sigma = sigma self.init_as_normal = init_as_normal self.reset_after_session = reset_after_session self.session_key = session_key self.item_key = item_key self.time_key = time_key self.user_key = user_key self.grad_cap = grad_cap self.train_random_order = train_random_order self.lmbd = lmbd self.user_propagation_mode = user_propagation_mode self.user_to_output = user_to_output self.item_embedding = item_embedding self.init_item_embeddings = init_item_embeddings self.rng = np.random.RandomState(seed=seed) if adapt == &#39;rmsprop&#39;: self.adapt = &#39;rmsprop&#39; elif adapt == &#39;adagrad&#39;: self.adapt = &#39;adagrad&#39; elif adapt == &#39;adadelta&#39;: self.adapt = &#39;adadelta&#39; elif adapt == &#39;adam&#39;: self.adapt = &#39;adam&#39; else: self.adapt = False if loss == &#39;cross-entropy&#39;: if final_act == &#39;tanh&#39;: self.final_activation = self.softmaxth else: self.final_activation = self.softmax self.loss_function = self.cross_entropy elif loss == &#39;bpr&#39;: if final_act == &#39;linear&#39;: self.final_activation = self.linear elif final_act == &#39;relu&#39;: self.final_activation = self.relu else: self.final_activation = self.tanh self.loss_function = self.bpr elif loss == &#39;top1&#39;: if final_act == &#39;linear&#39;: self.final_activation = self.linear elif final_act == &#39;relu&#39;: self.final_activation = self.relu else: self.final_activation = self.tanh self.loss_function = self.top1 else: raise NotImplementedError(&#39;loss {} not implemented&#39;.format(loss)) if hidden_act == &#39;relu&#39;: self.hidden_activation = self.relu elif hidden_act == &#39;tanh&#39;: self.hidden_activation = self.tanh else: raise NotImplementedError(&#39;hidden activation {} not implemented&#39;.format(hidden_act)) if user_to_session_act == &#39;relu&#39;: self.s_init_act = self.relu elif user_to_session_act == &#39;tanh&#39;: self.s_init_act = self.tanh else: raise NotImplementedError(&#39;user-to-session activation {} not implemented&#39;.format(hidden_act)) self.n_sample = n_sample self.sample_alpha = sample_alpha ######################ACTIVATION FUNCTIONS##################### def linear(self, X): return X def tanh(self, X): return T.tanh(X) def softmax(self, X): e_x = T.exp(X - X.max(axis=1).dimshuffle(0, &#39;x&#39;)) return e_x / e_x.sum(axis=1).dimshuffle(0, &#39;x&#39;) def softmaxth(self, X): X = self.tanh(X) e_x = T.exp(X - X.max(axis=1).dimshuffle(0, &#39;x&#39;)) return e_x / e_x.sum(axis=1).dimshuffle(0, &#39;x&#39;) def relu(self, X): return T.maximum(X, 0) def sigmoid(self, X): return T.nnet.sigmoid(X) #################################LOSS FUNCTIONS################################ def cross_entropy(self, yhat): return T.cast(T.mean(-T.log(T.diag(yhat) + 1e-24)), theano.config.floatX) def bpr(self, yhat): return T.cast(T.mean(-T.log(T.nnet.sigmoid(T.diag(yhat) - yhat.T))), theano.config.floatX) def top1(self, yhat): yhatT = yhat.T return T.cast(T.mean( T.mean(T.nnet.sigmoid(-T.diag(yhat) + yhatT) + T.nnet.sigmoid(yhatT ** 2), axis=0) - T.nnet.sigmoid( T.diag(yhat) ** 2) / self.batch_size), theano.config.floatX) ############################################################################### def floatX(self, X): return np.asarray(X, dtype=theano.config.floatX) def init_weights(self, shape): sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (shape[0] + shape[1])) if self.init_as_normal: return theano.shared(self.floatX(self.rng.randn(*shape) * sigma), borrow=True) else: return theano.shared(self.floatX(self.rng.rand(*shape) * sigma * 2 - sigma), borrow=True) def init_matrix(self, shape): sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (shape[0] + shape[1])) if self.init_as_normal: return self.floatX(self.rng.randn(*shape) * sigma) else: return self.floatX(self.rng.rand(*shape) * sigma * 2 - sigma) def extend_weights(self, W, n_new): matrix = W.get_value() sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (matrix.shape[0] + matrix.shape[1] + n_new)) if self.init_as_normal: new_rows = self.floatX(self.rng.randn(n_new, matrix.shape[1]) * sigma) else: new_rows = self.floatX(self.rng.rand(n_new, matrix.shape[1]) * sigma * 2 - sigma) W.set_value(np.vstack([matrix, new_rows])) def set_item_embeddings(self, E, values): if isinstance(values, dict): keys, values = values.keys(), np.vstack(list(values.values())) elif isinstance(values, np.ndarray): # use item ids ranging from 0 to the number of rows in values keys, values = np.arange(values.shape[0]), values else: raise NotImplementedError(&#39;Unsupported type&#39;) # map item ids to the internal indices mask = np.in1d(keys, self.itemidmap.index, assume_unique=True) idx = self.itemidmap[keys].dropna().values.astype(np.int) emb = E.get_value() emb[idx] = values[mask] E.set_value(emb) def preprocess_data(self, data): # sort by user and time key in order data.sort_values([self.user_key, self.session_key, self.time_key], inplace=True) data.reset_index(drop=True, inplace=True) offset_session = np.r_[0, data.groupby([self.user_key, self.session_key], sort=False).size().cumsum()[:-1]] user_indptr = np.r_[0, data.groupby(self.user_key, sort=False)[self.session_key].nunique().cumsum()[:-1]] return user_indptr, offset_session def save_state(self): state = OrderedDict() for i in range(len(self.session_layers)): state[&#39;Ws_in_&#39; + str(i)] = self.Ws_in[i].get_value() state[&#39;Ws_hh_&#39; + str(i)] = self.Ws_hh[i].get_value() state[&#39;Ws_rz_&#39; + str(i)] = self.Ws_rz[i].get_value() state[&#39;Bs_h_&#39; + str(i)] = self.Bs_h[i].get_value() state[&#39;Hs_&#39; + str(i)] = self.Hs[i].get_value() state[&#39;Wsy&#39;] = self.Wsy.get_value() state[&#39;By&#39;] = self.By.get_value() for i in range(len(self.user_layers)): state[&#39;Wu_in_&#39; + str(i)] = self.Wu_in[i].get_value() state[&#39;Wu_hh_&#39; + str(i)] = self.Wu_hh[i].get_value() state[&#39;Wu_rz_&#39; + str(i)] = self.Wu_rz[i].get_value() state[&#39;Bu_h_&#39; + str(i)] = self.Bu_h[i].get_value() state[&#39;Hu_&#39; + str(i)] = self.Hu[i].get_value() if self.user_to_output: state[&#39;Wuy&#39;] = self.Wuy.get_value() state[&#39;Wu_to_s_init&#39;] = self.Ws_init[0].get_value() state[&#39;Bu_to_s_init&#39;] = self.Bs_init[0].get_value() if self.user_propagation_mode == &#39;all&#39;: state[&#39;Wu_to_s&#39;] = self.Wu_to_s[0].get_value() return state def load_state(self, state): for i in range(len(self.session_layers)): self.Ws_in[i].set_value(state[&#39;Ws_in_&#39; + str(i)], borrow=True) self.Ws_hh[i].set_value(state[&#39;Ws_hh_&#39; + str(i)], borrow=True) self.Ws_rz[i].set_value(state[&#39;Ws_rz_&#39; + str(i)], borrow=True) self.Bs_h[i].set_value(state[&#39;Bs_h_&#39; + str(i)], borrow=True) self.Hs[i].set_value(state[&#39;Hs_&#39; + str(i)], borrow=True) self.Wsy.set_value(state[&#39;Wsy&#39;], borrow=True) self.By.set_value(state[&#39;By&#39;], borrow=True) for i in range(len(self.user_layers)): self.Wu_in[i].set_value(state[&#39;Wu_in_&#39; + str(i)], borrow=True) self.Wu_hh[i].set_value(state[&#39;Wu_hh_&#39; + str(i)], borrow=True) self.Wu_rz[i].set_value(state[&#39;Wu_rz_&#39; + str(i)], borrow=True) self.Bu_h[i].set_value(state[&#39;Bu_h_&#39; + str(i)], borrow=True) self.Hu[i].set_value(state[&#39;Hu_&#39; + str(i)], borrow=True) if self.user_to_output: self.Wuy.set_value(state[&#39;Wuy&#39;], borrow=True) self.Ws_init[0].set_value(state[&#39;Wu_to_s_init&#39;], borrow=True) self.Bs_init[0].set_value(state[&#39;Bu_to_s_init&#39;], borrow=True) if self.user_propagation_mode == &#39;all&#39;: self.Wu_to_s[0].set_value(state[&#39;Wu_to_s&#39;], borrow=True) def print_state(self): for i in range(len(self.session_layers)): print_norm(self.Ws_in[i], &#39;Ws_in_&#39; + str(i)) print_norm(self.Ws_hh[i], &#39;Ws_hh_&#39; + str(i)) print_norm(self.Ws_rz[i], &#39;Ws_rz_&#39; + str(i)) print_norm(self.Bs_h[i], &#39;Bs_h_&#39; + str(i)) print_norm(self.Hs[i], &#39;Hs_&#39; + str(i)) print_norm(self.Wsy, &#39;Wsy&#39;) print_norm(self.By, &#39;By&#39;) for i in range(len(self.user_layers)): print_norm(self.Wu_in[i], &#39;Wu_in_&#39; + str(i)) print_norm(self.Wu_hh[i], &#39;Wu_hh_&#39; + str(i)) print_norm(self.Wu_rz[i], &#39;Wu_rz_&#39; + str(i)) print_norm(self.Bu_h[i], &#39;Bu_h_&#39; + str(i)) print_norm(self.Hu[i], &#39;Hu_&#39; + str(i)) if self.user_to_output: print_norm(self.Wuy, &#39;Wuy&#39;) print_norm(self.Ws_init[0], &#39;Wu_to_s_init&#39;) print_norm(self.Bs_init[0], &#39;Bu_to_s_init&#39;) if self.user_propagation_mode == &#39;all&#39;: print_norm(self.Wu_to_s[0], &#39;Wu_to_s&#39;) def init(self): rnn_input_size = self.n_items if self.item_embedding is not None: self.E_item = self.init_weights((self.n_items, self.item_embedding)) if self.init_item_embeddings is not None: self.set_item_embeddings(self.E_item, self.init_item_embeddings) rnn_input_size = self.item_embedding # Initialize the session parameters self.Ws_in, self.Ws_hh, self.Ws_rz, self.Bs_h, self.Hs = [], [], [], [], [] for i in range(len(self.session_layers)): m = [] m.append( self.init_matrix((self.session_layers[i - 1] if i &gt; 0 else rnn_input_size, self.session_layers[i]))) m.append( self.init_matrix((self.session_layers[i - 1] if i &gt; 0 else rnn_input_size, self.session_layers[i]))) m.append( self.init_matrix((self.session_layers[i - 1] if i &gt; 0 else rnn_input_size, self.session_layers[i]))) self.Ws_in.append(theano.shared(value=np.hstack(m), borrow=True)) self.Ws_hh.append(self.init_weights((self.session_layers[i], self.session_layers[i]))) m2 = [] m2.append(self.init_matrix((self.session_layers[i], self.session_layers[i]))) m2.append(self.init_matrix((self.session_layers[i], self.session_layers[i]))) self.Ws_rz.append(theano.shared(value=np.hstack(m2), borrow=True)) self.Bs_h.append( theano.shared(value=np.zeros((self.session_layers[i] * 3,), dtype=theano.config.floatX), borrow=True)) self.Hs.append( theano.shared(value=np.zeros((self.batch_size, self.session_layers[i]), dtype=theano.config.floatX), borrow=True)) # Session to output weights self.Wsy = self.init_weights((self.n_items, self.session_layers[-1])) # Global output bias self.By = theano.shared(value=np.zeros((self.n_items, 1), dtype=theano.config.floatX), borrow=True) # Initialize the user parameters self.Wu_in, self.Wu_hh, self.Wu_rz, self.Bu_h, self.Hu = [], [], [], [], [] for i in range(len(self.user_layers)): m = [] m.append(self.init_matrix( (self.user_layers[i - 1] if i &gt; 0 else self.session_layers[-1], self.user_layers[i]))) m.append(self.init_matrix( (self.user_layers[i - 1] if i &gt; 0 else self.session_layers[-1], self.user_layers[i]))) m.append(self.init_matrix( (self.user_layers[i - 1] if i &gt; 0 else self.session_layers[-1], self.user_layers[i]))) self.Wu_in.append(theano.shared(value=np.hstack(m), borrow=True)) self.Wu_hh.append(self.init_weights((self.user_layers[i], self.user_layers[i]))) m2 = [] m2.append(self.init_matrix((self.user_layers[i], self.user_layers[i]))) m2.append(self.init_matrix((self.user_layers[i], self.user_layers[i]))) self.Wu_rz.append(theano.shared(value=np.hstack(m2), borrow=True)) self.Bu_h.append( theano.shared(value=np.zeros((self.user_layers[i] * 3,), dtype=theano.config.floatX), borrow=True)) self.Hu.append( theano.shared(value=np.zeros((self.batch_size, self.user_layers[i]), dtype=theano.config.floatX), borrow=True)) if self.user_to_output: # User to output weights self.Wuy = self.init_weights((self.n_items, self.user_layers[-1])) # User-to-Session parameters self.Ws_init, self.Bs_init = [], [] self.Ws_init.append(self.init_weights((self.user_layers[-1], self.session_layers[0]))) self.Bs_init.append( theano.shared(value=np.zeros((self.session_layers[0],), dtype=theano.config.floatX), borrow=True)) if self.user_propagation_mode == &#39;all&#39;: m = [] m.append(self.init_matrix((self.user_layers[-1], self.session_layers[0]))) m.append(self.init_matrix((self.user_layers[-1], self.session_layers[0]))) m.append(self.init_matrix((self.user_layers[-1], self.session_layers[0]))) self.Wu_to_s = [theano.shared(value=np.hstack(m), borrow=True)] def dropout(self, X, drop_p): if drop_p &gt; 0: retain_prob = 1 - drop_p X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX) / retain_prob return X def adam(self, param, grad, updates, sample_idx=None, epsilon=1e-6): v1 = np.float32(self.decay) v2 = np.float32(1.0 - self.decay) acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True) meang = theano.shared(param.get_value(borrow=False) * 0., borrow=True) countt = theano.shared(param.get_value(borrow=False) * 0., borrow=True) if sample_idx is None: acc_new = v1 * acc + v2 * grad ** 2 meang_new = v1 * meang + v2 * grad countt_new = countt + 1 updates[acc] = acc_new updates[meang] = meang_new updates[countt] = countt_new else: acc_s = acc[sample_idx] meang_s = meang[sample_idx] countt_s = countt[sample_idx] acc_new = v1 * acc_s + v2 * grad ** 2 meang_new = v1 * meang_s + v2 * grad countt_new = countt_s + 1.0 updates[acc] = T.set_subtensor(acc_s, acc_new) updates[meang] = T.set_subtensor(meang_s, meang_new) updates[countt] = T.set_subtensor(countt_s, countt_new) return (meang_new / (1 - v1 ** countt_new)) / (T.sqrt(acc_new / (1 - v1 ** countt_new)) + epsilon) def adagrad(self, param, grad, updates, sample_idx=None, epsilon=1e-6): acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True) if sample_idx is None: acc_new = acc + grad ** 2 updates[acc] = acc_new else: acc_s = acc[sample_idx] acc_new = acc_s + grad ** 2 updates[acc] = T.set_subtensor(acc_s, acc_new) gradient_scaling = T.cast(T.sqrt(acc_new + epsilon), theano.config.floatX) return grad / gradient_scaling def adadelta(self, param, grad, updates, sample_idx=None, epsilon=1e-6): v1 = np.float32(self.decay) v2 = np.float32(1.0 - self.decay) acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True) upd = theano.shared(param.get_value(borrow=False) * 0., borrow=True) if sample_idx is None: acc_new = acc + grad ** 2 updates[acc] = acc_new grad = T.sqrt(upd + epsilon) * grad upd_new = v1 * upd + v2 * grad ** 2 updates[upd] = upd_new else: acc_s = acc[sample_idx] acc_new = acc_s + grad ** 2 updates[acc] = T.set_subtensor(acc_s, acc_new) upd_s = upd[sample_idx] upd_new = v1 * upd_s + v2 * grad ** 2 updates[upd] = T.set_subtensor(upd_s, upd_new) grad = T.sqrt(upd_s + epsilon) * grad gradient_scaling = T.cast(T.sqrt(acc_new + epsilon), theano.config.floatX) return grad / gradient_scaling def rmsprop(self, param, grad, updates, sample_idx=None, epsilon=1e-6): v1 = np.float32(self.decay) v2 = np.float32(1.0 - self.decay) acc = theano.shared(param.get_value(borrow=False) * 0., borrow=True) if sample_idx is None: acc_new = v1 * acc + v2 * grad ** 2 updates[acc] = acc_new else: acc_s = acc[sample_idx] acc_new = v1 * acc_s + v2 * grad ** 2 updates[acc] = T.set_subtensor(acc_s, acc_new) gradient_scaling = T.cast(T.sqrt(acc_new + epsilon), theano.config.floatX) return grad / gradient_scaling def RMSprop(self, cost, params, full_params, sampled_params, sidxs, epsilon=1e-6): grads = [T.grad(cost=cost, wrt=param) for param in params] sgrads = [T.grad(cost=cost, wrt=sparam) for sparam in sampled_params] updates = OrderedDict() if self.grad_cap &gt; 0: norm = T.cast(T.sqrt(T.sum([T.sum([T.sum(g ** 2) for g in g_list]) for g_list in grads]) + T.sum( [T.sum(g ** 2) for g in sgrads])), theano.config.floatX) grads = [[T.switch(T.ge(norm, self.grad_cap), g * self.grad_cap / norm, g) for g in g_list] for g_list in grads] sgrads = [T.switch(T.ge(norm, self.grad_cap), g * self.grad_cap / norm, g) for g in sgrads] for p_list, g_list in zip(params, grads): for p, g in zip(p_list, g_list): if self.adapt: if self.adapt == &#39;adagrad&#39;: g = self.adagrad(p, g, updates) if self.adapt == &#39;rmsprop&#39;: g = self.rmsprop(p, g, updates) if self.adapt == &#39;adadelta&#39;: g = self.adadelta(p, g, updates) if self.adapt == &#39;adam&#39;: g = self.adam(p, g, updates) if self.momentum &gt; 0: velocity = theano.shared(p.get_value(borrow=False) * 0., borrow=True) velocity2 = self.momentum * velocity - np.float32(self.learning_rate) * (g + self.lmbd * p) updates[velocity] = velocity2 updates[p] = p + velocity2 else: updates[p] = p * np.float32(1.0 - self.learning_rate * self.lmbd) - np.float32( self.learning_rate) * g for i in range(len(sgrads)): g = sgrads[i] fullP = full_params[i] sample_idx = sidxs[i] sparam = sampled_params[i] if self.adapt: if self.adapt == &#39;adagrad&#39;: g = self.adagrad(fullP, g, updates, sample_idx) if self.adapt == &#39;rmsprop&#39;: g = self.rmsprop(fullP, g, updates, sample_idx) if self.adapt == &#39;adadelta&#39;: g = self.adadelta(fullP, g, updates, sample_idx) if self.adapt == &#39;adam&#39;: g = self.adam(fullP, g, updates, sample_idx) if self.lmbd &gt; 0: delta = np.float32(self.learning_rate) * (g + self.lmbd * sparam) else: delta = np.float32(self.learning_rate) * g if self.momentum &gt; 0: velocity = theano.shared(fullP.get_value(borrow=False) * 0., borrow=True) vs = velocity[sample_idx] velocity2 = self.momentum * vs - delta updates[velocity] = T.set_subtensor(vs, velocity2) updates[fullP] = T.inc_subtensor(sparam, velocity2) else: updates[fullP] = T.inc_subtensor(sparam, - delta) return updates def model(self, X, Sstart, Ustart, Hs, Hu, Y=None, drop_p_hidden_usr=0.0, drop_p_hidden_ses=0.0, drop_p_init=0.0): # # USER GRU # # update the User GRU with the last hidden state of the Session GRU # NOTE: the User GRU gets actually updated only when a new session starts user_in = T.dot(Hs[-1], self.Wu_in[0]) + self.Bu_h[0] user_in = user_in.T # ^ 3 * user_layers[0] x batch_size rz_u = T.nnet.sigmoid(user_in[self.user_layers[0]:] + T.dot(Hu[0], self.Wu_rz[0]).T) # ^ 2 * user_layers[0] x batch_size h_u = self.hidden_activation(T.dot(Hu[0] * rz_u[:self.user_layers[0]].T, self.Wu_hh[0]).T + user_in[:self.user_layers[0]]) # ^ user_layers[0] x batch_size z = rz_u[self.user_layers[0]:].T # batch_size x user_layers[0] h_u = (1.0 - z) * Hu[0] + z * h_u.T h_u = self.dropout(h_u, drop_p_hidden_usr) # ^ batch_size x user_layers[0] # update the User GRU only when a new session starts # Hu contains the state of the previous session h_u = Hu[0] * (1 - Sstart[:, None]) + h_u * Sstart[:, None] # ^ batch_size x user_layers[0] # reset the user network state for new users h_u = T.zeros_like(h_u) * Ustart[:, None] + h_u * (1 - Ustart[:, None]) Hu_new = [h_u] for i in range(1, len(self.user_layers)): user_in = T.dot(h_u, self.Wu_in[i]) + self.Bu_h[i] user_in = user_in.T rz_u = T.nnet.sigmoid(user_in[self.user_layers[i]:] + T.dot(Hu[i], self.Wu_rz[i]).T) h_u = self.hidden_activation(T.dot(Hu[i] * rz_u[:self.user_layers[i]].T, self.Wu_hh[i]).T + user_in[:self.user_layers[i]]) z = rz_u[self.user_layers[i]:].T h_u = (1.0 - z) * Hu[i] + z * h_u.T h_u = self.dropout(h_u, drop_p_hidden_usr) h_u = Hu[i] * (1 - Sstart[:, None]) + h_u * Sstart[:, None] h_u = T.zeros_like(h_u) * Ustart[:, None] + h_u * (1 - Ustart[:, None]) Hu_new.append(h_u) # # SESSION GRU # # Process the input items if self.item_embedding is not None: # get the item embedding SE_item = self.E_item[X] # sampled item embedding vec = T.dot(SE_item, self.Ws_in[0]) + self.Bs_h[0] Sin = SE_item else: Sx = self.Ws_in[0][X] vec = Sx + self.Bs_h[0] Sin = Sx session_in = vec.T # ^ session_layers[0] x batch_size # initialize the h_s with h_c only for starting sessions h_s_init = self.dropout(self.s_init_act(T.dot(h_u, self.Ws_init[0]) + self.Bs_init), drop_p_init) h_s = Hs[0] * (1 - Sstart[:, None]) + h_s_init * Sstart[:, None] # reset h_s for starting users h_s = h_s * (1 - Ustart[:, None]) + T.zeros_like(h_s) * Ustart[:, None] self.h_s_init = h_s if self.user_propagation_mode == &#39;all&#39;: # this propagates the bias throughout all the session user_bias = T.dot(h_u, self.Wu_to_s[0]).T # ^ 3*session_layers[0] x batch_size # update the Session GRU rz_s = T.nnet.sigmoid(user_bias[self.session_layers[0]:] + session_in[self.session_layers[0]:] + T.dot(h_s, self.Ws_rz[0]).T) # ^ 2*session_layers[0] x batch_size h_s = self.hidden_activation(T.dot(h_s * rz_s[:self.session_layers[0]].T, self.Ws_hh[0]).T + session_in[:self.session_layers[0]]) # ^ session_layers[0] x batch_size else: rz_s = T.nnet.sigmoid(session_in[self.session_layers[0]:] + T.dot(h_s, self.Ws_rz[0]).T) h_s = self.hidden_activation(T.dot(h_s * rz_s[:self.session_layers[0]].T, self.Ws_hh[0]).T + session_in[:self.session_layers[0]]) z = rz_s[self.session_layers[0]:].T # ^ batch_size x session_layers[0] h_s = (1.0 - z) * Hs[0] + z * h_s.T h_s = self.dropout(h_s, drop_p_hidden_ses) # ^ batch_size x session_layers[0] Hs_new = [h_s] for i in range(1, len(self.session_layers)): session_in = T.dot(h_s, self.Ws_in[i]) + self.Bs_h[i] session_in = session_in.T rz_s = T.nnet.sigmoid(session_in[self.session_layers[i]:] + T.dot(Hs[i], self.Ws_rz[i]).T) h_s = self.hidden_activation(T.dot(Hs[i] * rz_s[:self.session_layers[i]].T, self.Ws_hh[i]).T + session_in[:self.session_layers[i]]) z = rz_s[self.session_layers[i]:].T h_s = (1.0 - z) * Hs[i] + z * h_s.T h_s = self.dropout(h_s, drop_p_hidden_ses) Hs_new.append(h_s) self.h_s_new = h_s if Y is not None: Ssy = self.Wsy[Y] SBy = self.By[Y] preact = T.dot(h_s, Ssy.T) + SBy.flatten() sampled_params = [Sin, Ssy, SBy] if self.user_to_output: Scy = self.Wuy[Y] preact += T.dot(h_u, Scy.T) sampled_params.append(Scy) y = self.final_activation(preact) return Hs_new, Hu_new, y, sampled_params else: preact = T.dot(h_s, self.Wsy.T) + self.By.flatten() if self.user_to_output: preact += T.dot(h_u, self.Wuy.T) y = self.final_activation(preact) return Hs_new, Hu_new, y, [Sin] def fit(self, train_data, valid_data=None, retrain=False, sample_store=10000000, patience=3, margin=1.003, save_to=None, load_from=None): &#39;&#39;&#39; Trains the network. Parameters -- train_data : pandas.DataFrame Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps). It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties). valid_data: pandas.DataFrame Validation data. If not none, it enables early stopping. Contains the transactions in the same format as in train_data, and it is used exclusively to compute the loss after each training iteration over train_data. retrain : boolean If False, do normal train. If True, do additional train (weights from previous trainings are kept as the initial network) (default: False) sample_store : int If additional negative samples are used (n_sample &gt; 0), the efficiency of GPU utilization can be sped up, by precomputing a large batch of negative samples (and recomputing when necessary). This parameter regulizes the size of this precomputed ID set. Its value is the maximum number of int values (IDs) to be stored. Precomputed IDs are stored in the RAM. For the most efficient computation, a balance must be found between storing few examples and constantly interrupting GPU computations for a short time vs. computing many examples and interrupting GPU computations for a long time (but rarely). patience: int Patience of the early stopping procedure. Number of iterations with not decreasing validation loss before terminating the training procedure margin: float Margin of early stopping. Percentage improvement over the current best validation loss to do not incur into a patience penalty save_to: string Path where to save the state of the best model resulting from training. If early stopping is enabled, saves the model with the lowest validation loss. Otherwise, saves the model corresponding to the last iteration. load_from: string Path from where to load the state of a previously saved model. &#39;&#39;&#39; self.predict = None self.update = None self.error_during_train = False itemids = train_data[self.item_key].unique() self.n_items = len(itemids) self.init() # initialize the network if load_from: logger.info(&#39;Resuming from state: {}&#39;.format(load_from)) self.load_state(pickle.load(open(load_from, &#39;rb&#39;))) if not retrain: self.itemidmap = pd.Series(data=np.arange(self.n_items), index=itemids) train_data = pd.merge(train_data, pd.DataFrame({self.item_key: itemids, &#39;ItemIdx&#39;: self.itemidmap[itemids].values}), on=self.item_key, how=&#39;inner&#39;) user_indptr, offset_sessions = self.preprocess_data(train_data) else: raise Exception(&#39;Not supported yet!&#39;) if valid_data is not None: valid_data = pd.merge(valid_data, pd.DataFrame({self.item_key: itemids, &#39;ItemIdx&#39;: self.itemidmap[itemids].values}), on=self.item_key, how=&#39;inner&#39;) user_indptr_valid, offset_sessions_valid = self.preprocess_data(valid_data) X, Y = T.ivectors(2) Sstart, Ustart = T.fvectors(2) Hs_new, Hu_new, Y_pred, sampled_params = self.model(X, Sstart, Ustart, self.Hs, self.Hu, Y, drop_p_hidden_usr=self.dropout_p_hidden_usr, drop_p_hidden_ses=self.dropout_p_hidden_ses, drop_p_init=self.dropout_p_init) cost = self.loss_function(Y_pred) # set up the parameter and sampled parameter vectors if self.item_embedding is None: params = [self.Ws_in[1:], self.Ws_hh, self.Ws_rz, self.Bs_h, self.Ws_init, self.Bs_init, self.Wu_in, self.Wu_hh, self.Wu_rz, self.Bu_h] full_params = [self.Ws_in[0], self.Wsy, self.By] else: params = [self.Ws_in, self.Ws_hh, self.Ws_rz, self.Bs_h, self.Ws_init, self.Bs_init, self.Wu_in, self.Wu_hh, self.Wu_rz, self.Bu_h] full_params = [self.E_item, self.Wsy, self.By] if self.user_propagation_mode == &#39;all&#39;: params.append(self.Wu_to_s) sidxs = [X, Y, Y] if self.user_to_output: full_params.append(self.Wuy) sidxs.append(Y) updates = self.RMSprop(cost, params, full_params, sampled_params, sidxs) eval_updates = OrderedDict() # Update the hidden states of the Session GRU for i in range(len(self.Hs)): updates[self.Hs[i]] = Hs_new[i] eval_updates[self.Hs[i]] = Hs_new[i] # Update the hidden states of the User GRU for i in range(len(self.Hu)): updates[self.Hu[i]] = Hu_new[i] eval_updates[self.Hu[i]] = Hu_new[i] # Compile the training and evaluation functions self.train_function = function(inputs=[X, Sstart, Ustart, Y], outputs=cost, updates=updates, allow_input_downcast=True, on_unused_input=&#39;warn&#39;) self.eval_function = function(inputs=[X, Sstart, Ustart, Y], outputs=cost, updates=eval_updates, allow_input_downcast=True, on_unused_input=&#39;warn&#39;) # Negative item sampling if self.n_sample: self.neg_sampler = Sampler(train_data, self.n_sample, rng=self.rng, item_key=self.item_key, sample_alpha=self.sample_alpha, sample_store=sample_store) # Training starts here best_valid, best_state = None, None my_patience = patience epoch = 0 while epoch &lt; self.n_epochs and my_patience &gt; 0: train_cost = self.iterate(train_data, self.train_function, offset_sessions, user_indptr) # self.print_state() if np.isnan(train_cost): return if valid_data is not None: valid_cost = self.iterate(valid_data, self.eval_function, offset_sessions_valid, user_indptr_valid) if best_valid is None or valid_cost &lt; best_valid: best_valid = valid_cost best_state = self.save_state() my_patience = patience elif valid_cost &gt;= best_valid * margin: my_patience -= 1 logger.info( &#39;Epoch {} - train cost: {:.4f} - valid cost: {:.4f} (patience: {})&#39;.format(epoch, train_cost, valid_cost, my_patience)) else: logger.info(&#39;Epoch {} - train cost: {:.4f}&#39;.format(epoch, train_cost)) epoch += 1 if my_patience == 0: logger.info(&#39;Early stopping condition met!&#39;) if best_state: # always load the state associated with the best validation cost self.load_state(best_state) if save_to: if best_state: state = best_state else: state = self.save_state() logger.info(&#39;Saving model to: {}&#39;.format(save_to)) pickle.dump(state, open(save_to, &#39;wb&#39;), pickle.HIGHEST_PROTOCOL) def iterate(self, data, fun, offset_sessions, user_indptr, reset_state=True): if reset_state: # Reset session layers for i in range(len(self.session_layers)): self.Hs[i].set_value(np.zeros((self.batch_size, self.session_layers[i]), dtype=theano.config.floatX), borrow=True) # Reset user layers for i in range(len(self.user_layers)): self.Hu[i].set_value(np.zeros((self.batch_size, self.user_layers[i]), dtype=theano.config.floatX), borrow=True) # variables to manage iterations over users n_users = len(user_indptr) offset_users = offset_sessions[user_indptr] user_idx_arr = np.arange(n_users - 1) user_iters = np.arange(self.batch_size) user_maxiter = user_iters.max() user_start = offset_users[user_idx_arr[user_iters]] user_end = offset_users[user_idx_arr[user_iters] + 1] # variables to manage iterations over sessions session_iters = user_indptr[user_iters] session_start = offset_sessions[session_iters] session_end = offset_sessions[session_iters + 1] sstart = np.zeros((self.batch_size,), dtype=np.float32) ustart = np.zeros((self.batch_size,), dtype=np.float32) finished = False n = 0 c = [] while not finished: session_minlen = (session_end - session_start).min() out_idx = data.ItemIdx.values[session_start] for i in range(session_minlen - 1): in_idx = out_idx out_idx = data.ItemIdx.values[session_start + i + 1] if self.n_sample: sample = self.neg_sampler.next_sample() y = np.hstack([out_idx, sample]) else: y = out_idx cost = fun(in_idx, sstart, ustart, y) n += 1 # reset sstart and ustart sstart = np.zeros_like(sstart, dtype=np.float32) ustart = np.zeros_like(ustart, dtype=np.float32) c.append(cost) if np.isnan(cost): logger.error(&#39;NaN error!&#39;) self.error_during_train = True return session_start = session_start + session_minlen - 1 session_start_mask = np.arange(len(session_iters))[(session_end - session_start) &lt;= 1] sstart[session_start_mask] = 1 for idx in session_start_mask: session_iters[idx] += 1 if session_iters[idx] + 1 &gt;= len(offset_sessions): finished = True break session_start[idx] = offset_sessions[session_iters[idx]] session_end[idx] = offset_sessions[session_iters[idx] + 1] # reset the User hidden state at user change user_change_mask = np.arange(len(user_iters))[(user_end - session_start &lt;= 0)] ustart[user_change_mask] = 1 for idx in user_change_mask: user_maxiter += 1 if user_maxiter + 1 &gt;= len(offset_users): finished = True break user_iters[idx] = user_maxiter user_start[idx] = offset_users[user_maxiter] user_end[idx] = offset_users[user_maxiter + 1] session_iters[idx] = user_indptr[user_maxiter] session_start[idx] = offset_sessions[session_iters[idx]] session_end[idx] = offset_sessions[session_iters[idx] + 1] avgc = np.mean(c) return avgc def predict_next_batch(self, session_ids, input_item_ids, input_user_ids, predict_for_item_ids=None, batch=100): &#39;&#39;&#39; Gives predicton scores for a selected set of items. Can be used in batch mode to predict for multiple independent events (i.e. events of different sessions) at once and thus speed up evaluation. If the session ID at a given coordinate of the session_ids parameter remains the same during subsequent calls of the function, the corresponding hidden state of the network will be kept intact (i.e. that&#39;s how one can predict an item to a session). If it changes, the hidden state of the network is reset to zeros. Parameters -- session_ids : 1D array Contains the session IDs of the events of the batch. Its length must equal to the prediction batch size (batch param). input_item_ids : 1D array Contains the item IDs of the events of the batch. Every item ID must be must be in the training data of the network. Its length must equal to the prediction batch size (batch param). input_user_ids : 1D array Contains the user IDs of the events of the batch. Every user ID must be must be in the training data of the network. Its length must equal to the prediction batch size (batch param). predict_for_item_ids : 1D array (optional) IDs of items for which the network should give prediction scores. Every ID must be in the training set. The default value is None, which means that the network gives prediction on its every output (i.e. for all items in the training set). batch : int Prediction batch size. Returns -- out : pandas.DataFrame Prediction scores for selected items for every event of the batch. Columns: events of the batch; rows: items. Rows are indexed by the item IDs. &#39;&#39;&#39; if self.error_during_train: raise Exception if self.predict is None or self.predict_batch != batch: X, Y = T.ivectors(2) Sstart, Ustart = T.fvectors(2) for i in range(len(self.session_layers)): self.Hs[i].set_value(np.zeros((batch, self.session_layers[i]), dtype=theano.config.floatX), borrow=True) for i in range(len(self.user_layers)): self.Hu[i].set_value(np.zeros((batch, self.user_layers[i]), dtype=theano.config.floatX), borrow=True) if predict_for_item_ids is not None: Hs_new, Hu_new, yhat, _ = self.model(X, Sstart, Ustart, self.Hs, self.Hu, Y) else: Hs_new, Hu_new, yhat, _ = self.model(X, Sstart, Ustart, self.Hs, self.Hu) updatesH = OrderedDict() for i in range(len(self.Hs)): updatesH[self.Hs[i]] = Hs_new[i] for i in range(len(self.Hu)): updatesH[self.Hu[i]] = Hu_new[i] if predict_for_item_ids is not None: self.predict = function(inputs=[X, Sstart, Ustart, Y], outputs=yhat, updates=updatesH, on_unused_input=&#39;warn&#39;, allow_input_downcast=True) else: self.predict = function(inputs=[X, Sstart, Ustart], outputs=yhat, updates=updatesH, on_unused_input=&#39;warn&#39;, allow_input_downcast=True) self.current_session = np.ones(batch) * -1 self.current_users = np.ones(batch) * -1 self.predict_batch = batch session_change = session_ids != self.current_session self.current_session = session_ids.copy() user_change = input_user_ids != self.current_users self.current_users = input_user_ids.copy() in_idxs = self.itemidmap[input_item_ids] if predict_for_item_ids is not None: iIdxs = self.itemidmap[predict_for_item_ids] preds = np.asarray(self.predict(in_idxs, session_change, user_change, iIdxs)).T return pd.DataFrame(data=preds, index=predict_for_item_ids) else: preds = np.asarray(self.predict(in_idxs, session_change, user_change)).T return pd.DataFrame(data=preds, index=self.itemidmap.index) . . class RNNRecommender(ISeqRecommender): &quot;&quot;&quot; A **simplified** interface to Recurrent Neural Network models for Session-based recommendation. Based on the following two papers: * Recurrent Neural Networks with Top-k Gains for Session-based Recommendations, Hidasi and Karatzoglou, CIKM 2018 * Personalizing Session-based Recommendation with Hierarchical Recurrent Neural Networks, Quadrana et al, Recsys 2017 &quot;&quot;&quot; def __init__(self, session_layers, user_layers=None, batch_size=32, learning_rate=0.1, momentum=0.0, dropout=None, epochs=10, personalized=False): &quot;&quot;&quot; :param session_layers: number of units per layer used at session level. It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks. :param user_layers: number of units per layer used at user level. Required only by personalized models. It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks. :param batch_size: the mini-batch size used in training :param learning_rate: the learning rate used in training (Adagrad optimized) :param momentum: the momentum coefficient used in training :param dropout: dropout coefficients. If personalized=False, it&#39;s a float value for the hidden-layer(s) dropout. If personalized=True, it&#39;s a 3-tuple with the values for the dropout of (user hidden, session hidden, user-to-session hidden) layers. :param epochs: number of training epochs :param personalized: whether to train a personalized model using the HRNN model. It will require user ids at prediction time. &quot;&quot;&quot; super(RNNRecommender).__init__() if isinstance(session_layers, int): session_layers = [session_layers] if isinstance(user_layers, int): user_layers = [user_layers] self.session_layers = session_layers self.user_layers = user_layers self.batch_size = batch_size self.learning_rate = learning_rate self.momentum = momentum if dropout is None: if not personalized: dropout = 0.0 else: dropout = (0.0, 0.0, 0.0) self.dropout = dropout self.epochs = epochs self.personalized = personalized self.pseudo_session_id = 0 def __str__(self): return &#39;RNNRecommender(&#39; &#39;session_layers={session_layers}, &#39; &#39;user_layers={user_layers}, &#39; &#39;batch_size={batch_size}, &#39; &#39;learning_rate={learning_rate}, &#39; &#39;momentum={momentum}, &#39; &#39;dropout={dropout}, &#39; &#39;epochs={epochs}, &#39; &#39;personalized={personalized}, &#39; &#39;)&#39;.format(**self.__dict__) def fit(self, train_data): self.logger.info(&#39;Converting training data to GRU4Rec format&#39;) # parse training data to GRU4Rec format train_data = dataset_to_gru4rec_format(dataset=train_data) if not self.personalized: # fit GRU4Rec self.model = GRU4Rec(layers=self.session_layers, n_epochs=self.epochs, batch_size=self.batch_size, learning_rate=self.learning_rate, momentum=self.momentum, dropout_p_hidden=self.dropout, session_key=&#39;session_id&#39;, item_key=&#39;item_id&#39;, time_key=&#39;ts&#39;) else: if self.user_layers is None: raise ValueError(&#39;You should set the value of user_layers before training the personalized model.&#39;) if len(self.dropout) != 3: raise ValueError(&#39;dropout should be a 3 tuple with &#39; &#39;(user hidden, session hidden, user-to-session hidden) dropout values.&#39;) self.model = HGRU4Rec(session_layers=self.session_layers, user_layers=self.user_layers, batch_size=self.batch_size, n_epochs=self.epochs, learning_rate=self.learning_rate, momentum=self.momentum, dropout_p_hidden_usr=self.dropout[0], dropout_p_hidden_ses=self.dropout[1], dropout_p_init=self.dropout[2], session_key=&#39;session_id&#39;, user_key=&#39;user_id&#39;, item_key=&#39;item_id&#39;, time_key=&#39;ts&#39;) self.logger.info(&#39;Training started&#39;) self.model.fit(train_data) self.logger.info(&#39;Training completed&#39;) def recommend(self, user_profile, user_id=None): if not self.personalized: for item in user_profile: pred = self.model.predict_next_batch(np.array([self.pseudo_session_id]), np.array([item]), batch=1) else: if user_id is None: raise ValueError(&#39;user_id required by personalized models&#39;) for item in user_profile: pred = self.model.predict_next_batch(np.array([self.pseudo_session_id]), np.array([item]), np.array([user_id]), batch=1) # sort items by predicted score pred.sort_values(0, ascending=False, inplace=True) # increase the psuedo-session id so that future call to recommend() won&#39;t be connected self.pseudo_session_id += 1 # convert to the required output format return [([x.index], x._2) for x in pred.reset_index().itertuples()] . . rnnrecommender = RNNRecommender(session_layers=[20], batch_size=16, learning_rate=0.1, momentum=0.1, dropout=0.1, epochs=5, personalized=False) rnnrecommender.fit(train_data) . 2021-04-25 14:00:57,953 - INFO - Converting training data to GRU4Rec format 2021-04-25 14:00:57,980 - INFO - Training started WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library. 2021-04-25 14:01:11,615 - WARNING - We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library. . Epoch0 loss: 0.627941 Epoch1 loss: 0.533681 Epoch2 loss: 0.505859 Epoch3 loss: 0.491371 . 2021-04-25 14:02:15,525 - INFO - Training completed . Epoch4 loss: 0.483965 . Personalized RNN . Here we fit the recommedation algorithm over the sessions in the training set. . This is a simplified interface to Recurrent Neural Network models for Session-based recommendation. Based on the following two papers: . Recurrent Neural Networks with Top-k Gains for Session-based Recommendations, Hidasi and Karatzoglou, CIKM 2018 | Personalizing Session-based Recommendation with Hierarchical Recurrent Neural Networks, Quadrana et al, Recsys 2017 | . In this notebook, we will consider the session-aware (personalized) version of the algorithm. Here&#39;s a schematic representation of the model: . . Each user session goes through a session RNN, which models short-term user preferences. At the end of each session, the state of the session RNN is used to update a user RNN, which models more long-term user preferences. It&#39;s state is passed forward to the next session RNN, which can now personalize recommendations depending on both short-term and long-term user interests. . The hyper-parameters of the model are: . session_layers: number of units per layer used at session level. It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks. | user_layers: number of units per layer used at user level. Required only by personalized models. It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks. | batch_size: the mini-batch size used in training | learning_rate: the learning rate used in training (Adagrad optimized) | momentum: the momentum coefficient used in training | dropout: it&#39;s a 3-tuple with the values for the dropout of (user hidden, session hidden, user-to-session hidden) layers. | epochs: number of training epochs | personalized: whether to train a personalized model using the HRNN model (True in this case). | . NOTE: HGRU4Rec originally has more hyper-parameters. Going through all of them is out from the scope of this tutorial, but we suggest to check-out the original source code here in case you are interested. . prnnrecommender = RNNRecommender(session_layers=[20], user_layers=[20], batch_size=16, learning_rate=0.5, momentum=0.1, dropout=(0.1,0.1,0.1), epochs=5, personalized=True) prnnrecommender.fit(train_data) . 2021-04-25 14:02:15,598 - INFO - Converting training data to GRU4Rec format 2021-04-25 14:02:15,633 - INFO - Training started 2021-04-25 14:02:51,671 - INFO - Epoch 0 - train cost: 1.0400 2021-04-25 14:02:52,260 - INFO - Epoch 1 - train cost: 0.9588 2021-04-25 14:02:52,854 - INFO - Epoch 2 - train cost: 0.9023 2021-04-25 14:02:53,433 - INFO - Epoch 3 - train cost: 0.8703 2021-04-25 14:02:54,036 - INFO - Epoch 4 - train cost: 0.8492 2021-04-25 14:02:54,039 - INFO - Training completed . KNN recommender . The class KNNRecommender takes the following initialization hyper-parameters: . model: One among the following KNN models: . iknn: ItemKNN, item-to-item KNN based on the last item in the session to determine the items to be recommended. | sknn: SessionKNN, compares the entire current session with the past sessions in the training data to determine the items to be recommended. | v-sknn: VMSessionKNN, use linearly decayed real-valued vectors to encode the current session, then compares the current session with the past sessions in the training data using the dot-product to determine the items to be recommended. | s-sknn: SeqSessionKNN, this variant also puts more weight on elements that appear later in the session by using a custom scoring function (see the paper by Ludewng and Jannach). | sf-sknn: SeqFilterSessionKNN, this variant also puts more weight on elements that appear later in the session in a more restrictive way by using a custom scoring function (see the paper by Ludewng and Jannach). | . | param init_args: The model initialization arguments. See the following initializations or check util.knn for more details on each model: . iknn: ItemKNN(n_sims=100, lmbd=20, alpha=0.5) | sknn: SessionKNN(k, sample_size=500, sampling=&#39;recent&#39;, similarity=&#39;jaccard&#39;, remind=False, pop_boost=0) | v-sknn: VMSessionKNN(k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;cosine&#39;, weighting=&#39;div&#39;, dwelling_time=False, last_n_days=None, last_n_clicks=None, extend=False, weighting_score=&#39;div_score&#39;, weighting_time=False, normalize=True) | s-knn: SeqSessionKNN(k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;jaccard&#39;, weighting=&#39;div&#39;, remind=False, pop_boost=0, extend=False, normalize=True) | sf-sknn: SeqFilterSessionKNN(k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;jaccard&#39;, remind=False, pop_boost=0,extend=False, normalize=True) | . | . class ItemKNN: &#39;&#39;&#39; ItemKNN(n_sims = 100, lmbd = 20, alpha = 0.5, session_key = &#39;SessionId&#39;, item_key = &#39;ItemId&#39;, time_key = &#39;Time&#39;) Item-to-item predictor that computes the the similarity to all items to the given item. Similarity of two items is given by: .. math:: s_{i,j}= sum_{s}I {(s,i) in D &amp; (s,j) in D } / (supp_i+ lambda)^{ alpha}(supp_j+ lambda)^{1- alpha} Parameters -- n_sims : int Only give back non-zero scores to the N most similar items. Should be higher or equal than the cut-off of your evaluation. (Default value: 100) lmbd : float Regularization. Discounts the similarity of rare items (incidental co-occurrences). (Default value: 20) alpha : float Balance between normalizing with the supports of the two items. 0.5 gives cosine similarity, 1.0 gives confidence (as in association rules). session_key : string header of the session ID column in the input file (default: &#39;SessionId&#39;) item_key : string header of the item ID column in the input file (default: &#39;ItemId&#39;) time_key : string header of the timestamp column in the input file (default: &#39;Time&#39;) &#39;&#39;&#39; def __init__(self, n_sims=100, lmbd=20, alpha=0.5, session_key=&#39;SessionId&#39;, item_key=&#39;ItemId&#39;, time_key=&#39;Time&#39;): self.n_sims = n_sims self.lmbd = lmbd self.alpha = alpha self.item_key = item_key self.session_key = session_key self.time_key = time_key def fit(self, data): &#39;&#39;&#39; Trains the predictor. Parameters -- data: pandas.DataFrame Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps). It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties). &#39;&#39;&#39; data.set_index(np.arange(len(data)), inplace=True) self.itemids = data[self.item_key].unique() n_items = len(self.itemids) data = pd.merge(data, pd.DataFrame({self.item_key: self.itemids, &#39;ItemIdx&#39;: np.arange(len(self.itemids))}), on=self.item_key, how=&#39;inner&#39;) sessionids = data[self.session_key].unique() data = pd.merge(data, pd.DataFrame({self.session_key: sessionids, &#39;SessionIdx&#39;: np.arange(len(sessionids))}), on=self.session_key, how=&#39;inner&#39;) supp = data.groupby(&#39;SessionIdx&#39;).size() session_offsets = np.zeros(len(supp) + 1, dtype=np.int32) session_offsets[1:] = supp.cumsum() index_by_sessions = data.sort_values([&#39;SessionIdx&#39;, self.time_key]).index.values supp = data.groupby(&#39;ItemIdx&#39;).size() item_offsets = np.zeros(n_items + 1, dtype=np.int32) item_offsets[1:] = supp.cumsum() index_by_items = data.sort_values([&#39;ItemIdx&#39;, self.time_key]).index.values self.sims = dict() for i in range(n_items): iarray = np.zeros(n_items) start = item_offsets[i] end = item_offsets[i + 1] for e in index_by_items[start:end]: uidx = data.SessionIdx.values[e] ustart = session_offsets[uidx] uend = session_offsets[uidx + 1] user_events = index_by_sessions[ustart:uend] iarray[data.ItemIdx.values[user_events]] += 1 iarray[i] = 0 norm = np.power((supp[i] + self.lmbd), self.alpha) * np.power((supp.values + self.lmbd), (1.0 - self.alpha)) norm[norm == 0] = 1 iarray = iarray / norm indices = np.argsort(iarray)[-1:-1 - self.n_sims:-1] self.sims[self.itemids[i]] = pd.Series(data=iarray[indices], index=self.itemids[indices]) def predict_next(self, session_id, input_item_id, predict_for_item_ids=None, skip=False, type=&#39;view&#39;, timestamp=0): &#39;&#39;&#39; Gives predicton scores for a selected set of items on how likely they be the next item in the session. Parameters -- session_id : int or string The session IDs of the event. input_item_id : int or string The item ID of the event. Must be in the set of item IDs of the training set. predict_for_item_ids : 1D array IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set. Returns -- out : pandas.Series Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs. &#39;&#39;&#39; if predict_for_item_ids is None: predict_for_item_ids = self.itemids preds = np.zeros(len(predict_for_item_ids)) sim_list = self.sims[input_item_id] mask = np.in1d(predict_for_item_ids, sim_list.index) preds[mask] = sim_list[predict_for_item_ids[mask]] return pd.Series(data=preds, index=predict_for_item_ids) . . class SeqFilterSessionKNN: &#39;&#39;&#39; SessionKNN( k, sample_size=500, sampling=&#39;recent&#39;, similarity = &#39;jaccard&#39;, remind=False, pop_boost=0, session_key = &#39;SessionId&#39;, item_key= &#39;ItemId&#39;) Parameters -- k : int Number of neighboring session to calculate the item scores from. (Default value: 100) sample_size : int Defines the length of a subset of all training sessions to calculate the nearest neighbors from. (Default value: 500) sampling : string String to define the sampling method for sessions (recent, random). (default: recent) similarity : string String to define the method for the similarity calculation (jaccard, cosine, binary, tanimoto). (default: jaccard) remind : bool Should the last items of the current session be boosted to the top as reminders pop_boost : int Push popular items in the neighbor sessions by this factor. (default: 0 to leave out) extend : bool Add evaluated sessions to the maps normalize : bool Normalize the scores in the end session_key : string Header of the session ID column in the input file. (default: &#39;SessionId&#39;) item_key : string Header of the item ID column in the input file. (default: &#39;ItemId&#39;) time_key : string Header of the timestamp column in the input file. (default: &#39;Time&#39;) &#39;&#39;&#39; def __init__(self, k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;jaccard&#39;, remind=False, pop_boost=0, extend=False, normalize=True, session_key=&#39;SessionId&#39;, item_key=&#39;ItemId&#39;, time_key=&#39;Time&#39;): self.remind = remind self.k = k self.sample_size = sample_size self.sampling = sampling self.similarity = similarity self.pop_boost = pop_boost self.session_key = session_key self.item_key = item_key self.time_key = time_key self.extend = extend self.normalize = normalize # updated while recommending self.session = -1 self.session_items = [] self.relevant_sessions = set() # cache relations once at startup self.session_item_map = dict() self.item_session_map = dict() self.session_time = dict() self.followed_by = dict() self.sim_time = 0 def fit(self, train, items=None): &#39;&#39;&#39; Trains the predictor. Parameters -- data: pandas.DataFrame Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps). It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties). &#39;&#39;&#39; index_session = train.columns.get_loc(self.session_key) index_item = train.columns.get_loc(self.item_key) index_time = train.columns.get_loc(self.time_key) self.itemids = train[self.item_key].unique() session = -1 session_items = set() last_item = -1 time = -1 # cnt = 0 for row in train.itertuples(index=False): # cache items of sessions if row[index_session] != session: if len(session_items) &gt; 0: self.session_item_map.update({session: session_items}) # cache the last time stamp of the session self.session_time.update({session: time}) session = row[index_session] session_items = set() else: if last_item != -1: # fill followed by map for filtering of candidate items if not last_item in self.followed_by: self.followed_by[last_item] = set() self.followed_by[last_item].add(row[index_item]) time = row[index_time] session_items.add(row[index_item]) # cache sessions involving an item map_is = self.item_session_map.get(row[index_item]) if map_is is None: map_is = set() self.item_session_map.update({row[index_item]: map_is}) map_is.add(row[index_session]) last_item = row[index_item] # Add the last tuple self.session_item_map.update({session: session_items}) self.session_time.update({session: time}) def predict_next(self, session_id, input_item_id, predict_for_item_ids=None, skip=False, type=&#39;view&#39;, timestamp=0): &#39;&#39;&#39; Gives predicton scores for a selected set of items on how likely they be the next item in the session. Parameters -- session_id : int or string The session IDs of the event. input_item_id : int or string The item ID of the event. Must be in the set of item IDs of the training set. predict_for_item_ids : 1D array IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set. Returns -- out : pandas.Series Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs. &#39;&#39;&#39; # gc.collect() # process = psutil.Process(os.getpid()) # print( &#39;cknn.predict_next: &#39;, process.memory_info().rss, &#39; memory used&#39;) if (self.session != session_id): # new session if (self.extend): item_set = set(self.session_items) self.session_item_map[self.session] = item_set; for item in item_set: map_is = self.item_session_map.get(item) if map_is is None: map_is = set() self.item_session_map.update({item: map_is}) map_is.add(self.session) ts = time.time() self.session_time.update({self.session: ts}) last_item = -1 for item in self.session_items: if last_item != -1: if not last_item in self.followed_by: self.followed_by[last_item] = set() self.followed_by[last_item].add(item) last_item = item self.session = session_id self.session_items = list() self.relevant_sessions = set() if type == &#39;view&#39;: self.session_items.append(input_item_id) if skip: return neighbors = self.find_neighbors(set(self.session_items), input_item_id, session_id) scores = self.score_items(neighbors, input_item_id) # add some reminders if self.remind: reminderScore = 5 takeLastN = 3 cnt = 0 for elem in self.session_items[-takeLastN:]: cnt = cnt + 1 # reminderScore = reminderScore + (cnt/100) oldScore = scores.get(elem) newScore = 0 if oldScore is None: newScore = reminderScore else: newScore = oldScore + reminderScore # print &#39;old score &#39;, oldScore # update the score and add a small number for the position newScore = (newScore * reminderScore) + (cnt / 100) scores.update({elem: newScore}) # push popular ones if self.pop_boost &gt; 0: pop = self.item_pop(neighbors) # Iterate over the item neighbors # print itemScores for key in scores: item_pop = pop.get(key) # Gives some minimal MRR boost? scores.update({key: (scores[key] + (self.pop_boost * item_pop))}) # Create things in the format .. if predict_for_item_ids is None: predict_for_item_ids = self.itemids predictions = np.zeros(len(predict_for_item_ids)) mask = np.in1d(predict_for_item_ids, list(scores.keys())) items = predict_for_item_ids[mask] values = [scores[x] for x in items] predictions[mask] = values series = pd.Series(data=predictions, index=predict_for_item_ids) if self.normalize: series = series / series.max() return series def item_pop(self, sessions): &#39;&#39;&#39; Returns a dict(item,score) of the item popularity for the given list of sessions (only a set of ids) Parameters -- sessions: set Returns -- out : dict &#39;&#39;&#39; result = dict() max_pop = 0 for session, weight in sessions: items = self.items_for_session(session) for item in items: count = result.get(item) if count is None: result.update({item: 1}) else: result.update({item: count + 1}) if (result.get(item) &gt; max_pop): max_pop = result.get(item) for key in result: result.update({key: (result[key] / max_pop)}) return result def jaccard(self, first, second): &#39;&#39;&#39; Calculates the jaccard index for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; sc = time.clock() intersection = len(first &amp; second) union = len(first | second) res = intersection / union self.sim_time += (time.clock() - sc) return res def cosine(self, first, second): &#39;&#39;&#39; Calculates the cosine similarity for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; li = len(first &amp; second) la = len(first) lb = len(second) result = li / sqrt(la) * sqrt(lb) return result def tanimoto(self, first, second): &#39;&#39;&#39; Calculates the cosine tanimoto similarity for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; li = len(first &amp; second) la = len(first) lb = len(second) result = li / (la + lb - li) return result def binary(self, first, second): &#39;&#39;&#39; Calculates the ? for 2 sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; a = len(first &amp; second) b = len(first) c = len(second) result = (2 * a) / ((2 * a) + b + c) return result def items_for_session(self, session): &#39;&#39;&#39; Returns all items in the session Parameters -- session: Id of a session Returns -- out : set &#39;&#39;&#39; return self.session_item_map.get(session); def sessions_for_item(self, item_id): &#39;&#39;&#39; Returns all session for an item Parameters -- item: Id of the item session Returns -- out : set &#39;&#39;&#39; return self.item_session_map.get(item_id) def most_recent_sessions(self, sessions, number): &#39;&#39;&#39; Find the most recent sessions in the given set Parameters -- sessions: set of session ids Returns -- out : set &#39;&#39;&#39; sample = set() tuples = list() for session in sessions: time = self.session_time.get(session) if time is None: print(&#39; EMPTY TIMESTAMP!! &#39;, session) tuples.append((session, time)) tuples = sorted(tuples, key=itemgetter(1), reverse=True) # print &#39;sorted list &#39;, sortedList cnt = 0 for element in tuples: cnt = cnt + 1 if cnt &gt; number: break sample.add(element[0]) # print &#39;returning sample of size &#39;, len(sample) return sample def possible_neighbor_sessions(self, session_items, input_item_id, session_id): &#39;&#39;&#39; Find a set of session to later on find neighbors in. A self.sample_size of 0 uses all sessions in which any item of the current session appears. self.sampling can be performed with the options &quot;recent&quot; or &quot;random&quot;. &quot;recent&quot; selects the self.sample_size most recent sessions while &quot;random&quot; just choses randomly. Parameters -- sessions: set of session ids Returns -- out : set &#39;&#39;&#39; self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id); if self.sample_size == 0: # use all session as possible neighbors print(&#39;!!!!! runnig KNN without a sample size (check config)&#39;) return self.relevant_sessions else: # sample some sessions self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id); if len(self.relevant_sessions) &gt; self.sample_size: if self.sampling == &#39;recent&#39;: sample = self.most_recent_sessions(self.relevant_sessions, self.sample_size) elif self.sampling == &#39;random&#39;: sample = random.sample(self.relevant_sessions, self.sample_size) else: sample = self.relevant_sessions[:self.sample_size] return sample else: return self.relevant_sessions def calc_similarity(self, session_items, sessions): &#39;&#39;&#39; Calculates the configured similarity for the items in session_items and each session in sessions. Parameters -- session_items: set of item ids sessions: list of session ids Returns -- out : list of tuple (session_id,similarity) &#39;&#39;&#39; # print &#39;nb of sessions to test &#39;, len(sessionsToTest), &#39; metric: &#39;, self.metric neighbors = [] cnt = 0 for session in sessions: cnt = cnt + 1 # get items of the session, look up the cache first session_items_test = self.items_for_session(session) similarity = getattr(self, self.similarity)(session_items_test, session_items) if similarity &gt; 0: neighbors.append((session, similarity)) return neighbors # -- # Find a set of neighbors, returns a list of tuples (sessionid: similarity) # -- def find_neighbors(self, session_items, input_item_id, session_id): &#39;&#39;&#39; Finds the k nearest neighbors for the given session_id and the current item input_item_id. Parameters -- session_items: set of item ids input_item_id: int session_id: int Returns -- out : list of tuple (session_id, similarity) &#39;&#39;&#39; possible_neighbors = self.possible_neighbor_sessions(session_items, input_item_id, session_id) possible_neighbors = self.calc_similarity(session_items, possible_neighbors) possible_neighbors = sorted(possible_neighbors, reverse=True, key=lambda x: x[1]) possible_neighbors = possible_neighbors[:self.k] return possible_neighbors def score_items(self, neighbors, input_item_id): &#39;&#39;&#39; Compute a set of scores for all items given a set of neighbors. Parameters -- neighbors: set of session ids Returns -- out : list of tuple (item, score) &#39;&#39;&#39; # now we have the set of relevant items to make predictions scores = dict() # iterate over the sessions for session in neighbors: # get the items in this session items = self.items_for_session(session[0]) for item in items: if input_item_id in self.followed_by and item in self.followed_by[ input_item_id]: # hard filter the candidates old_score = scores.get(item) new_score = session[1] if old_score is None: scores.update({item: new_score}) else: new_score = old_score + new_score scores.update({item: new_score}) return scores . . class VMSessionKNN: &#39;&#39;&#39; VMSessionKNN( k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;cosine&#39;, weighting=&#39;div&#39;, dwelling_time=False, last_n_days=None, last_n_clicks=None, extend=False, weighting_score=&#39;div_score&#39;, weighting_time=False, normalize=True, session_key = &#39;SessionId&#39;, item_key= &#39;ItemId&#39;, time_key= &#39;Time&#39;) Parameters -- k : int Number of neighboring session to calculate the item scores from. (Default value: 100) sample_size : int Defines the length of a subset of all training sessions to calculate the nearest neighbors from. (Default value: 500) sampling : string String to define the sampling method for sessions (recent, random). (default: recent) similarity : string String to define the method for the similarity calculation (jaccard, cosine, binary, tanimoto). (default: jaccard) weighting : string Decay function to determine the importance/weight of individual actions in the current session (linear, same, div, log, quadratic). (default: div) weighting_score : string Decay function to lower the score of candidate items from a neighboring sessions that were selected by less recently clicked items in the current session. (linear, same, div, log, quadratic). (default: div_score) weighting_time : boolean Experimental function to give less weight to items from older sessions (default: False) dwelling_time : boolean Experimental function to use the dwelling time for item view actions as a weight in the similarity calculation. (default: False) last_n_days : int Use only data from the last N days. (default: None) last_n_clicks : int Use only the last N clicks of the current session when recommending. (default: None) extend : bool Add evaluated sessions to the maps. normalize : bool Normalize the scores in the end. session_key : string Header of the session ID column in the input file. (default: &#39;SessionId&#39;) item_key : string Header of the item ID column in the input file. (default: &#39;ItemId&#39;) time_key : string Header of the timestamp column in the input file. (default: &#39;Time&#39;) &#39;&#39;&#39; def __init__(self, k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;cosine&#39;, weighting=&#39;div&#39;, dwelling_time=False, last_n_days=None, last_n_clicks=None, extend=False, weighting_score=&#39;div_score&#39;, weighting_time=False, normalize=True, session_key=&#39;SessionId&#39;, item_key=&#39;ItemId&#39;, time_key=&#39;Time&#39;): self.k = k self.sample_size = sample_size self.sampling = sampling self.weighting = weighting self.dwelling_time = dwelling_time self.weighting_score = weighting_score self.weighting_time = weighting_time self.similarity = similarity self.session_key = session_key self.item_key = item_key self.time_key = time_key self.extend = extend self.normalize = normalize self.last_n_days = last_n_days self.last_n_clicks = last_n_clicks # updated while recommending self.session = -1 self.session_items = [] self.relevant_sessions = set() # cache relations once at startup self.session_item_map = dict() self.item_session_map = dict() self.session_time = dict() self.min_time = -1 self.sim_time = 0 def fit(self, data, items=None): &#39;&#39;&#39; Trains the predictor. Parameters -- data: pandas.DataFrame Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps). It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties). &#39;&#39;&#39; if self.last_n_days != None: max_time = dt.fromtimestamp(data[self.time_key].max()) date_threshold = max_time.date() - td(self.last_n_days) stamp = dt.combine(date_threshold, dt.min.time()).timestamp() train = data[data[self.time_key] &gt;= stamp] else: train = data self.num_items = train[self.item_key].max() index_session = train.columns.get_loc(self.session_key) index_item = train.columns.get_loc(self.item_key) index_time = train.columns.get_loc(self.time_key) self.itemids = train[self.item_key].unique() session = -1 session_items = set() time = -1 # cnt = 0 for row in train.itertuples(index=False): # cache items of sessions if row[index_session] != session: if len(session_items) &gt; 0: self.session_item_map.update({session: session_items}) # cache the last time stamp of the session self.session_time.update({session: time}) if time &lt; self.min_time: self.min_time = time session = row[index_session] session_items = set() time = row[index_time] session_items.add(row[index_item]) # cache sessions involving an item map_is = self.item_session_map.get(row[index_item]) if map_is is None: map_is = set() self.item_session_map.update({row[index_item]: map_is}) map_is.add(row[index_session]) # Add the last tuple self.session_item_map.update({session: session_items}) self.session_time.update({session: time}) def predict_next(self, session_id, input_item_id, predict_for_item_ids=None, skip=False, type=&#39;view&#39;, timestamp=0): &#39;&#39;&#39; Gives predicton scores for a selected set of items on how likely they be the next item in the session. Parameters -- session_id : int or string The session IDs of the event. input_item_id : int or string The item ID of the event. Must be in the set of item IDs of the training set. predict_for_item_ids : 1D array IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set. Returns -- out : pandas.Series Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs. &#39;&#39;&#39; # gc.collect() # process = psutil.Process(os.getpid()) # print( &#39;cknn.predict_next: &#39;, process.memory_info().rss, &#39; memory used&#39;) if (self.session != session_id): # new session if (self.extend): item_set = set(self.session_items) self.session_item_map[self.session] = item_set; for item in item_set: map_is = self.item_session_map.get(item) if map_is is None: map_is = set() self.item_session_map.update({item: map_is}) map_is.add(self.session) ts = time.time() self.session_time.update({self.session: ts}) self.last_ts = -1 self.session = session_id self.session_items = list() self.dwelling_times = list() self.relevant_sessions = set() if type == &#39;view&#39;: self.session_items.append(input_item_id) if self.dwelling_time: if self.last_ts &gt; 0: self.dwelling_times.append(timestamp - self.last_ts) self.last_ts = timestamp if skip: return items = self.session_items if self.last_n_clicks is None else self.session_items[-self.last_n_clicks:] neighbors = self.find_neighbors(items, input_item_id, session_id, self.dwelling_times, timestamp) scores = self.score_items(neighbors, items, timestamp) # Create things in the format .. if predict_for_item_ids is None: predict_for_item_ids = self.itemids predictions = np.zeros(len(predict_for_item_ids)) mask = np.in1d(predict_for_item_ids, list(scores.keys())) items = predict_for_item_ids[mask] values = [scores[x] for x in items] predictions[mask] = values series = pd.Series(data=predictions, index=predict_for_item_ids) if self.normalize: series = series / series.max() return series def item_pop(self, sessions): &#39;&#39;&#39; Returns a dict(item,score) of the item popularity for the given list of sessions (only a set of ids) Parameters -- sessions: set Returns -- out : dict &#39;&#39;&#39; result = dict() max_pop = 0 for session, weight in sessions: items = self.items_for_session(session) for item in items: count = result.get(item) if count is None: result.update({item: 1}) else: result.update({item: count + 1}) if (result.get(item) &gt; max_pop): max_pop = result.get(item) for key in result: result.update({key: (result[key] / max_pop)}) return result def jaccard(self, first, second): &#39;&#39;&#39; Calculates the jaccard index for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; sc = time.clock() intersection = len(first &amp; second) union = len(first | second) res = intersection / union self.sim_time += (time.clock() - sc) return res def cosine(self, first, second): &#39;&#39;&#39; Calculates the cosine similarity for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; li = len(first &amp; second) la = len(first) lb = len(second) result = li / sqrt(la) * sqrt(lb) return result def tanimoto(self, first, second): &#39;&#39;&#39; Calculates the cosine tanimoto similarity for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; li = len(first &amp; second) la = len(first) lb = len(second) result = li / (la + lb - li) return result def binary(self, first, second): &#39;&#39;&#39; Calculates the ? for 2 sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; a = len(first &amp; second) b = len(first) c = len(second) result = (2 * a) / ((2 * a) + b + c) return result def vec(self, first, second, map): &#39;&#39;&#39; Calculates the ? for 2 sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; a = first &amp; second sum = 0 for i in a: sum += map[i] result = sum / len(map) return result def items_for_session(self, session): &#39;&#39;&#39; Returns all items in the session Parameters -- session: Id of a session Returns -- out : set &#39;&#39;&#39; return self.session_item_map.get(session); def vec_for_session(self, session): &#39;&#39;&#39; Returns all items in the session Parameters -- session: Id of a session Returns -- out : set &#39;&#39;&#39; return self.session_vec_map.get(session); def sessions_for_item(self, item_id): &#39;&#39;&#39; Returns all session for an item Parameters -- item: Id of the item session Returns -- out : set &#39;&#39;&#39; return self.item_session_map.get(item_id) if item_id in self.item_session_map else set() def most_recent_sessions(self, sessions, number): &#39;&#39;&#39; Find the most recent sessions in the given set Parameters -- sessions: set of session ids Returns -- out : set &#39;&#39;&#39; sample = set() tuples = list() for session in sessions: time = self.session_time.get(session) if time is None: print(&#39; EMPTY TIMESTAMP!! &#39;, session) tuples.append((session, time)) tuples = sorted(tuples, key=itemgetter(1), reverse=True) # print &#39;sorted list &#39;, sortedList cnt = 0 for element in tuples: cnt = cnt + 1 if cnt &gt; number: break sample.add(element[0]) # print &#39;returning sample of size &#39;, len(sample) return sample def possible_neighbor_sessions(self, session_items, input_item_id, session_id): &#39;&#39;&#39; Find a set of session to later on find neighbors in. A self.sample_size of 0 uses all sessions in which any item of the current session appears. self.sampling can be performed with the options &quot;recent&quot; or &quot;random&quot;. &quot;recent&quot; selects the self.sample_size most recent sessions while &quot;random&quot; just choses randomly. Parameters -- sessions: set of session ids Returns -- out : set &#39;&#39;&#39; self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id) if self.sample_size == 0: # use all session as possible neighbors print(&#39;!!!!! runnig KNN without a sample size (check config)&#39;) return self.relevant_sessions else: # sample some sessions if len(self.relevant_sessions) &gt; self.sample_size: if self.sampling == &#39;recent&#39;: sample = self.most_recent_sessions(self.relevant_sessions, self.sample_size) elif self.sampling == &#39;random&#39;: sample = random.sample(self.relevant_sessions, self.sample_size) else: sample = self.relevant_sessions[:self.sample_size] return sample else: return self.relevant_sessions def calc_similarity(self, session_items, sessions, dwelling_times, timestamp): &#39;&#39;&#39; Calculates the configured similarity for the items in session_items and each session in sessions. Parameters -- session_items: set of item ids sessions: list of session ids Returns -- out : list of tuple (session_id,similarity) &#39;&#39;&#39; pos_map = {} length = len(session_items) count = 1 for item in session_items: if self.weighting is not None: pos_map[item] = getattr(self, self.weighting)(count, length) count += 1 else: pos_map[item] = 1 dt = dwelling_times.copy() dt.append(0) dt = pd.Series(dt, index=session_items) dt = dt / dt.max() # dt[session_items[-1]] = dt.mean() if len(session_items) &gt; 1 else 1 dt[session_items[-1]] = 1 if self.dwelling_time: # print(dt) for i in range(len(dt)): pos_map[session_items[i]] *= dt.iloc[i] # print(pos_map) # print &#39;nb of sessions to test &#39;, len(sessionsToTest), &#39; metric: &#39;, self.metric items = set(session_items) neighbors = [] cnt = 0 for session in sessions: cnt = cnt + 1 # get items of the session, look up the cache first n_items = self.items_for_session(session) sts = self.session_time[session] similarity = self.vec(items, n_items, pos_map) if similarity &gt; 0: if self.weighting_time: diff = timestamp - sts days = round(diff / 60 / 60 / 24) decay = pow(7 / 8, days) similarity *= decay # print(&quot;days:&quot;,days,&quot; =&gt; &quot;,decay) neighbors.append((session, similarity)) return neighbors # -- # Find a set of neighbors, returns a list of tuples (sessionid: similarity) # -- def find_neighbors(self, session_items, input_item_id, session_id, dwelling_times, timestamp): &#39;&#39;&#39; Finds the k nearest neighbors for the given session_id and the current item input_item_id. Parameters -- session_items: set of item ids input_item_id: int session_id: int Returns -- out : list of tuple (session_id, similarity) &#39;&#39;&#39; possible_neighbors = self.possible_neighbor_sessions(session_items, input_item_id, session_id) possible_neighbors = self.calc_similarity(session_items, possible_neighbors, dwelling_times, timestamp) possible_neighbors = sorted(possible_neighbors, reverse=True, key=lambda x: x[1]) possible_neighbors = possible_neighbors[:self.k] return possible_neighbors def score_items(self, neighbors, current_session, timestamp): &#39;&#39;&#39; Compute a set of scores for all items given a set of neighbors. Parameters -- neighbors: set of session ids Returns -- out : list of tuple (item, score) &#39;&#39;&#39; # now we have the set of relevant items to make predictions scores = dict() # iterate over the sessions for session in neighbors: # get the items in this session items = self.items_for_session(session[0]) step = 1 for item in reversed(current_session): if item in items: decay = getattr(self, self.weighting_score)(step) break step += 1 for item in items: old_score = scores.get(item) similarity = session[1] if old_score is None: scores.update({item: (similarity * decay)}) else: new_score = old_score + (similarity * decay) scores.update({item: new_score}) return scores def linear_score(self, i): return 1 - (0.1 * i) if i &lt;= 100 else 0 def same_score(self, i): return 1 def div_score(self, i): return 1 / i def log_score(self, i): return 1 / (log10(i + 1.7)) def quadratic_score(self, i): return 1 / (i * i) def linear(self, i, length): return 1 - (0.1 * (length - i)) if i &lt;= 10 else 0 def same(self, i, length): return 1 def div(self, i, length): return i / length def log(self, i, length): return 1 / (log10((length - i) + 1.7)) def quadratic(self, i, length): return (i / length) ** 2 . . class SessionKNN: &#39;&#39;&#39; SessionKNN( k, sample_size=500, sampling=&#39;recent&#39;, similarity = &#39;jaccard&#39;, remind=False, pop_boost=0, session_key = &#39;SessionId&#39;, item_key= &#39;ItemId&#39;) Parameters -- k : int Number of neighboring session to calculate the item scores from. (Default value: 100) sample_size : int Defines the length of a subset of all training sessions to calculate the nearest neighbors from. (Default value: 500) sampling : string String to define the sampling method for sessions (recent, random). (default: recent) similarity : string String to define the method for the similarity calculation (jaccard, cosine, binary, tanimoto). (default: jaccard) remind : bool Should the last items of the current session be boosted to the top as reminders pop_boost : int Push popular items in the neighbor sessions by this factor. (default: 0 to leave out) extend : bool Add evaluated sessions to the maps normalize : bool Normalize the scores in the end session_key : string Header of the session ID column in the input file. (default: &#39;SessionId&#39;) item_key : string Header of the item ID column in the input file. (default: &#39;ItemId&#39;) time_key : string Header of the timestamp column in the input file. (default: &#39;Time&#39;) &#39;&#39;&#39; def __init__(self, k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;jaccard&#39;, remind=False, pop_boost=0, extend=False, normalize=True, session_key=&#39;SessionId&#39;, item_key=&#39;ItemId&#39;, time_key=&#39;Time&#39;): self.remind = remind self.k = k self.sample_size = sample_size self.sampling = sampling self.similarity = similarity self.pop_boost = pop_boost self.session_key = session_key self.item_key = item_key self.time_key = time_key self.extend = extend self.normalize = normalize # updated while recommending self.session = -1 self.session_items = [] self.relevant_sessions = set() # cache relations once at startup self.session_item_map = dict() self.item_session_map = dict() self.session_time = dict() self.sim_time = 0 def fit(self, train): &#39;&#39;&#39; Trains the predictor. Parameters -- data: pandas.DataFrame Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps). It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties). &#39;&#39;&#39; index_session = train.columns.get_loc(self.session_key) index_item = train.columns.get_loc(self.item_key) index_time = train.columns.get_loc(self.time_key) self.itemids = train[self.item_key].unique() session = -1 session_items = set() time = -1 # cnt = 0 for row in train.itertuples(index=False): # cache items of sessions if row[index_session] != session: if len(session_items) &gt; 0: self.session_item_map.update({session: session_items}) # cache the last time stamp of the session self.session_time.update({session: time}) session = row[index_session] session_items = set() time = row[index_time] session_items.add(row[index_item]) # cache sessions involving an item map_is = self.item_session_map.get(row[index_item]) if map_is is None: map_is = set() self.item_session_map.update({row[index_item]: map_is}) map_is.add(row[index_session]) # Add the last tuple self.session_item_map.update({session: session_items}) self.session_time.update({session: time}) def predict_next(self, session_id, input_item_id, predict_for_item_ids=None, skip=False, type=&#39;view&#39;, timestamp=0): &#39;&#39;&#39; Gives predicton scores for a selected set of items on how likely they be the next item in the session. Parameters -- session_id : int or string The session IDs of the event. input_item_id : int or string The item ID of the event. Must be in the set of item IDs of the training set. predict_for_item_ids : 1D array IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set. Returns -- out : pandas.Series Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs. &#39;&#39;&#39; # gc.collect() # process = psutil.Process(os.getpid()) # print( &#39;cknn.predict_next: &#39;, process.memory_info().rss, &#39; memory used&#39;) if (self.session != session_id): # new session if (self.extend): item_set = set(self.session_items) self.session_item_map[self.session] = item_set; for item in item_set: map_is = self.item_session_map.get(item) if map_is is None: map_is = set() self.item_session_map.update({item: map_is}) map_is.add(self.session) ts = time.time() self.session_time.update({self.session: ts}) self.session = session_id self.session_items = list() self.relevant_sessions = set() if type == &#39;view&#39;: self.session_items.append(input_item_id) if skip: return neighbors = self.find_neighbors(set(self.session_items), input_item_id, session_id) scores = self.score_items(neighbors) # add some reminders if self.remind: reminderScore = 5 takeLastN = 3 cnt = 0 for elem in self.session_items[-takeLastN:]: cnt = cnt + 1 # reminderScore = reminderScore + (cnt/100) oldScore = scores.get(elem) newScore = 0 if oldScore is None: newScore = reminderScore else: newScore = oldScore + reminderScore # print &#39;old score &#39;, oldScore # update the score and add a small number for the position newScore = (newScore * reminderScore) + (cnt / 100) scores.update({elem: newScore}) # push popular ones if self.pop_boost &gt; 0: pop = self.item_pop(neighbors) # Iterate over the item neighbors # print itemScores for key in scores: item_pop = pop.get(key) # Gives some minimal MRR boost? scores.update({key: (scores[key] + (self.pop_boost * item_pop))}) # Create things in the format .. if predict_for_item_ids is None: predict_for_item_ids = self.itemids predictions = np.zeros(len(predict_for_item_ids)) mask = np.in1d(predict_for_item_ids, list(scores.keys())) items = predict_for_item_ids[mask] values = [scores[x] for x in items] predictions[mask] = values series = pd.Series(data=predictions, index=predict_for_item_ids) if self.normalize: series = series / series.max() return series def item_pop(self, sessions): &#39;&#39;&#39; Returns a dict(item,score) of the item popularity for the given list of sessions (only a set of ids) Parameters -- sessions: set Returns -- out : dict &#39;&#39;&#39; result = dict() max_pop = 0 for session, weight in sessions: items = self.items_for_session(session) for item in items: count = result.get(item) if count is None: result.update({item: 1}) else: result.update({item: count + 1}) if (result.get(item) &gt; max_pop): max_pop = result.get(item) for key in result: result.update({key: (result[key] / max_pop)}) return result def jaccard(self, first, second): &#39;&#39;&#39; Calculates the jaccard index for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; sc = time.clock() intersection = len(first &amp; second) union = len(first | second) res = intersection / union self.sim_time += (time.clock() - sc) return res def cosine(self, first, second): &#39;&#39;&#39; Calculates the cosine similarity for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; li = len(first &amp; second) la = len(first) lb = len(second) result = li / sqrt(la) * sqrt(lb) return result def tanimoto(self, first, second): &#39;&#39;&#39; Calculates the cosine tanimoto similarity for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; li = len(first &amp; second) la = len(first) lb = len(second) result = li / (la + lb - li) return result def binary(self, first, second): &#39;&#39;&#39; Calculates the ? for 2 sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; a = len(first &amp; second) b = len(first) c = len(second) result = (2 * a) / ((2 * a) + b + c) return result def random(self, first, second): &#39;&#39;&#39; Calculates the ? for 2 sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; return random.random() def items_for_session(self, session): &#39;&#39;&#39; Returns all items in the session Parameters -- session: Id of a session Returns -- out : set &#39;&#39;&#39; return self.session_item_map.get(session); def sessions_for_item(self, item_id): &#39;&#39;&#39; Returns all session for an item Parameters -- item: Id of the item session Returns -- out : set &#39;&#39;&#39; return self.item_session_map.get(item_id) def most_recent_sessions(self, sessions, number): &#39;&#39;&#39; Find the most recent sessions in the given set Parameters -- sessions: set of session ids Returns -- out : set &#39;&#39;&#39; sample = set() tuples = list() for session in sessions: time = self.session_time.get(session) if time is None: print(&#39; EMPTY TIMESTAMP!! &#39;, session) tuples.append((session, time)) tuples = sorted(tuples, key=itemgetter(1), reverse=True) # print &#39;sorted list &#39;, sortedList cnt = 0 for element in tuples: cnt = cnt + 1 if cnt &gt; number: break sample.add(element[0]) # print &#39;returning sample of size &#39;, len(sample) return sample def possible_neighbor_sessions(self, session_items, input_item_id, session_id): &#39;&#39;&#39; Find a set of session to later on find neighbors in. A self.sample_size of 0 uses all sessions in which any item of the current session appears. self.sampling can be performed with the options &quot;recent&quot; or &quot;random&quot;. &quot;recent&quot; selects the self.sample_size most recent sessions while &quot;random&quot; just choses randomly. Parameters -- sessions: set of session ids Returns -- out : set &#39;&#39;&#39; self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id); if self.sample_size == 0: # use all session as possible neighbors print(&#39;!!!!! runnig KNN without a sample size (check config)&#39;) return self.relevant_sessions else: # sample some sessions self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id); if len(self.relevant_sessions) &gt; self.sample_size: if self.sampling == &#39;recent&#39;: sample = self.most_recent_sessions(self.relevant_sessions, self.sample_size) elif self.sampling == &#39;random&#39;: sample = random.sample(self.relevant_sessions, self.sample_size) else: sample = self.relevant_sessions[:self.sample_size] return sample else: return self.relevant_sessions def calc_similarity(self, session_items, sessions): &#39;&#39;&#39; Calculates the configured similarity for the items in session_items and each session in sessions. Parameters -- session_items: set of item ids sessions: list of session ids Returns -- out : list of tuple (session_id,similarity) &#39;&#39;&#39; # print &#39;nb of sessions to test &#39;, len(sessionsToTest), &#39; metric: &#39;, self.metric neighbors = [] cnt = 0 for session in sessions: cnt = cnt + 1 # get items of the session, look up the cache first session_items_test = self.items_for_session(session) similarity = getattr(self, self.similarity)(session_items_test, session_items) if similarity &gt; 0: neighbors.append((session, similarity)) return neighbors # -- # Find a set of neighbors, returns a list of tuples (sessionid: similarity) # -- def find_neighbors(self, session_items, input_item_id, session_id): &#39;&#39;&#39; Finds the k nearest neighbors for the given session_id and the current item input_item_id. Parameters -- session_items: set of item ids input_item_id: int session_id: int Returns -- out : list of tuple (session_id, similarity) &#39;&#39;&#39; possible_neighbors = self.possible_neighbor_sessions(session_items, input_item_id, session_id) possible_neighbors = self.calc_similarity(session_items, possible_neighbors) possible_neighbors = sorted(possible_neighbors, reverse=True, key=lambda x: x[1]) possible_neighbors = possible_neighbors[:self.k] return possible_neighbors def score_items(self, neighbors): &#39;&#39;&#39; Compute a set of scores for all items given a set of neighbors. Parameters -- neighbors: set of session ids Returns -- out : list of tuple (item, score) &#39;&#39;&#39; # now we have the set of relevant items to make predictions scores = dict() # iterate over the sessions for session in neighbors: # get the items in this session items = self.items_for_session(session[0]) for item in items: old_score = scores.get(item) new_score = session[1] if old_score is None: scores.update({item: new_score}) else: new_score = old_score + new_score scores.update({item: new_score}) return scores . . class SeqSessionKNN: &#39;&#39;&#39; SeqSessionKNN( k, sample_size=500, sampling=&#39;recent&#39;, similarity = &#39;jaccard&#39;, remind=False, pop_boost=0, session_key = &#39;SessionId&#39;, item_key= &#39;ItemId&#39;) Parameters -- k : int Number of neighboring session to calculate the item scores from. (Default value: 100) sample_size : int Defines the length of a subset of all training sessions to calculate the nearest neighbors from. (Default value: 500) sampling : string String to define the sampling method for sessions (recent, random). (default: recent) similarity : string String to define the method for the similarity calculation (jaccard, cosine, binary, tanimoto). (default: jaccard) remind : bool Should the last items of the current session be boosted to the top as reminders pop_boost : int Push popular items in the neighbor sessions by this factor. (default: 0 to leave out) extend : bool Add evaluated sessions to the maps normalize : bool Normalize the scores in the end session_key : string Header of the session ID column in the input file. (default: &#39;SessionId&#39;) item_key : string Header of the item ID column in the input file. (default: &#39;ItemId&#39;) time_key : string Header of the timestamp column in the input file. (default: &#39;Time&#39;) &#39;&#39;&#39; def __init__(self, k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;jaccard&#39;, weighting=&#39;div&#39;, remind=False, pop_boost=0, extend=False, normalize=True, session_key=&#39;SessionId&#39;, item_key=&#39;ItemId&#39;, time_key=&#39;Time&#39;): self.remind = remind self.k = k self.sample_size = sample_size self.sampling = sampling self.weighting = weighting self.similarity = similarity self.pop_boost = pop_boost self.session_key = session_key self.item_key = item_key self.time_key = time_key self.extend = extend self.normalize = normalize # updated while recommending self.session = -1 self.session_items = [] self.relevant_sessions = set() # cache relations once at startup self.session_item_map = dict() self.item_session_map = dict() self.session_time = dict() self.sim_time = 0 def fit(self, train, items=None): &#39;&#39;&#39; Trains the predictor. Parameters -- data: pandas.DataFrame Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps). It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties). &#39;&#39;&#39; index_session = train.columns.get_loc(self.session_key) index_item = train.columns.get_loc(self.item_key) index_time = train.columns.get_loc(self.time_key) self.itemids = train[self.item_key].unique() session = -1 session_items = set() time = -1 # cnt = 0 for row in train.itertuples(index=False): # cache items of sessions if row[index_session] != session: if len(session_items) &gt; 0: self.session_item_map.update({session: session_items}) # cache the last time stamp of the session self.session_time.update({session: time}) session = row[index_session] session_items = set() time = row[index_time] session_items.add(row[index_item]) # cache sessions involving an item map_is = self.item_session_map.get(row[index_item]) if map_is is None: map_is = set() self.item_session_map.update({row[index_item]: map_is}) map_is.add(row[index_session]) # Add the last tuple self.session_item_map.update({session: session_items}) self.session_time.update({session: time}) def predict_next(self, session_id, input_item_id, predict_for_item_ids=None, skip=False, type=&#39;view&#39;, timestamp=0): &#39;&#39;&#39; Gives predicton scores for a selected set of items on how likely they be the next item in the session. Parameters -- session_id : int or string The session IDs of the event. input_item_id : int or string The item ID of the event. Must be in the set of item IDs of the training set. predict_for_item_ids : 1D array IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set. Returns -- out : pandas.Series Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs. &#39;&#39;&#39; # gc.collect() # process = psutil.Process(os.getpid()) # print( &#39;cknn.predict_next: &#39;, process.memory_info().rss, &#39; memory used&#39;) if (self.session != session_id): # new session if (self.extend): item_set = set(self.session_items) self.session_item_map[self.session] = item_set for item in item_set: map_is = self.item_session_map.get(item) if map_is is None: map_is = set() self.item_session_map.update({item: map_is}) map_is.add(self.session) ts = time.time() self.session_time.update({self.session: ts}) self.session = session_id self.session_items = list() self.relevant_sessions = set() if type == &#39;view&#39;: self.session_items.append(input_item_id) if skip: return neighbors = self.find_neighbors(set(self.session_items), input_item_id, session_id) scores = self.score_items(neighbors, self.session_items) # add some reminders if self.remind: reminderScore = 5 takeLastN = 3 cnt = 0 for elem in self.session_items[-takeLastN:]: cnt = cnt + 1 # reminderScore = reminderScore + (cnt/100) oldScore = scores.get(elem) newScore = 0 if oldScore is None: newScore = reminderScore else: newScore = oldScore + reminderScore # print &#39;old score &#39;, oldScore # update the score and add a small number for the position newScore = (newScore * reminderScore) + (cnt / 100) scores.update({elem: newScore}) # push popular ones if self.pop_boost &gt; 0: pop = self.item_pop(neighbors) # Iterate over the item neighbors # print itemScores for key in scores: item_pop = pop.get(key) # Gives some minimal MRR boost? scores.update({key: (scores[key] + (self.pop_boost * item_pop))}) # Create things in the format .. if predict_for_item_ids is None: predict_for_item_ids = self.itemids predictions = np.zeros(len(predict_for_item_ids)) mask = np.in1d(predict_for_item_ids, list(scores.keys())) items = predict_for_item_ids[mask] values = [scores[x] for x in items] predictions[mask] = values series = pd.Series(data=predictions, index=predict_for_item_ids) if self.normalize: series = series / series.max() return series def item_pop(self, sessions): &#39;&#39;&#39; Returns a dict(item,score) of the item popularity for the given list of sessions (only a set of ids) Parameters -- sessions: set Returns -- out : dict &#39;&#39;&#39; result = dict() max_pop = 0 for session, weight in sessions: items = self.items_for_session(session) for item in items: count = result.get(item) if count is None: result.update({item: 1}) else: result.update({item: count + 1}) if (result.get(item) &gt; max_pop): max_pop = result.get(item) for key in result: result.update({key: (result[key] / max_pop)}) return result def jaccard(self, first, second): &#39;&#39;&#39; Calculates the jaccard index for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; sc = time.clock() intersection = len(first &amp; second) union = len(first | second) res = intersection / union self.sim_time += (time.clock() - sc) return res def cosine(self, first, second): &#39;&#39;&#39; Calculates the cosine similarity for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; li = len(first &amp; second) la = len(first) lb = len(second) result = li / sqrt(la) * sqrt(lb) return result def tanimoto(self, first, second): &#39;&#39;&#39; Calculates the cosine tanimoto similarity for two sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; li = len(first &amp; second) la = len(first) lb = len(second) result = li / (la + lb - li) return result def binary(self, first, second): &#39;&#39;&#39; Calculates the ? for 2 sessions Parameters -- first: Id of a session second: Id of a session Returns -- out : float value &#39;&#39;&#39; a = len(first &amp; second) b = len(first) c = len(second) result = (2 * a) / ((2 * a) + b + c) return result def items_for_session(self, session): &#39;&#39;&#39; Returns all items in the session Parameters -- session: Id of a session Returns -- out : set &#39;&#39;&#39; return self.session_item_map.get(session); def sessions_for_item(self, item_id): &#39;&#39;&#39; Returns all session for an item Parameters -- item: Id of the item session Returns -- out : set &#39;&#39;&#39; return self.item_session_map.get(item_id) def most_recent_sessions(self, sessions, number): &#39;&#39;&#39; Find the most recent sessions in the given set Parameters -- sessions: set of session ids Returns -- out : set &#39;&#39;&#39; sample = set() tuples = list() for session in sessions: time = self.session_time.get(session) if time is None: print(&#39; EMPTY TIMESTAMP!! &#39;, session) tuples.append((session, time)) tuples = sorted(tuples, key=itemgetter(1), reverse=True) # print &#39;sorted list &#39;, sortedList cnt = 0 for element in tuples: cnt = cnt + 1 if cnt &gt; number: break sample.add(element[0]) # print &#39;returning sample of size &#39;, len(sample) return sample def possible_neighbor_sessions(self, session_items, input_item_id, session_id): &#39;&#39;&#39; Find a set of session to later on find neighbors in. A self.sample_size of 0 uses all sessions in which any item of the current session appears. self.sampling can be performed with the options &quot;recent&quot; or &quot;random&quot;. &quot;recent&quot; selects the self.sample_size most recent sessions while &quot;random&quot; just choses randomly. Parameters -- sessions: set of session ids Returns -- out : set &#39;&#39;&#39; self.relevant_sessions = self.relevant_sessions | self.sessions_for_item(input_item_id); if self.sample_size == 0: # use all session as possible neighbors print(&#39;!!!!! runnig KNN without a sample size (check config)&#39;) return self.relevant_sessions else: # sample some sessions if len(self.relevant_sessions) &gt; self.sample_size: if self.sampling == &#39;recent&#39;: sample = self.most_recent_sessions(self.relevant_sessions, self.sample_size) elif self.sampling == &#39;random&#39;: sample = random.sample(self.relevant_sessions, self.sample_size) else: sample = self.relevant_sessions[:self.sample_size] return sample else: return self.relevant_sessions def calc_similarity(self, session_items, sessions): &#39;&#39;&#39; Calculates the configured similarity for the items in session_items and each session in sessions. Parameters -- session_items: set of item ids sessions: list of session ids Returns -- out : list of tuple (session_id,similarity) &#39;&#39;&#39; # print &#39;nb of sessions to test &#39;, len(sessionsToTest), &#39; metric: &#39;, self.metric neighbors = [] cnt = 0 for session in sessions: cnt = cnt + 1 # get items of the session, look up the cache first session_items_test = self.items_for_session(session) similarity = getattr(self, self.similarity)(session_items_test, session_items) if similarity &gt; 0: neighbors.append((session, similarity)) return neighbors # -- # Find a set of neighbors, returns a list of tuples (sessionid: similarity) # -- def find_neighbors(self, session_items, input_item_id, session_id): &#39;&#39;&#39; Finds the k nearest neighbors for the given session_id and the current item input_item_id. Parameters -- session_items: set of item ids input_item_id: int session_id: int Returns -- out : list of tuple (session_id, similarity) &#39;&#39;&#39; possible_neighbors = self.possible_neighbor_sessions(session_items, input_item_id, session_id) possible_neighbors = self.calc_similarity(session_items, possible_neighbors) possible_neighbors = sorted(possible_neighbors, reverse=True, key=lambda x: x[1]) possible_neighbors = possible_neighbors[:self.k] return possible_neighbors def score_items(self, neighbors, current_session): &#39;&#39;&#39; Compute a set of scores for all items given a set of neighbors. Parameters -- neighbors: set of session ids Returns -- out : list of tuple (item, score) &#39;&#39;&#39; # now we have the set of relevant items to make predictions scores = dict() # iterate over the sessions for session in neighbors: # get the items in this session items = self.items_for_session(session[0]) step = 1 for item in reversed(current_session): if item in items: decay = getattr(self, self.weighting)(step) break step += 1 for item in items: old_score = scores.get(item) similarity = session[1] if old_score is None: scores.update({item: (similarity * decay)}) else: new_score = old_score + (similarity * decay) scores.update({item: new_score}) return scores def linear(self, i): return 1 - (0.1 * i) if i &lt;= 100 else 0 def same(self, i): return 1 def div(self, i): return 1 / i def log(self, i): return 1 / (log10(i + 1.7)) def quadratic(self, i): return 1 / (i * i) . . class KNNRecommender(ISeqRecommender): &quot;&quot;&quot; Interface to ItemKNN and Session-based KNN methods. Based on: Evaluation of Session-based Recommendation Algorithms, Malte Ludewig and Dietmar Jannach &quot;&quot;&quot; knn_models = { &#39;iknn&#39;: ItemKNN, &#39;sknn&#39;: SessionKNN, &#39;v-sknn&#39;: VMSessionKNN, &#39;s-sknn&#39;: SeqSessionKNN, &#39;sf-sknn&#39;: SeqFilterSessionKNN } def __init__(self, model=&#39;cknn&#39;, **init_args): &quot;&quot;&quot; :param model: One among the following KNN models: - iknn: ItemKNN, item-to-item KNN based on the *last* item in the session to determine the items to be recommended. - sknn: SessionKNN, compares the *entire* current session with the past sessions in the training data to determine the items to be recommended. - v-sknn: VMSessionKNN, use linearly decayed real-valued vectors to encode the current session, then compares the current session with the past sessions in the training data using the dot-product to determine the items to be recommended. - s-sknn: SeqSessionKNN, this variant also puts more weight on elements that appear later in the session by using a custom scoring function (see the paper by Ludewng and Jannach). - sf-sknn: SeqFilterSessionKNN, this variant also puts more weight on elements that appear later in the session in a more restrictive way by using a custom scoring function (see the paper by Ludewng and Jannach). :param init_args: The model initialization arguments. See the following initializations or check `util.knn` for more details on each model: - iknn: ItemKNN(n_sims=100, lmbd=20, alpha=0.5) - sknn: SessionKNN(k, sample_size=500, sampling=&#39;recent&#39;, similarity=&#39;jaccard&#39;, remind=False, pop_boost=0) - v-sknn: VMSessionKNN(k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;cosine&#39;, weighting=&#39;div&#39;, dwelling_time=False, last_n_days=None, last_n_clicks=None, extend=False, weighting_score=&#39;div_score&#39;, weighting_time=False, normalize=True) - s-knn: SeqSessionKNN(k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;jaccard&#39;, weighting=&#39;div&#39;, remind=False, pop_boost=0, extend=False, normalize=True) - sf-sknn: SeqFilterSessionKNN(k, sample_size=1000, sampling=&#39;recent&#39;, similarity=&#39;jaccard&#39;, remind=False, pop_boost=0, extend=False, normalize=True) &quot;&quot;&quot; super(KNNRecommender).__init__() if model not in self.knn_models: raise ValueError(&quot;Unknown KNN model &#39;{}&#39;. The available ones are: {}&quot;.format( model, list(self.knn_models.keys()) )) self.init_args = init_args self.init_args.update(dict(session_key=&#39;session_id&#39;, item_key=&#39;item_id&#39;, time_key=&#39;ts&#39;)) self.model = self.knn_models[model](**self.init_args) self.pseudo_session_id = 0 def __str__(self): return str(self.model) def fit(self, train_data): self.logger.info(&#39;Converting training data to GRU4Rec format&#39;) # parse training data to GRU4Rec format train_data = dataset_to_gru4rec_format(dataset=train_data) self.logger.info(&#39;Training started&#39;) self.model.fit(train_data) self.logger.info(&#39;Training completed&#39;) self.pseudo_session_id = 0 def recommend(self, user_profile, user_id=None): for item in user_profile: pred = self.model.predict_next(session_id=self.pseudo_session_id, input_item_id=item) # sort items by predicted score pred.sort_values(0, ascending=False, inplace=True) # increase the psuedo-session id so that future call to recommend() won&#39;t be connected self.pseudo_session_id += 1 # convert to the required output format return [([x.index], x._2) for x in pred.reset_index().itertuples()] . . knnrecommender = KNNRecommender(model=&#39;sknn&#39;, k=10) knnrecommender.fit(train_data) . 2021-04-25 14:10:51,373 - INFO - Converting training data to GRU4Rec format 2021-04-25 14:10:51,398 - INFO - Training started 2021-04-25 14:10:51,429 - INFO - Training completed . Sequential Evaluation . In the evaluation of sequence-aware recommenders, each sequence in the test set is split into: . the user profile, used to compute recommendations, is composed by the first k events in the sequence; | the ground truth, used for performance evaluation, is composed by the remainder of the sequence. | . In the cells below, you can control the dimension of the user profile by assigning a positive value to GIVEN_K, which correspond to the number of events from the beginning of the sequence that will be assigned to the initial user profile. This ensures that each user profile in the test set will have exactly the same initial size, but the size of the ground truth will change for every sequence. . Alternatively, by assigning a negative value to GIVEN_K, you will set the initial size of the ground truth. In this way the ground truth will have the same size for all sequences, but the dimension of the user profile will differ. . def precision(ground_truth, prediction): &quot;&quot;&quot; Compute Precision metric :param ground_truth: the ground truth set or sequence :param prediction: the predicted set or sequence :return: the value of the metric &quot;&quot;&quot; ground_truth = remove_duplicates(ground_truth) prediction = remove_duplicates(prediction) precision_score = count_a_in_b_unique(prediction, ground_truth) / float(len(prediction)) assert 0 &lt;= precision_score &lt;= 1 return precision_score def recall(ground_truth, prediction): &quot;&quot;&quot; Compute Recall metric :param ground_truth: the ground truth set or sequence :param prediction: the predicted set or sequence :return: the value of the metric &quot;&quot;&quot; ground_truth = remove_duplicates(ground_truth) prediction = remove_duplicates(prediction) recall_score = 0 if len(prediction) == 0 else count_a_in_b_unique(prediction, ground_truth) / float( len(ground_truth)) assert 0 &lt;= recall_score &lt;= 1 return recall_score def mrr(ground_truth, prediction): &quot;&quot;&quot; Compute Mean Reciprocal Rank metric. Reciprocal Rank is set 0 if no predicted item is in contained the ground truth. :param ground_truth: the ground truth set or sequence :param prediction: the predicted set or sequence :return: the value of the metric &quot;&quot;&quot; rr = 0. for rank, p in enumerate(prediction): if p in ground_truth: rr = 1. / (rank + 1) break return rr def count_a_in_b_unique(a, b): &quot;&quot;&quot; :param a: list of lists :param b: list of lists :return: number of elements of a in b &quot;&quot;&quot; count = 0 for el in a: if el in b: count += 1 return count def remove_duplicates(l): return [list(x) for x in set(tuple(x) for x in l)] . . METRICS = {&#39;precision&#39;:precision, &#39;recall&#39;:recall, &#39;mrr&#39;: mrr} TOPN=100 # length of the recommendation list . Evaluation with sequentially revealed user-profiles . Here we evaluate the quality of the recommendations in a setting in which user profiles are revealed sequentially. . The user profile starts from the first GIVEN_K events (or, alternatively, from the last -GIVEN_K events if GIVEN_K&lt;0). The recommendations are evaluated against the next LOOK_AHEAD events (the ground truth). The user profile is next expanded to the next STEP events, the ground truth is scrolled forward accordingly, and the evaluation continues until the sequence ends. . In typical next-item recommendation, we start with GIVEN_K=1, generate a set of alternatives that will evaluated against the next event in the sequence (LOOK_AHEAD=1), move forward of one step (STEP=1) and repeat until the sequence ends. . You can set the LOOK_AHEAD=&#39;all&#39; to see what happens if you had to recommend a whole sequence instead of a set of a set of alternatives to a user. . NOTE: Metrics are averaged over each sequence first, then averaged over all test sequences. . . GIVEN_K=1, LOOK_AHEAD=1, STEP=1 corresponds to the classical next-item evaluation . def eval_seqreveal(recommender, user_flg=0): GIVEN_K = 1 LOOK_AHEAD = 1 STEP=1 if user_flg: test_sequences, test_users = get_test_sequences_and_users(test_data, GIVEN_K, train_data[&#39;user_id&#39;].values) # we need user ids now! print(&#39;{} sequences available for evaluation ({} users)&#39;.format(len(test_sequences), len(np.unique(test_users)))) results = sequential_evaluation(recommender, test_sequences=test_sequences, users=test_users, given_k=GIVEN_K, look_ahead=LOOK_AHEAD, evaluation_functions=METRICS.values(), top_n=TOPN, scroll=True, # scrolling averages metrics over all profile lengths step=STEP) else: test_sequences = get_test_sequences(test_data, GIVEN_K) print(&#39;{} sequences available for evaluation&#39;.format(len(test_sequences))) results = sequential_evaluation(recommender, test_sequences=test_sequences, given_k=GIVEN_K, look_ahead=LOOK_AHEAD, evaluation_functions=METRICS.values(), top_n=TOPN, scroll=True, # scrolling averages metrics over all profile lengths step=STEP) # print(&#39;Sequential evaluation (GIVEN_K={}, LOOK_AHEAD={}, STEP={})&#39;.format(GIVEN_K, LOOK_AHEAD, STEP)) # for mname, mvalue in zip(METRICS.keys(), results): # print(&#39; t{}@{}: {:.4f}&#39;.format(mname, TOPN, mvalue)) return [results, GIVEN_K, LOOK_AHEAD, STEP] . . . Logging . for model in [poprecommender, fsmrecommender, mmcrecommender, p2vrecommender, rnnrecommender, fpmcrecommender, prnnrecommender, ]: if model in [fpmcrecommender, prnnrecommender]: results = eval_seqreveal(model, user_flg=1) # results = eval_staticprofile(model, user_flg=1) else: results = eval_seqreveal(model) wandb.init(name=&#39;seqreveal-&#39;+type(model).__name__, project=&#39;SARS Music30 x1&#39;, notes=&#39;sequentially revelaed user profile evaluation&#39;, tags=[&#39;sequence&#39;, &#39;music&#39;, &#39;seqreveal&#39;]) wandb.log({ &quot;Model&quot;: type(model).__name__, &quot;GIVEN_K&quot;: results[1], &quot;LOOK_AHEAD&quot;: results[2], &quot;STEP&quot;: results[3], &quot;Precision@100&quot;: results[0][0], &quot;Recall@100&quot;: results[0][1], &quot;MRR@100&quot;: results[0][2], }) . Evaluation with &quot;static&quot; user-profiles . Here we evaluate the quality of the recommendations in a setting in which user profiles are instead static. . The user profile starts from the first GIVEN_K events (or, alternatively, from the last -GIVEN_K events if GIVEN_K&lt;0). The recommendations are evaluated against the next LOOK_AHEAD events (the ground truth). . The user profile is not extended and the ground truth doesn&#39;t move forward. This allows to obtain &quot;snapshots&quot; of the recommendation performance for different user profile and ground truth lenghts. . Also here you can set the LOOK_AHEAD=&#39;all&#39; to see what happens if you had to recommend a whole sequence instead of a set of a set of alternatives to a user. . def eval_staticprofile(recommender, user_flg=0): GIVEN_K = 1 LOOK_AHEAD = &#39;all&#39; STEP=1 if user_flg: test_sequences, test_users = get_test_sequences_and_users(test_data, GIVEN_K, train_data[&#39;user_id&#39;].values) # we need user ids now! print(&#39;{} sequences available for evaluation ({} users)&#39;.format(len(test_sequences), len(np.unique(test_users)))) results = sequential_evaluation(recommender, test_sequences=test_sequences, users=test_users, given_k=GIVEN_K, look_ahead=LOOK_AHEAD, evaluation_functions=METRICS.values(), top_n=TOPN, scroll=False # notice that scrolling is disabled! ) else: test_sequences = get_test_sequences(test_data, GIVEN_K) print(&#39;{} sequences available for evaluation&#39;.format(len(test_sequences))) results = sequential_evaluation(recommender, test_sequences=test_sequences, given_k=GIVEN_K, look_ahead=LOOK_AHEAD, evaluation_functions=METRICS.values(), top_n=TOPN, scroll=False # notice that scrolling is disabled! ) return [results, GIVEN_K, LOOK_AHEAD, STEP] . . Logging . for model in [poprecommender, fsmrecommender, mmcrecommender, p2vrecommender, rnnrecommender, fpmcrecommender, prnnrecommender, ]: if model in [fpmcrecommender, prnnrecommender]: results = eval_staticprofile(model, user_flg=1) else: results = eval_staticprofile(model) wandb.init(name=&#39;staticprofile-&#39;+type(model).__name__, project=&#39;SARS Music30 x1&#39;, notes=&#39;sequentially static user profile evaluation&#39;, tags=[&#39;sequence&#39;, &#39;music&#39;, &#39;staticprofile&#39;]) wandb.log({ &quot;Model&quot;: type(model).__name__, &quot;GIVEN_K&quot;: results[1], &quot;LOOK_AHEAD&quot;: results[2], &quot;STEP&quot;: results[3], &quot;Precision@100&quot;: results[0][0], &quot;Recall@100&quot;: results[0][1], &quot;MRR@100&quot;: results[0][2], }) . Analysis of next-item recommendation . Here we propose to analyse the performance of the recommender system in the scenario of next-item recommendation over the following dimensions: . the length of the recommendation list, and | the length of the user profile. | . NOTE: This evaluation is by no means exhaustive, as different the hyper-parameters of the recommendation algorithm should be carefully tuned before drawing any conclusions. Unfortunately, given the time constraints for this tutorial, we had to leave hyper-parameter tuning out. A very useful reference about careful evaluation of (session-based) recommenders can be found at: . Evaluation of Session-based Recommendation Algorithms, Ludewig and Jannach, 2018 (paper) | . Evaluation for different recommendation list lengths . Logging . for model in [poprecommender, fsmrecommender, mmcrecommender, p2vrecommender, rnnrecommender, fpmcrecommender, prnnrecommender, ]: if model in [fpmcrecommender, prnnrecommender]: results = eval_reclength(model, user_flg=1) else: results = eval_reclength(model) wandb.init(name=&#39;plotreclen-&#39;+type(model).__name__, project=&#39;SARS Music30 x1&#39;, notes=&#39;rec list length variation evaluation&#39;, tags=[&#39;sequence&#39;, &#39;music&#39;, &#39;plotreclen&#39;]) wandb.log({&quot;Precision&quot;: results[0][0], &quot;Recall&quot;: results[0][1], &quot;MRR&quot;: results[0][2], &quot;Model&quot;: type(model).__name__, &quot;GIVEN_K&quot;: results[1], &quot;LOOK_AHEAD&quot;: results[2], &quot;STEP&quot;: results[3], }) . Evaluation for different user profile lengths . def eval_profilelength(recommender, user_flg=0): given_k_list = [1, 2, 3, 4] LOOK_AHEAD = 1 STEP = 1 TOPN = 20 res_list = [] if user_flg: test_sequences, test_users = get_test_sequences_and_users(test_data, max(given_k_list), train_data[&#39;user_id&#39;].values) # we need user ids now! print(&#39;{} sequences available for evaluation ({} users)&#39;.format(len(test_sequences), len(np.unique(test_users)))) for gk in given_k_list: print(&#39;Evaluating profiles having length: {}&#39;.format(gk)) res_tmp = sequential_evaluation(recommender, test_sequences=test_sequences, users=test_users, given_k=gk, look_ahead=LOOK_AHEAD, evaluation_functions=METRICS.values(), top_n=TOPN, scroll=False, # here we stop at each profile length step=STEP) mvalues = list(zip(METRICS.keys(), res_tmp)) res_list.append((gk, mvalues)) else: test_sequences = get_test_sequences(test_data, max(given_k_list)) print(&#39;{} sequences available for evaluation&#39;.format(len(test_sequences))) for gk in given_k_list: print(&#39;Evaluating profiles having length: {}&#39;.format(gk)) res_tmp = sequential_evaluation(recommender, test_sequences=test_sequences, given_k=gk, look_ahead=LOOK_AHEAD, evaluation_functions=METRICS.values(), top_n=TOPN, scroll=False, # here we stop at each profile length step=STEP) mvalues = list(zip(METRICS.keys(), res_tmp)) res_list.append((gk, mvalues)) # show separate plots per metric # fig, axes = plt.subplots(nrows=1, ncols=len(METRICS), figsize=(15,5)) res_list_t = list(zip(*res_list)) results = [] for midx, metric in enumerate(METRICS): mvalues = [res_list_t[1][j][midx][1] for j in range(len(res_list_t[1]))] fig, ax = plt.subplots(figsize=(5,5)) ax.plot(given_k_list, mvalues) ax.set_title(metric) ax.set_xticks(given_k_list) ax.set_xlabel(&#39;Profile length&#39;) fig.tight_layout() results.append(fig) return [results, TOPN, LOOK_AHEAD, STEP] . . Logging . for model in [poprecommender, fsmrecommender, mmcrecommender, p2vrecommender, rnnrecommender, fpmcrecommender, prnnrecommender, ]: if model in [fpmcrecommender, prnnrecommender]: results = eval_profilelength(model, user_flg=1) else: results = eval_profilelength(model) wandb.init(name=&#39;plotproflen-&#39;+type(model).__name__, project=&#39;SARS Music30 x1&#39;, notes=&#39;profile length variation evaluation&#39;, tags=[&#39;sequence&#39;, &#39;music&#39;, &#39;plotproflen&#39;]) wandb.log({&quot;Precision&quot;: results[0][0], &quot;Recall&quot;: results[0][1], &quot;MRR&quot;: results[0][2], &quot;Model&quot;: type(model).__name__, &quot;TOP_N&quot;: results[1], &quot;LOOK_AHEAD&quot;: results[2], &quot;STEP&quot;: results[3], }) . Artifact versioning . Model logging . import pickle run = wandb.init(job_type=&quot;model-logging&quot;, name=&quot;artifact-model&quot;, project=&#39;SARS Music30 x1&#39;) for model in [poprecommender, fsmrecommender, mmcrecommender, p2vrecommender, rnnrecommender, fpmcrecommender, prnnrecommender, knnrecommender]: # with open(type(model).__name__+&#39;.p&#39;, &#39;wb&#39;) as handle: # pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL) artifact = wandb.Artifact(type(model).__name__, type=&#39;model&#39;) artifact.add_file(type(model).__name__+&#39;.p&#39;) run.log_artifact(artifact) . . Finishing last run (ID:2uswdiud) before initializing another... Waiting for W&amp;B process to finish, PID 3744Program ended successfully. Find user logs for this run at: /content/wandb/run-20210425_175159-2uswdiud/logs/debug.log Find internal logs for this run at: /content/wandb/run-20210425_175159-2uswdiud/logs/debug-internal.log Synced 4 W&amp;B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s) Synced misty-dew-37: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/2uswdiud ...Successfully finished last run (ID:2uswdiud). Initializing new run: Tracking run with wandb version 0.10.27 Syncing run artifact-model to Weights &amp; Biases (Documentation). Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1 Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1hx1av67 Run data is saved locally in /content/wandb/run-20210425_175524-1hx1av67 Data logging . run = wandb.init(job_type=&quot;data-logging&quot;, name=&quot;artifact-data&quot;, project=&#39;SARS Music30 x1&#39;) artifact = wandb.Artifact(&#39;datasets&#39;, type=&#39;dataset&#39;) train_data.name = &#39;train_dataset&#39; test_data.name = &#39;test_dataset&#39; for dataset in [train_data, test_data]: dataset.to_csv(dataset.name+&#39;.p&#39;, index=False) artifact.add_file(dataset.name+&#39;.p&#39;) run.log_artifact(artifact) . . Finishing last run (ID:1lfb8icq) before initializing another... Waiting for W&amp;B process to finish, PID 3950Program ended successfully. Find user logs for this run at: /content/wandb/run-20210425_180606-1lfb8icq/logs/debug.log Find internal logs for this run at: /content/wandb/run-20210425_180606-1lfb8icq/logs/debug-internal.log Synced 4 W&amp;B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s) Synced artifact-data: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/1lfb8icq ...Successfully finished last run (ID:1lfb8icq). Initializing new run: Tracking run with wandb version 0.10.27 Syncing run artifact-data to Weights &amp; Biases (Documentation). Project page: https://wandb.ai/sparsh121/SARS%20Music30%20x1 Run page: https://wandb.ai/sparsh121/SARS%20Music30%20x1/runs/3tjioy7t Run data is saved locally in /content/wandb/run-20210425_180817-3tjioy7t &lt;wandb.sdk.wandb_artifacts.Artifact at 0x7f2a22eeeb50&gt; . W&amp;B Experiment Link . https://wandb.ai/sparsh121/SARS%20Music30%20x1/overview?workspace=user-sparsh121 . . Credits .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/sequence%20music/2021/04/26/Sequence-Aware-Recommenders-Music.html",
            "relUrl": "/sequence%20music/2021/04/26/Sequence-Aware-Recommenders-Music.html",
            "date": " ‚Ä¢ Apr 26, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Retail Product Recommendation with Negative Implicit Feedback",
            "content": "Data Loading . df = pd.read_csv(&#39;rawdata.csv&#39;, header = 0, names = [&#39;event&#39;,&#39;userid&#39;,&#39;itemid&#39;,&#39;timestamp&#39;], dtype={0:&#39;category&#39;, 1:&#39;category&#39;, 2:&#39;category&#39;}, parse_dates=[&#39;timestamp&#39;]) . df.head() . event userid itemid timestamp . 0 view_item | 2763227 | 11056 | 2020-01-13 16:05:31.244000+00:00 | . 1 add_to_cart | 2828666 | 14441 | 2020-01-13 22:36:38.680000+00:00 | . 2 view_item | 0620225789 | 14377 | 2020-01-14 10:54:41.886000+00:00 | . 3 view_item | 0620225789 | 14377 | 2020-01-14 10:54:47.692000+00:00 | . 4 add_to_cart | 0620225789 | 14377 | 2020-01-14 10:54:48.479000+00:00 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 99998 entries, 0 to 99997 Data columns (total 4 columns): event 99998 non-null category userid 99998 non-null category itemid 99998 non-null category timestamp 99998 non-null datetime64[ns, UTC] dtypes: category(3), datetime64[ns, UTC](1) memory usage: 1.7 MB . Wrangling . Removing Duplicates . df = df.drop_duplicates() . Label Encoding . userid_encoder = preprocessing.LabelEncoder() df.userid = userid_encoder.fit_transform(df.userid) # itemid normalization itemid_encoder = preprocessing.LabelEncoder() df.itemid = itemid_encoder.fit_transform(df.itemid) . Exploration . df.describe().T . count mean std min 25% 50% 75% max . userid 99432.0 | 4682.814677 | 3011.178734 | 0.0 | 2507.0 | 3687.0 | 6866.0 | 11476.0 | . itemid 99432.0 | 1344.579964 | 769.627122 | 0.0 | 643.0 | 1356.0 | 1997.0 | 2633.0 | . df.describe(exclude=&#39;int&#39;).T . count unique top freq first last . event 99432 | 5 | begin_checkout | 41459 | NaT | NaT | . timestamp 99432 | 61372 | 2020-01-16 04:21:49.377000+00:00 | 25 | 2020-01-13 16:05:31.244000+00:00 | 2020-03-10 13:02:21.376000+00:00 | . df.timestamp.max() - df.timestamp.min() . Timedelta(&#39;56 days 20:56:50.132000&#39;) . df.event.value_counts() . begin_checkout 41459 view_item 35397 purchase 9969 add_to_cart 7745 remove_from_cart 4862 Name: event, dtype: int64 . df.event.value_counts()/df.userid.nunique() . begin_checkout 3.612355 view_item 3.084168 purchase 0.868607 add_to_cart 0.674828 remove_from_cart 0.423630 Name: event, dtype: float64 . User Interactions . Add-to-cart Event Counts . Purchase Event Counts . Item Interactions . Rule-based Approaches . Top-N Trending Products . def top_trending(n, timeperiod, timestamp): start = str(timestamp.replace(microsecond=0) - pd.Timedelta(minutes=timeperiod)) end = str(timestamp.replace(microsecond=0)) trending_items = df.loc[(df.timestamp.between(start,end) &amp; (df.event==&#39;view_item&#39;)),:].sort_values(&#39;timestamp&#39;, ascending=False) return trending_items.itemid.value_counts().index[:n] . . user_current_time = df.timestamp[100] top_trending(5, 50, user_current_time) . Int64Index([2241, 972, 393, 1118, 126], dtype=&#39;int64&#39;) . Top-N Least Viewed Items . def least_n_items(n=10): temp1 = df.loc[df.event==&#39;view_item&#39;].groupby([&#39;itemid&#39;])[&#39;event&#39;].count().sort_values(ascending=True).reset_index() temp2 = df.groupby(&#39;itemid&#39;).timestamp.max().reset_index() item_ids = pd.merge(temp1,temp2,on=&#39;itemid&#39;).sort_values([&#39;event&#39;, &#39;timestamp&#39;], ascending=[True, False]).reset_index().loc[:n-1,&#39;itemid&#39;] return itemid_encoder.inverse_transform(item_ids.values) . . least_n_items(10) . array([&#39;15742&#39;, &#39;16052&#39;, &#39;16443&#39;, &#39;16074&#39;, &#39;16424&#39;, &#39;11574&#39;, &#39;11465&#39;, &#39;16033&#39;, &#39;11711&#39;, &#39;16013&#39;], dtype=object) . Data Transformation . Many times there are no explicit ratings or preferences given by users, that is, the interactions are usually implicit. This information may reflect users&#39; preference towards the items in an implicit manner. . Option 1 - Simple Count: The most simple technique is to count times of interactions between user and item for producing affinity scores. . Option 2 - Weighted Count: It is useful to consider the types of different interactions as weights in the count aggregation. For example, assuming weights of the three differen types, &quot;click&quot;, &quot;add&quot;, and &quot;purchase&quot;, are 1, 2, and 3, respectively. . Option 3 - Time-dependent Count: In many scenarios, time dependency plays a critical role in preparing dataset for building a collaborative filtering model that captures user interests drift over time. One of the common techniques for achieving time dependent count is to add a time decay factor in the counting. . A. Count . data_count = df.groupby([&#39;userid&#39;, &#39;itemid&#39;]).agg({&#39;timestamp&#39;: &#39;count&#39;}).reset_index() data_count.columns = [&#39;userid&#39;, &#39;itemid&#39;, &#39;affinity&#39;] data_count.head() . . userid itemid affinity . 0 0 | 328 | 1 | . 1 1 | 1122 | 1 | . 2 1 | 1204 | 1 | . 3 1 | 1271 | 1 | . 4 1 | 1821 | 1 | . B. Weighted Count . data_w[&#39;weight&#39;] = data_w[&#39;event&#39;].apply(lambda x: affinity_weights[x]) data_wcount = data_w.groupby([&#39;userid&#39;, &#39;itemid&#39;])[&#39;weight&#39;].sum().reset_index() data_wcount.columns = [&#39;userid&#39;, &#39;itemid&#39;, &#39;affinity&#39;] data_wcount.head() . . userid itemid affinity . 0 0 | 328 | 6 | . 1 1 | 1122 | 6 | . 2 1 | 1204 | 6 | . 3 1 | 1271 | 6 | . 4 1 | 1821 | 6 | . C. Time dependent Count . data_wt = data_w.groupby([&#39;userid&#39;, &#39;itemid&#39;])[&#39;timedecay&#39;].sum().reset_index() data_wt.columns = [&#39;userid&#39;, &#39;itemid&#39;, &#39;affinity&#39;] data_wt.head() . . userid itemid affinity . 0 0 | 328 | 0.117590 | . 1 1 | 1122 | 0.120232 | . 2 1 | 1204 | 0.120232 | . 3 1 | 1271 | 0.120232 | . 4 1 | 1821 | 0.120232 | . Train Test Split . Option 1 - Random Split: Random split simply takes in a data set and outputs the splits of the data, given the split ratios . Option 2 - Chronological split: Chronogically splitting method takes in a dataset and splits it on timestamp . data = data_w[[&#39;userid&#39;,&#39;itemid&#39;,&#39;timedecay&#39;,&#39;timestamp&#39;]] col = { &#39;col_user&#39;: &#39;userid&#39;, &#39;col_item&#39;: &#39;itemid&#39;, &#39;col_rating&#39;: &#39;timedecay&#39;, &#39;col_timestamp&#39;: &#39;timestamp&#39;, } col3 = { &#39;col_user&#39;: &#39;userid&#39;, &#39;col_item&#39;: &#39;itemid&#39;, &#39;col_timestamp&#39;: &#39;timestamp&#39;, } train, test = python_chrono_split(data, ratio=0.75, min_rating=10, filter_by=&#39;user&#39;, **col3) . . train.loc[train.userid==7,:] . userid itemid timedecay timestamp . 16679 7 | 1464 | 0.019174 | 2020-01-16 06:42:31.341000+00:00 | . 16691 7 | 1464 | 0.019174 | 2020-01-16 06:43:29.482000+00:00 | . 16692 7 | 2109 | 0.019174 | 2020-01-16 06:43:42.262000+00:00 | . 16694 7 | 1464 | 0.019174 | 2020-01-16 06:43:57.961000+00:00 | . 16805 7 | 201 | 0.019174 | 2020-01-16 06:45:55.261000+00:00 | . 16890 7 | 2570 | 0.019174 | 2020-01-16 06:54:12.315000+00:00 | . 16999 7 | 2570 | 0.019174 | 2020-01-16 06:54:29.130000+00:00 | . 17000 7 | 2570 | 0.057522 | 2020-01-16 06:54:35.097000+00:00 | . test.loc[test.userid==7,:] . userid itemid timedecay timestamp . 17001 7 | 1464 | 0.019174 | 2020-01-16 06:54:41.415000+00:00 | . 17003 7 | 1464 | 0.057522 | 2020-01-16 06:54:44.195000+00:00 | . Experiments . Item Popularity Recomendation Model . baseline_recommendations = pd.merge(item_counts, users_items_remove_seen, on=[&#39;itemid&#39;], how=&#39;inner&#39;) baseline_recommendations.head() . itemid count userid . 0 2564 | 461 | 7 | . 1 2564 | 461 | 21 | . 2 2564 | 461 | 73 | . 3 2564 | 461 | 75 | . 4 2564 | 461 | 113 | . print(&quot;MAP: t%f&quot; % eval_map, &quot;NDCG@K: t%f&quot; % eval_ndcg, &quot;Precision@K: t%f&quot; % eval_precision, &quot;Recall@K: t%f&quot; % eval_recall, sep=&#39; n&#39;) . MAP: 0.005334 NDCG@K: 0.010356 Precision@K: 0.007092 Recall@K: 0.011395 . Cornac BPR Model . all_predictions.head() . userid itemid prediction . 51214 7 | 2551 | -0.438445 | . 51215 7 | 481 | 2.522187 | . 51216 7 | 1185 | 2.406107 | . 51217 7 | 1766 | 1.112975 | . 51218 7 | 1359 | 2.083620 | . MAP: 0.004738 NDCG: 0.009597 Precision@K: 0.006601 Recall@K: 0.010597 . SARS Model . from reco_utils.recommender.sar.sar_singlenode import SARSingleNode TOP_K = 10 header = { &quot;col_user&quot;: &quot;userid&quot;, &quot;col_item&quot;: &quot;itemid&quot;, &quot;col_rating&quot;: &quot;timedecay&quot;, &quot;col_timestamp&quot;: &quot;timestamp&quot;, &quot;col_prediction&quot;: &quot;prediction&quot;, } model = SARSingleNode( similarity_type=&quot;jaccard&quot;, time_decay_coefficient=0, time_now=None, timedecay_formula=False, **header ) model.fit(train) . . Model: Top K: 10 MAP: 0.024426 NDCG: 0.032738 Precision@K: 0.019258 Recall@K: 0.036009 . Spotlight . Implicit Factorization Model . interactions = Interactions(user_ids = df.userid.astype(&#39;int32&#39;).values, item_ids = df.itemid.astype(&#39;int32&#39;).values, timestamps = df.timestamp.astype(&#39;int32&#39;), num_users = df.userid.nunique(), num_items = df.itemid.nunique()) train_user, test_user = random_train_test_split(interactions, test_percentage=0.2) model = ImplicitFactorizationModel(loss=&#39;bpr&#39;, embedding_dim=64, n_iter=10, batch_size=256, l2=0.0, learning_rate=0.01, optimizer_func=None, use_cuda=False, representation=None, sparse=False, num_negative_samples=10) model.fit(train_user, verbose=1) pr = precision_recall_score(model, test=test_user, train=train_user, k=10) print(&#39;Pricison@10 is {:.3f} and Recall@10 is {:.3f}&#39;.format(pr[0].mean(), pr[1].mean())) . . Epoch 0: loss 0.26659833122392174 Epoch 1: loss 0.06129162273462562 Epoch 2: loss 0.022607273167640066 Epoch 3: loss 0.013953083943443858 Epoch 4: loss 0.01050195922488137 Epoch 5: loss 0.009170394043447121 Epoch 6: loss 0.008144461540834697 Epoch 7: loss 0.007209992620171649 Epoch 8: loss 0.00663076309035038 Epoch 9: loss 0.006706491189820159 Pricison@10 is 0.007 and Recall@10 is 0.050 . Implicit Factorization Model with Grid Search . CNN Pooling Sequence Model . interactions = Interactions(user_ids = df.userid.astype(&#39;int32&#39;).values, item_ids = df.itemid.astype(&#39;int32&#39;).values+1, timestamps = df.timestamp.astype(&#39;int32&#39;)) train, test = random_train_test_split(interactions, test_percentage=0.2) train_seq = train.to_sequence(max_sequence_length=10) test_seq = test.to_sequence(max_sequence_length=10) model = ImplicitSequenceModel(loss=&#39;bpr&#39;, representation=&#39;pooling&#39;, embedding_dim=32, n_iter=10, batch_size=256, l2=0.0, learning_rate=0.01, optimizer_func=None, use_cuda=False, sparse=False, num_negative_samples=5) model.fit(train_seq, verbose=1) mrr_seq = sequence_mrr_score(model, test_seq) mrr_seq.mean() . . Epoch 0: loss 0.4226887328702895 Epoch 1: loss 0.23515070266410953 Epoch 2: loss 0.16919970976524665 Epoch 3: loss 0.1425025990751923 Epoch 4: loss 0.12612225017586692 Epoch 5: loss 0.11565039795441706 Epoch 6: loss 0.10787886735357222 Epoch 7: loss 0.10086931410383006 Epoch 8: loss 0.09461003749585542 Epoch 9: loss 0.09128284808553633 . 0.10435609591957387 . FastAI CollabLearner . learn.fit_one_cycle(1, 5e-6) . epoch train_loss valid_loss time . 0 | 2.054070 | 2.029182 | 00:20 | . learn.summary() . EmbeddingDotBias ====================================================================== Layer (type) Output Shape Param # Trainable ====================================================================== Embedding [50] 534,000 True ______________________________________________________________________ Embedding [50] 129,150 True ______________________________________________________________________ Embedding [1] 10,680 True ______________________________________________________________________ Embedding [1] 2,583 True ______________________________________________________________________ Total params: 676,413 Total trainable params: 676,413 Total non-trainable params: 0 Optimized with &#39;torch.optim.adam.Adam&#39;, betas=(0.9, 0.99) Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ Loss function : FlattenedLoss ====================================================================== Callbacks functions applied . learn.fit(10, 1e-3) . epoch train_loss valid_loss time . 0 | 1.770657 | 1.751797 | 00:18 | . 1 | 1.410351 | 1.528533 | 00:17 | . 2 | 1.153979 | 1.399136 | 00:17 | . 3 | 0.911953 | 1.326476 | 00:17 | . 4 | 0.784223 | 1.279517 | 00:17 | . 5 | 0.695546 | 1.248469 | 00:17 | . 6 | 0.637151 | 1.230954 | 00:18 | . 7 | 0.600011 | 1.216617 | 00:18 | . 8 | 0.573309 | 1.209507 | 00:18 | . 9 | 0.571132 | 1.204903 | 00:18 | .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/experiment%20retail/2021/04/25/Recommender-Implicit-Negative-Feedback.html",
            "relUrl": "/experiment%20retail/2021/04/25/Recommender-Implicit-Negative-Feedback.html",
            "date": " ‚Ä¢ Apr 25, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Retail Product Recommendations using word2vec",
            "content": "A person involved in sports-related activities might have an online buying pattern similar to this: . . If we can represent each of these products by a vector, then we can easily find similar products. So, if a user is checking out a product online, then we can easily recommend him/her similar products by using the vector similarity score between the products. . Data gathering and understanding . !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx . df = pd.read_excel(&#39;Online Retail.xlsx&#39;) df.head() . InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country . 0 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 2010-12-01 08:26:00 | 2.55 | 17850.0 | United Kingdom | . 1 536365 | 71053 | WHITE METAL LANTERN | 6 | 2010-12-01 08:26:00 | 3.39 | 17850.0 | United Kingdom | . 2 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 2010-12-01 08:26:00 | 2.75 | 17850.0 | United Kingdom | . 3 536365 | 84029G | KNITTED UNION FLAG HOT WATER BOTTLE | 6 | 2010-12-01 08:26:00 | 3.39 | 17850.0 | United Kingdom | . 4 536365 | 84029E | RED WOOLLY HOTTIE WHITE HEART. | 6 | 2010-12-01 08:26:00 | 3.39 | 17850.0 | United Kingdom | . Given below is the description of the fields in this dataset: . InvoiceNo: Invoice number, a unique number assigned to each transaction. . | StockCode: Product/item code. a unique number assigned to each distinct product. . | Description: Product description . | Quantity: The quantities of each product per transaction. . | InvoiceDate: Invoice Date and time. The day and time when each transaction was generated. . | CustomerID: Customer number, a unique number assigned to each customer. . | Data Preprocessing . df.isnull().sum() . InvoiceNo 0 StockCode 0 Description 1454 Quantity 0 InvoiceDate 0 UnitPrice 0 CustomerID 135080 Country 0 dtype: int64 . Since we have sufficient data, we will drop all the rows with missing values. . df.dropna(inplace=True) # again check missing values df.isnull().sum() . InvoiceNo 0 StockCode 0 Description 0 Quantity 0 InvoiceDate 0 UnitPrice 0 CustomerID 0 Country 0 dtype: int64 . df[&#39;StockCode&#39;]= df[&#39;StockCode&#39;].astype(str) . customers = df[&quot;CustomerID&quot;].unique().tolist() len(customers) . 4372 . There are 4,372 customers in our dataset. For each of these customers we will extract their buying history. In other words, we can have 4,372 sequences of purchases. . Data Preparation . It is a good practice to set aside a small part of the dataset for validation purpose. Therefore, we will use data of 90% of the customers to create word2vec embeddings. Let&#39;s split the data. . random.shuffle(customers) # extract 90% of customer ID&#39;s customers_train = [customers[i] for i in range(round(0.9*len(customers)))] # split data into train and validation set train_df = df[df[&#39;CustomerID&#39;].isin(customers_train)] validation_df = df[~df[&#39;CustomerID&#39;].isin(customers_train)] . Let&#39;s create sequences of purchases made by the customers in the dataset for both the train and validation set. . purchases_train = [] # populate the list with the product codes for i in tqdm(customers_train): temp = train_df[train_df[&quot;CustomerID&quot;] == i][&quot;StockCode&quot;].tolist() purchases_train.append(temp) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3935/3935 [00:05&lt;00:00, 664.97it/s] . purchases_val = [] # populate the list with the product codes for i in tqdm(validation_df[&#39;CustomerID&#39;].unique()): temp = validation_df[validation_df[&quot;CustomerID&quot;] == i][&quot;StockCode&quot;].tolist() purchases_val.append(temp) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 437/437 [00:00&lt;00:00, 1006.50it/s] . Build word2vec Embeddings for Products . model = Word2Vec(window = 10, sg = 1, hs = 0, negative = 10, # for negative sampling alpha=0.03, min_alpha=0.0007, seed = 14) model.build_vocab(purchases_train, progress_per=200) model.train(purchases_train, total_examples = model.corpus_count, epochs=10, report_delay=1) . (3657318, 3696290) . model.save(&quot;word2vec_2.model&quot;) . As we do not plan to train the model any further, we are calling init_sims(), which will make the model much more memory-efficient . model.init_sims(replace=True) . print(model) . Word2Vec(vocab=3153, size=100, alpha=0.03) . Now we will extract the vectors of all the words in our vocabulary and store it in one place for easy access . X = model[model.wv.vocab] X.shape . (3153, 100) . Visualize word2vec Embeddings . It is always quite helpful to visualize the embeddings that you have created. Over here we have 100 dimensional embeddings. We can&#39;t even visualize 4 dimensions let alone 100. Therefore, we are going to reduce the dimensions of the product embeddings from 100 to 2 by using the UMAP algorithm, it is used for dimensionality reduction. . import umap cluster_embedding = umap.UMAP(n_neighbors=30, min_dist=0.0, n_components=2, random_state=42).fit_transform(X) plt.figure(figsize=(10,9)) plt.scatter(cluster_embedding[:, 0], cluster_embedding[:, 1], s=3, cmap=&#39;Spectral&#39;); . . Every dot in this plot is a product. As you can see, there are several tiny clusters of these datapoints. These are groups of similar products. . Generate and validate recommendations . We are finally ready with the word2vec embeddings for every product in our online retail dataset. Now our next step is to suggest similar products for a certain product or a product&#39;s vector. . Let&#39;s first create a product-ID and product-description dictionary to easily map a product&#39;s description to its ID and vice versa. . products = train_df[[&quot;StockCode&quot;, &quot;Description&quot;]] # remove duplicates products.drop_duplicates(inplace=True, subset=&#39;StockCode&#39;, keep=&quot;last&quot;) # create product-ID and product-description dictionary products_dict = products.groupby(&#39;StockCode&#39;)[&#39;Description&#39;].apply(list).to_dict() . products_dict[&#39;84029E&#39;] . [&#39;RED WOOLLY HOTTIE WHITE HEART.&#39;] . We have defined the function below. It will take a product&#39;s vector (n) as input and return top 6 similar products. . Let&#39;s try out our function by passing the vector of the product &#39;90019A&#39; (&#39;SILVER M.O.P ORBIT BRACELET&#39;) . similar_products(model[&#39;90019A&#39;]) . [(&#39;SILVER M.O.P ORBIT DROP EARRINGS&#39;, 0.7879312634468079), (&#39;AMBER DROP EARRINGS W LONG BEADS&#39;, 0.7682332992553711), (&#39;JADE DROP EARRINGS W FILIGREE&#39;, 0.761816143989563), (&#39;DROP DIAMANTE EARRINGS PURPLE&#39;, 0.7489826679229736), (&#39;SILVER LARIAT BLACK STONE EARRINGS&#39;, 0.7389366626739502), (&#39;WHITE VINT ART DECO CRYSTAL NECKLAC&#39;, 0.7352254390716553)] . Cool! The results are pretty relevant and match well with the input product. However, this output is based on the vector of a single product only. What if we want recommend a user products based on the multiple purchases he or she has made in the past? . One simple solution is to take average of all the vectors of the products he has bought so far and use this resultant vector to find similar products. For that we will use the function below that takes in a list of product ID&#39;s and gives out a 100 dimensional vector which is mean of vectors of the products in the input list. . def aggregate_vectors(products): product_vec = [] for i in products: try: product_vec.append(model[i]) except KeyError: continue return np.mean(product_vec, axis=0) . . If you can recall, we have already created a separate list of purchase sequences for validation purpose. Now let&#39;s make use of that. . The length of the first list of products purchased by a user is 314. We will pass this products&#39; sequence of the validation set to the function aggregate_vectors. . Well, the function has returned an array of 100 dimension. It means the function is working fine. Now we can use this result to get the most similar products. Let&#39;s do it. . similar_products(aggregate_vectors(purchases_val[0])) . [(&#39;WHITE SPOT BLUE CERAMIC DRAWER KNOB&#39;, 0.6860978603363037), (&#39;RED SPOT CERAMIC DRAWER KNOB&#39;, 0.6785424947738647), (&#39;BLUE STRIPE CERAMIC DRAWER KNOB&#39;, 0.6783121824264526), (&#39;BLUE SPOT CERAMIC DRAWER KNOB&#39;, 0.6738985776901245), (&#39;CLEAR DRAWER KNOB ACRYLIC EDWARDIAN&#39;, 0.6731897592544556), (&#39;RED STRIPE CERAMIC DRAWER KNOB&#39;, 0.6667704582214355)] . As it turns out, our system has recommended 6 products based on the entire purchase history of a user. Moreover, if you want to get products suggestions based on the last few purchases only then also you can use the same set of functions. . Below we are giving only the last 10 products purchased as input. . similar_products(aggregate_vectors(purchases_val[0][-10:])) . [(&#39;BLUE SPOT CERAMIC DRAWER KNOB&#39;, 0.7394766807556152), (&#39;RED SPOT CERAMIC DRAWER KNOB&#39;, 0.7364704012870789), (&#39;WHITE SPOT BLUE CERAMIC DRAWER KNOB&#39;, 0.7347637414932251), (&#39;ASSORTED COLOUR BIRD ORNAMENT&#39;, 0.7345550060272217), (&#39;RED STRIPE CERAMIC DRAWER KNOB&#39;, 0.7305896878242493), (&#39;WHITE SPOT RED CERAMIC DRAWER KNOB&#39;, 0.6979628801345825)] . References . https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/ | https://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/ | https://www.analyticsinsight.net/building-recommendation-system-using-item2vec/ | https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484 | https://capablemachine.com/2020/06/23/word-embedding/ | .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/sequence%20retail/2021/04/24/rec-medium-word2vec.html",
            "relUrl": "/sequence%20retail/2021/04/24/rec-medium-word2vec.html",
            "date": " ‚Ä¢ Apr 24, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Recommender System with Node2vec Graph Embeddings",
            "content": "Data gathering and exploration . rating_df = pd.read_csv(&#39;ml100k_ratings.csv&#39;, sep=&#39;,&#39;, header=0) rating_df.head() . userId movieId rating timestamp . 0 1 | 1 | 4.0 | 964982703 | . 1 1 | 3 | 4.0 | 964981247 | . 2 1 | 6 | 4.0 | 964982224 | . 3 1 | 47 | 5.0 | 964983815 | . 4 1 | 50 | 5.0 | 964982931 | . movie_df = pd.read_csv(&#39;ml100k_movies.csv&#39;, sep=&#39;,&#39;, header=0) movie_df.head() . movieId title genres . 0 1 | Toy Story (1995) | Adventure|Animation|Children|Comedy|Fantasy | . 1 2 | Jumanji (1995) | Adventure|Children|Fantasy | . 2 3 | Grumpier Old Men (1995) | Comedy|Romance | . 3 4 | Waiting to Exhale (1995) | Comedy|Drama|Romance | . 4 5 | Father of the Bride Part II (1995) | Comedy | . Neighborhood method . Jaccard Similarity . If we ignore the ratings that the users have given to the movies, and consider the movies that the users have watched, we get a set of movies/users for every user/movie. Think of this formulation as a bipartite graph of users and movies where there is an edge between a user and a movie if a user has watched the movie, the edges have all same weights. . Create a dictionary of movies as keys and values as users that have rated them . Since we have a set of users to characterize each movie, to compute the similarity of two movies, we use Jaccard Index which, for two sets, is the ratio of number of elements in the intersection and number of elements in the union. . def jaccard(movie1, movie2, movie_sets): a = movie_sets[movie1] b = movie_sets[movie2] intersection = float(len(a.intersection(b))) return intersection / (len(a) + len(b) - intersection) . . Let&#39;s explore similarity between some movies, qualitatively. We use the movies dataframe to get the names of the movies via their Ids. . movie_df[movie_df.movieId == 260].title.values[0] . &#39;Star Wars: Episode IV - A New Hope (1977)&#39; . Jaccard distance between &#39;StarWars:EpisodeIV-ANewHope(1977)&#39; and &#39;StarWars:EpisodeV-TheEmpireStrikesBack(1980)&#39; is 0.70 Jaccard distance between &#39;StarWars:EpisodeIV-ANewHope(1977)&#39; and &#39;StarWars:EpisodeVI-ReturnoftheJedi(1983)&#39; is 0.64 Jaccard distance between &#39;StarWars:EpisodeIV-ANewHope(1977)&#39; and &#39;ToyStory(1995)&#39; is 0.40 . The movie Star Wars IV has higher similarity score with other Star Wars as compared to Toy Story. . Using the Jaccard Index, we can retrieve top-k similar movies to a given movie. This provides a way to recommend movies of a user which are similar to the movies that the user has watched. . get_similar_movies_jaccard(260, movie_sets) . {&#39;movie&#39;: &#39;Star Wars: Episode IV - A New Hope (1977)&#39;, &#39;sim_movies&#39;: [&#39;Star Wars: Episode IV - A New Hope (1977)&#39;, &#39;Star Wars: Episode V - The Empire Strikes Back (1980)&#39;, &#39;Star Wars: Episode VI - Return of the Jedi (1983)&#39;, &#39;Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)&#39;, &#39;Matrix, The (1999)&#39;]} . get_similar_movies_jaccard(1, movie_sets) . {&#39;movie&#39;: &#39;Toy Story (1995)&#39;, &#39;sim_movies&#39;: [&#39;Toy Story (1995)&#39;, &#39;Independence Day (a.k.a. ID4) (1996)&#39;, &#39;Jurassic Park (1993)&#39;, &#39;Star Wars: Episode IV - A New Hope (1977)&#39;, &#39;Forrest Gump (1994)&#39;]} . Cosine similarity . Rather than the set based similarity like Jaccard, we can define every movie as a sparse vector of dimension equal to the number of users and the vector entry corresponding to each user is given by the rating that the user has for the movie or zero if no rating exists (i.e. the user hasn&#39;t seen/rated the movie). . print(1.0 - cosine(movie_sparse_vecs[224], movie_sparse_vecs[897])) . 0.8324073552233735 . def get_similar_movies_nbd_cosine(movieid, movie_vecs, top_n=5): movie = movie_df[movie_df.movieId == movieid].title.values[0] movie_idx = movie2id[movieid] query = movie_vecs[movie_idx].reshape(1,-1) ranking = cosine_similarity(movie_vecs,query) top_ids = np.argsort(ranking, axis=0) top_ids = top_ids[::-1][:top_n] top_movie_ids = [movies[j[0]] for j in top_ids] sim_movies = [movie_df[movie_df.movieId == id].title.values[0] for id in top_movie_ids] return {&#39;movie&#39;: movie, &#39;sim_movies&#39;: sim_movies} . . movieid = 1 movie_data = movie_sparse_vecs get_similar_movies_nbd_cosine(movieid, movie_data, top_n=5) . {&#39;movie&#39;: &#39;Toy Story (1995)&#39;, &#39;sim_movies&#39;: [&#39;Toy Story (1995)&#39;, &#39;Toy Story 2 (1999)&#39;, &#39;Jurassic Park (1993)&#39;, &#39;Independence Day (a.k.a. ID4) (1996)&#39;, &#39;Star Wars: Episode IV - A New Hope (1977)&#39;]} . movieid = 260 movie_data = movie_sparse_vecs get_similar_movies_nbd_cosine(movieid, movie_data, top_n=5) . {&#39;movie&#39;: &#39;Star Wars: Episode IV - A New Hope (1977)&#39;, &#39;sim_movies&#39;: [&#39;Star Wars: Episode IV - A New Hope (1977)&#39;, &#39;Star Wars: Episode V - The Empire Strikes Back (1980)&#39;, &#39;Star Wars: Episode VI - Return of the Jedi (1983)&#39;, &#39;Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)&#39;, &#39;Matrix, The (1999)&#39;]} . Factorization method . Singular Value Decomposition . A very popular technique for recommendation systems is via matrix factorization. The idea is to reduce the dimensionality of the data before calculating similar movies/users. We factorize the user-item matrix to obtain the user factors and item factors which are the low-dimensional embeddings such that &#39;similar&#39; user/items are mapped to &#39;nearby&#39; points. . This kind of analysis can generate matches that are impossible to find with the techniques discussed above as the latent factors can capture attributes which are hard for raw data to deciper e.g. a latent factor can correspond to the degree to which a movie is female oriented or degree to which there is a slow development of the charcters. . Moreover, the user and the movies are embedded to the same space, which provides a direct way to compute user-movie similarity. . We will use Singular Value Decomposition (SVD) for factorizing the matrix. . Normalize the rating matrix . normalised_mat = ratings_mat - np.asarray([(np.mean(ratings_mat, 1))]).T . The number of the latent-factors is chosen to be 50 i.e. top-50 singular values of the SVD are considered. The choice of the number of latent factors is a hyperparameter of the model, and requires a more sophisticated analysis to tune. We provide no reason for the choice of 50. . n_factors = 50 A = normalised_mat.T / np.sqrt(ratings_mat.shape[0] - 1) U, S, V = svds(A, n_factors) print(U.shape, V.shape) . (610, 50) (50, 9724) . movie_factors = V.T user_factors = U . Instead of representing each movie as a sparse vector of the ratings of all 360,000 possible users for it, after factorizing the matrix each movie will be represented by a 50 dimensional dense vector. . Define a routine to get top-n movies similar to a given movie. . def get_similar_movies_matrix_factorization(data, movieid, top_n=10): index = movieid - 1 # Movie id starts from 1 movie = movie_df[movie_df.movieId == movieid].title.values[0] movie_row = data[index].reshape(1,-1) similarity = cosine_similarity(movie_row, data) sort_indexes = np.argsort(-similarity)[0] return {&#39;movie&#39;: movie, &#39;sim_movies&#39;: [movie_df[movie_df.movieId == id].title.values[0] for id in sort_indexes[:top_n] + 1]} . . movie_id = 260 get_similar_movies_matrix_factorization(movie_factors, movie_id) . {&#39;movie&#39;: &#39;Priest (1994)&#39;, &#39;sim_movies&#39;: [&#39;Priest (1994)&#39;, &#39;Heidi Fleiss: Hollywood Madam (1995)&#39;, &#39;Reds (1981)&#39;, &#39;I Went Down (1997)&#39;, &#39;Metroland (1997)&#39;, &#39;Love Serenade (1996)&#39;, &#39;Cold Fever (√Å k√∂ldum klaka) (1995)&#39;, &#39;Suture (1993)&#39;, &#39;Whole Wide World, The (1996)&#39;, &#39;Walking and Talking (1996)&#39;]} . movie_id = 1 get_similar_movies_matrix_factorization(movie_factors, movie_id) . {&#39;movie&#39;: &#39;Toy Story (1995)&#39;, &#39;sim_movies&#39;: [&#39;Toy Story (1995)&#39;, &#39;Back to the Future (1985)&#39;, &#34;Bug&#39;s Life, A (1998)&#34;, &#39;Babe (1995)&#39;, &#39;Star Wars: Episode IV - A New Hope (1977)&#39;, &#39;Who Framed Roger Rabbit? (1988)&#39;, &#39;Mrs. Doubtfire (1993)&#39;, &#39;When Harry Met Sally... (1989)&#39;, &#39;101 Dalmatians (One Hundred and One Dalmatians) (1961)&#39;, &#39;Home Alone (1990)&#39;]} . Since the user and movies are in the same space, we can also compute movies similar to a user. A recommendation model can be defined as showing movies similar to the given user. . def get_recommendations_matrix_factorization(userid, user_factors, movie_factors, top_n=10): user_vec = user_factors[userid - 1].reshape(1,-1) similarity = cosine_similarity(user_vec, movie_factors) sort_indexes = np.argsort(-similarity)[0] return [movie_df[movie_df.movieId == id].title.values[0] for id in sort_indexes[:top_n] + 1] . . top_recos = get_recommendations_matrix_factorization(1, user_factors, movie_factors) top_recos . [&#39;Jungle2Jungle (a.k.a. Jungle 2 Jungle) (1997)&#39;, &#34;Pete&#39;s Dragon (2016)&#34;, &#39;Cellular (2004)&#39;, &#39;Replacement Killers, The (1998)&#39;, &#39;Rough Night (2017)&#39;, &#39;Star Wars: Episode III - Revenge of the Sith (2005)&#39;, &#39;Gentlemen Prefer Blondes (1953)&#39;, &#39;Spanglish (2004)&#39;, &#39;Sorry to Bother You (2018)&#39;, &#39;Planet of the Apes (2001)&#39;] . Graph Embedding method . Create a user-movie graph with edge weights as the ratings. We will use DeepWalk to embed every node of the graph to a low-dimensional space. . user_item_edgelist = rating_df[[&#39;userId&#39;, &#39;movieId&#39;, &#39;rating&#39;]] user_item_edgelist.head() . userId movieId rating . 0 1 | 1 | 4.0 | . 1 1 | 3 | 4.0 | . 2 1 | 6 | 4.0 | . 3 1 | 44 | 5.0 | . 4 1 | 47 | 5.0 | . Create a user-movie weighted graph using python library networkx. . user_movie_graph = nx.Graph() . for x in user_item_edgelist.values: usr = (x[0], &#39;user&#39;) movie = (x[1], &#39;movie&#39;) user_movie_graph.add_node(user2dict[usr]) user_movie_graph.add_node(movie2dict[movie]) user_movie_graph.add_edge(user2dict[usr], movie2dict[movie], weight=float(x[2])) . user_movie_graph.number_of_edges() . 100836 . user_movie_graph.number_of_nodes() . 10334 . DeepWalk . We will use the implementation of DeepWalk provided in node2vec which is a bit different from original DeepWalk e.g. it uses negative sampling whereas the original DeepWalk paper used hierarchical sampling for the skip-gram model. . To create embeddings from the context and non-context pairs, we are using Gensim python library. One can easily use Google word2vec or Facebook fasttext for this task. . import numpy as np import networkx as nx import random class Graph(): def __init__(self, nx_G, is_directed, p, q): self.G = nx_G self.is_directed = is_directed self.p = p self.q = q def node2vec_walk(self, walk_length, start_node): &#39;&#39;&#39; Simulate a random walk starting from start node. &#39;&#39;&#39; G = self.G alias_nodes = self.alias_nodes alias_edges = self.alias_edges walk = [start_node] while len(walk) &lt; walk_length: cur = walk[-1] cur_nbrs = sorted(G.neighbors(cur)) if len(cur_nbrs) &gt; 0: if len(walk) == 1: walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])]) else: prev = walk[-2] next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0], alias_edges[(prev, cur)][1])] walk.append(next) else: break return walk def simulate_walks(self, num_walks, walk_length): &#39;&#39;&#39; Repeatedly simulate random walks from each node. &#39;&#39;&#39; G = self.G walks = [] nodes = list(G.nodes()) print(&#39;Walk iteration:&#39;) for walk_iter in range(num_walks): print(str(walk_iter+1), &#39;/&#39;, str(num_walks)) random.shuffle(nodes) for node in nodes: walks.append(self.node2vec_walk(walk_length=walk_length, start_node=node)) return walks def get_alias_edge(self, src, dst): &#39;&#39;&#39; Get the alias edge setup lists for a given edge. &#39;&#39;&#39; G = self.G p = self.p q = self.q unnormalized_probs = [] for dst_nbr in sorted(G.neighbors(dst)): if dst_nbr == src: unnormalized_probs.append(G[dst][dst_nbr][&#39;weight&#39;]/p) elif G.has_edge(dst_nbr, src): unnormalized_probs.append(G[dst][dst_nbr][&#39;weight&#39;]) else: unnormalized_probs.append(G[dst][dst_nbr][&#39;weight&#39;]/q) norm_const = sum(unnormalized_probs) try: normalized_probs = [float(u_prob)/norm_const for u_prob in unnormalized_probs] except: normalized_probs = [0.0 for u_prob in unnormalized_probs] return alias_setup(normalized_probs) def preprocess_transition_probs(self): &#39;&#39;&#39; Preprocessing of transition probabilities for guiding the random walks. &#39;&#39;&#39; G = self.G is_directed = self.is_directed alias_nodes = {} for node in G.nodes(): unnormalized_probs = [G[node][nbr][&#39;weight&#39;] for nbr in sorted(G.neighbors(node))] norm_const = sum(unnormalized_probs) try: normalized_probs = [float(u_prob)/norm_const for u_prob in unnormalized_probs] except: print(node) normalized_probs = [0.0 for u_prob in unnormalized_probs] alias_nodes[node] = alias_setup(normalized_probs) alias_edges = {} triads = {} if is_directed: for edge in G.edges(): alias_edges[edge] = self.get_alias_edge(edge[0], edge[1]) else: for edge in G.edges(): alias_edges[edge] = self.get_alias_edge(edge[0], edge[1]) alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0]) self.alias_nodes = alias_nodes self.alias_edges = alias_edges return def alias_setup(probs): &#39;&#39;&#39; Compute utility lists for non-uniform sampling from discrete distributions. Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/ for details &#39;&#39;&#39; K = len(probs) q = np.zeros(K) J = np.zeros(K, dtype=np.int) smaller = [] larger = [] for kk, prob in enumerate(probs): q[kk] = K*prob if q[kk] &lt; 1.0: smaller.append(kk) else: larger.append(kk) while len(smaller) &gt; 0 and len(larger) &gt; 0: small = smaller.pop() large = larger.pop() J[small] = large q[large] = q[large] + q[small] - 1.0 if q[large] &lt; 1.0: smaller.append(large) else: larger.append(large) return J, q def alias_draw(J, q): &#39;&#39;&#39; Draw sample from a non-uniform discrete distribution using alias sampling. &#39;&#39;&#39; K = len(J) kk = int(np.floor(np.random.rand()*K)) if np.random.rand() &lt; q[kk]: return kk else: return J[kk] . . G = Graph(user_movie_graph, is_directed=False, p=1, q=1) . p,q = 1 for DeeWalk as the random walks are completely unbiased. . G.preprocess_transition_probs() . Compute the random walks. . 10 walks for every node. | Each walk of length 80. | . walks = G.simulate_walks(num_walks=10, walk_length=80) . Walk iteration: 1 / 10 2 / 10 3 / 10 4 / 10 5 / 10 6 / 10 7 / 10 8 / 10 9 / 10 10 / 10 . len(walks) . 103340 . Learn Embeddings via Gensim, which creates context/non-context pairs and then Skip-gram. . def learn_embeddings(walks): &#39;&#39;&#39; Learn embeddings by optimizing the Skipgram objective using SGD. Uses Gensim Word2Vec. &#39;&#39;&#39; walks = [list(map(str, walk)) for walk in walks] model = Word2Vec(walks, size=50, window=10, min_count=0, sg=1, workers=8, iter=1) return model.wv . . node_embeddings = learn_embeddings(walks) . The output of gensim is a specific type of key-value pair with keys as the string-ed node ids and the values are numpy array of embeddings, each of shape (50,) . node_embeddings[&#39;0&#39;] . array([-2.1159494e-02, -4.3432057e-01, 6.9623584e-01, 4.8146245e-01, 9.6677847e-02, -3.1050149e-02, 1.9733864e-01, 7.9867625e-01, -6.6979128e-01, 6.5312237e-01, 3.1304079e-01, -1.3411559e-01, -1.5454048e-01, -2.7333325e-01, 1.4711864e-01, 2.2469629e-01, -4.3890166e-01, 2.9871342e-01, -6.9798152e-03, -1.7996507e-02, -6.7855030e-02, -4.3489739e-01, 1.5584855e-01, 7.7486165e-02, 3.7617716e-01, 2.8012756e-01, -8.3905622e-02, -3.5362533e-01, 2.2293477e-01, -1.4108117e-01, 1.6970167e-01, -6.3179672e-01, 1.5170584e-02, 6.1733756e-02, -1.2013953e-01, -2.3064958e-01, 2.4610328e-02, 5.0556450e-04, 2.1398006e-01, -1.0361964e-01, 4.8838145e-01, -3.6318046e-01, -3.1330651e-01, 8.6576389e-03, 1.9654050e-02, -4.6078888e-01, 2.7895319e-01, -1.7853497e-04, -1.7593203e-01, 2.8144377e-01], dtype=float32) . movie1 = str(movie2dict[(260, &#39;movie&#39;)]) movie2 = str(movie2dict[(1196, &#39;movie&#39;)]) 1.0 - cosine(node_embeddings[movie1], node_embeddings[movie2]) . 0.42126354575157166 . movie3 = str(movie2dict[(1210, &#39;movie&#39;)]) 1.0 - cosine(node_embeddings[movie1], node_embeddings[movie3]) . 0.29301050305366516 . movie4 = str(movie2dict[(1, &#39;movie&#39;)]) 1.0 - cosine(node_embeddings[movie1], node_embeddings[movie4]) . 0.5913276672363281 . Since we worked with integer ids for nodes, let&#39;s create reverse mapping dictionaries that map integer user/movie to their actual ids. . reverse_movie2dict = {k:v for v,k in movie2dict.items()} reverse_user2dict = {k:v for v,k in user2dict.items()} . node_vecs = [node_embeddings[str(i)] for i in range(cnt)] node_vecs = np.array(node_vecs) node_vecs.shape . (10334, 50) . Movies similar to a given movie as an evaluation of the system. . def get_similar_movies_graph_embeddings(movieid, movie_embed, top_n=10): movie_idx = movie2dict[movieid] query = movie_embed[movie_idx].reshape(1,-1) ranking = cosine_similarity(query, movie_embed) top_ids = np.argsort(-ranking)[0] top_movie_ids = [reverse_movie2dict[j] for j in top_ids if j in reverse_movie2dict][:top_n] sim_movies = [movie_df[movie_df.movieId == id[0]].title.values[0] for id in top_movie_ids] return sim_movies . . get_similar_movies_graph_embeddings((260, &#39;movie&#39;), node_vecs)[:10] . [&#39;Priest (1994)&#39;, &#39;Heidi Fleiss: Hollywood Madam (1995)&#39;, &#39;My Crazy Life (Mi vida loca) (1993)&#39;, &#39;Before the Rain (Pred dozhdot) (1994)&#39;, &#39;Boys of St. Vincent, The (1992)&#39;, &#39;Last Dance (1996)&#39;, &#39;Awfully Big Adventure, An (1995)&#39;, &#39;Queen Margot (Reine Margot, La) (1994)&#39;, &#34;Widows&#39; Peak (1994)&#34;, &#39;What Happened Was... (1994)&#39;] . get_similar_movies_graph_embeddings((122, &#39;movie&#39;), node_vecs)[:10] . [&#39;Awfully Big Adventure, An (1995)&#39;, &#39;What Happened Was... (1994)&#39;, &#39;Song of the Little Road (Pather Panchali) (1955)&#39;, &#39;Death and the Maiden (1994)&#39;, &#39;Heidi Fleiss: Hollywood Madam (1995)&#39;, &#39;My Crazy Life (Mi vida loca) (1993)&#39;, &#39;Love &amp; Human Remains (1993)&#39;, &#34;It&#39;s My Party (1996)&#34;, &#39;Perez Family, The (1995)&#39;, &#39;Bitter Moon (1992)&#39;] . We can also define the recommendation model based on the cosine similarity i.e the movies are ranked for a given user in terms of the cosine similarities of their corresponding embeddings with the embedding of the user. . def get_recommended_movies_graph_embeddings(userid, node_embed, top_n=10): user_idx = user2dict[userid] query = node_embed[user_idx].reshape(1,-1) ranking = cosine_similarity(query, node_embed) top_ids = np.argsort(-ranking)[0] top_movie_ids = [reverse_movie2dict[j] for j in top_ids if j in reverse_movie2dict][:top_n] reco_movies = [movie_df[movie_df.movieId == id[0]].title.values[0] for id in top_movie_ids] return reco_movies . . get_recommended_movies_graph_embeddings((1, &#39;user&#39;), node_vecs, top_n=10) . [&#39;Best Men (1997)&#39;, &#39;Newton Boys, The (1998)&#39;, &#39;Howard the Duck (1986)&#39;, &#34;Gulliver&#39;s Travels (1939)&#34;, &#39;Shaft (1971)&#39;, &#39;Teenage Mutant Ninja Turtles III (1993)&#39;, &#39;Welcome to Woop-Woop (1997)&#39;, &#39;Song of the South (1946)&#39;, &#39;Three Caballeros, The (1945)&#39;, &#39;Lord of the Rings, The (1978)&#39;] . Evaluation . As another evalution, let&#39;s compare the generated recommendation for a user to the movies tnat the user has actually rated highly. We will get top 10 recommendations for a user, ranked by the cosine similarity, and compute how many of these movies comes from the set of the movies that the user has rated &gt;= 4.5. This tantamounts to Precision@10 metric. For comparison, we will also compute the Precision for the recommendations produced by the matrix factorization model. . idx = 1 recos = set(get_recommended_movies_graph_embeddings((idx, &#39;user&#39;), node_vecs, top_n=10)) true_pos = set([movie_df[movie_df.movieId == id].title.values[0] for id in rating_df[(rating_df[&#39;userId&#39;] == idx) &amp; (rating_df[&#39;rating&#39;] &gt;= 4.5)].movieId.values]) recos.intersection(true_pos) . {&#34;Gulliver&#39;s Travels (1939)&#34;, &#39;Lord of the Rings, The (1978)&#39;, &#39;Newton Boys, The (1998)&#39;, &#39;Shaft (1971)&#39;, &#39;Three Caballeros, The (1945)&#39;} . mf_recos = set(get_recommendations_matrix_factorization(idx, user_factors, movie_factors)) mf_recos.intersection(true_pos) . set() . idx = 2 recos = set(get_recommended_movies_graph_embeddings((idx, &#39;user&#39;), node_vecs, top_n=10)) true_pos = set([movie_df[movie_df.movieId == id].title.values[0] for id in rating_df[(rating_df[&#39;userId&#39;] == idx) &amp; (rating_df[&#39;rating&#39;] &gt;= 4.5)].movieId.values]) recos.intersection(true_pos) . {&#39;Dark Knight, The (2008)&#39;, &#39;Inglourious Basterds (2009)&#39;, &#39;The Jinx: The Life and Deaths of Robert Durst (2015)&#39;, &#39;Warrior (2011)&#39;, &#39;Wolf of Wall Street, The (2013)&#39;} . mf_recos = set(get_recommendations_matrix_factorization(idx, user_factors, movie_factors)) mf_recos.intersection(true_pos) . set() . idx = 3 recos = set(get_recommended_movies_graph_embeddings((idx, &#39;user&#39;), node_vecs, top_n=10)) true_pos = set([movie_df[movie_df.movieId == id].title.values[0] for id in rating_df[(rating_df[&#39;userId&#39;] == idx) &amp; (rating_df[&#39;rating&#39;] &gt;= 4.5)].movieId.values]) recos.intersection(true_pos) . {&#39;Alien Contamination (1980)&#39;, &#39;Android (1982)&#39;, &#39;Clonus Horror, The (1979)&#39;, &#39;Death Race 2000 (1975)&#39;, &#39;Galaxy of Terror (Quest) (1981)&#39;, &#39;Hangar 18 (1980)&#39;, &#39;Looker (1981)&#39;, &#39;Master of the Flying Guillotine (Du bi quan wang da po xue di zi) (1975)&#39;, &#39;Saturn 3 (1980)&#39;, &#39;The Lair of the White Worm (1988)&#39;} . mf_recos = set(get_recommendations_matrix_factorization(idx, user_factors, movie_factors)) mf_recos.intersection(true_pos) . set() . Enriched network with additional information : Genres . Genres of the movies can be used as additional signal for better recommendations . movie_genre_edgelist = movie_df[[&#39;movieId&#39;, &#39;genres&#39;]] movie_genre_edgelist.head() . movieId genres . 0 1 | Adventure|Animation|Children|Comedy|Fantasy | . 1 2 | Adventure|Children|Fantasy | . 2 3 | Comedy|Romance | . 3 4 | Comedy|Drama|Romance | . 4 5 | Comedy | . genre2int . {&#39;(no genres listed)&#39;: 10353, &#39;Action&#39;: 10341, &#39;Adventure&#39;: 10334, &#39;Animation&#39;: 10335, &#39;Children&#39;: 10336, &#39;Comedy&#39;: 10337, &#39;Crime&#39;: 10342, &#39;Documentary&#39;: 10349, &#39;Drama&#39;: 10340, &#39;Fantasy&#39;: 10338, &#39;Film-Noir&#39;: 10352, &#39;Horror&#39;: 10344, &#39;IMAX&#39;: 10350, &#39;Musical&#39;: 10348, &#39;Mystery&#39;: 10345, &#39;Romance&#39;: 10339, &#39;Sci-Fi&#39;: 10346, &#39;Thriller&#39;: 10343, &#39;War&#39;: 10347, &#39;Western&#39;: 10351} . Combine the user-movie and movie-genre graph . user_movie_genre_graph = nx.Graph() user_movie_genre_graph.add_weighted_edges_from([(x,y,user_movie_graph[x][y][&#39;weight&#39;]) for x,y in user_movie_graph.edges()]) user_movie_genre_graph.add_weighted_edges_from([(x,y,movie_genre_graph[x][y][&#39;weight&#39;]) for x,y in movie_genre_graph.edges()]) . user_movie_genre_graph.number_of_edges() . 122882 . G_enriched = Graph(user_movie_genre_graph, is_directed=False, p=1, q=1) G_enriched.preprocess_transition_probs() . walks_enriched = G_enriched.simulate_walks(num_walks=10, walk_length=80) . Walk iteration: 1 / 10 2 / 10 3 / 10 4 / 10 5 / 10 6 / 10 7 / 10 8 / 10 9 / 10 10 / 10 . node_embeddings_enriched = learn_embeddings(walks_enriched) . node_vecs_enriched = [node_embeddings_enriched[str(i)] for i in range(cnt)] node_vecs_enriched = np.array(node_vecs_enriched) node_vecs_enriched.shape . (10354, 50) . get_similar_movies_graph_embeddings((260, &#39;movie&#39;), node_vecs_enriched)[:10] . [&#39;Priest (1994)&#39;, &#39;Mrs. Parker and the Vicious Circle (1994)&#39;, &#39;Last Dance (1996)&#39;, &#39;Tom &amp; Viv (1994)&#39;, &#39;Georgia (1995)&#39;, &#39;My Crazy Life (Mi vida loca) (1993)&#39;, &#39;Before the Rain (Pred dozhdot) (1994)&#39;, &#39;Haunted World of Edward D. Wood Jr., The (1996)&#39;, &#39;To Live (Huozhe) (1994)&#39;, &#39;Vanya on 42nd Street (1994)&#39;] . get_similar_movies_graph_embeddings((260, &#39;movie&#39;), node_vecs)[:10] . [&#39;Priest (1994)&#39;, &#39;Heidi Fleiss: Hollywood Madam (1995)&#39;, &#39;My Crazy Life (Mi vida loca) (1993)&#39;, &#39;Before the Rain (Pred dozhdot) (1994)&#39;, &#39;Boys of St. Vincent, The (1992)&#39;, &#39;Last Dance (1996)&#39;, &#39;Awfully Big Adventure, An (1995)&#39;, &#39;Queen Margot (Reine Margot, La) (1994)&#39;, &#34;Widows&#39; Peak (1994)&#34;, &#39;What Happened Was... (1994)&#39;] . idx = 1 true_pos = set([movie_df[movie_df.movieId == id].title.values[0] for id in rating_df[(rating_df[&#39;userId&#39;] == idx) &amp; (rating_df[&#39;rating&#39;] &gt;= 4.5)].movieId.values]) mf_recos = set(get_recommendations_matrix_factorization(idx, user_factors, movie_factors)) print(len(mf_recos.intersection(true_pos))) ge_recos = set(get_recommended_movies_graph_embeddings((idx, &#39;user&#39;), node_vecs, top_n=10)) print(len(ge_recos.intersection(true_pos))) ge_enriched_reso = set(get_recommended_movies_graph_embeddings((idx, &#39;user&#39;), node_vecs_enriched, top_n=10)) print(len(ge_enriched_reso.intersection(true_pos))) . 0 5 5 . idx = 8 true_pos = set([movie_df[movie_df.movieId == id].title.values[0] for id in rating_df[(rating_df[&#39;userId&#39;] == idx) &amp; (rating_df[&#39;rating&#39;] &gt;= 4.5)].movieId.values]) mf_recos = set(get_recommendations_matrix_factorization(idx, user_factors, movie_factors)) print(len(mf_recos.intersection(true_pos))) ge_recos = set(get_recommended_movies_graph_embeddings((idx, &#39;user&#39;), node_vecs, top_n=10)) print(len(ge_recos.intersection(true_pos))) ge_enriched_reso = set(get_recommended_movies_graph_embeddings((idx, &#39;user&#39;), node_vecs_enriched, top_n=10)) print(len(ge_enriched_reso.intersection(true_pos))) . 0 2 1 . idx = 20 true_pos = set([movie_df[movie_df.movieId == id].title.values[0] for id in rating_df[(rating_df[&#39;userId&#39;] == idx) &amp; (rating_df[&#39;rating&#39;] &gt;= 4.5)].movieId.values]) mf_recos = set(get_recommendations_matrix_factorization(idx, user_factors, movie_factors)) print(len(mf_recos.intersection(true_pos))) ge_recos = set(get_recommended_movies_graph_embeddings((idx, &#39;user&#39;), node_vecs, top_n=10)) print(len(ge_recos.intersection(true_pos))) ge_enriched_reso = set(get_recommended_movies_graph_embeddings((idx, &#39;user&#39;), node_vecs_enriched, top_n=10)) print(len(ge_enriched_reso.intersection(true_pos))) . 0 0 1 .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/graph%20embedding%20movielens%20factorization/2021/04/24/Recommendation-Node2vec.html",
            "relUrl": "/graph%20embedding%20movielens%20factorization/2021/04/24/Recommendation-Node2vec.html",
            "date": " ‚Ä¢ Apr 24, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Similar Product Recommender system using Deep Learning for an online e-commerce store",
            "content": ". Import libraries required for file operations . import os import pickle from glob import glob # import basic numerical libraries import numpy as np import pandas as pd # import keras libraries for image recognition from keras.applications import VGG16 from keras.applications.vgg16 import preprocess_input from keras.preprocessing import image as kimage . Data preparation . # download and unzip shirts folder from the directory !wget https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/shirts.zip !unzip shirts.zip . shirts_dict = dict() for shirt in glob(&#39;shirts/*.jpg&#39;): # load all shirts img = kimage.load_img(shirt, target_size=(224, 224)) # VGG accepts images in 224 X 224 pixels img = preprocess_input(np.expand_dims(kimage.img_to_array(img), axis=0)) # so some preprocessing id = shirt.split(&#39;/&#39;)[-1].split(&#39;.&#39;)[0] shirts_dict[id] = img # map image &amp; shirt id . Number of shirts = 2908 . . Model training . model = VGG16(include_top=False, weights=&#39;imagenet&#39;) shirts_matrix = np.zeros([no_of_shirts, 25088]) # initialize the matrix with zeros for i, (id, img) in enumerate(shirts_dict.items()): shirts_matrix[i, :] = model.predict(img).ravel() # flatten the matrix . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 58892288/58889256 [==============================] - 0s 0us/step . model.summary() . Model: &#34;vgg16&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, None, None, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, None, None, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, None, None, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, None, None, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, None, None, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, None, None, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, None, None, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, None, None, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, None, None, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, None, None, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, None, None, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, None, None, 512) 0 ================================================================= Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________ . . Inference pipeline . matrix_id_to_shirt_id = dict() shirt_id_to_matrix_id = dict() for i, (id, img) in enumerate(shirts_dict.items()): matrix_id_to_shirt_id[i] = id shirt_id_to_matrix_id[id] = i . . Finding top 10 similar shirts . Display the sample shirt . from IPython.display import Image Image(&#39;shirts/1015.jpg&#39;) . Print images of top-10 similar shirts . import glob import matplotlib.pyplot as plt import matplotlib.image as mpimg %matplotlib inline images = [] for shirt in closest_shirts: shirt = &#39;shirts/&#39;+shirt+&#39;.jpg&#39; for img_path in glob.glob(shirt): images.append(mpimg.imread(img_path)) plt.figure(figsize=(20,10)) columns = 5 for i, image in enumerate(images): plt.subplot(len(images) / columns + 1, columns, i + 1) plt.imshow(image) . Model persistence . from sklearn.externals import joblib joblib.dump(similarity, &#39;similarity.pkl&#39;) joblib.dump(shirt_id_to_matrix_id, &#39;shirt_id_to_matrix_id.pkl&#39;) joblib.dump(matrix_id_to_shirt_id, &#39;matrix_id_to_shirt_id.pkl&#39;) . loaded_model = joblib.load(&#39;similarity.pkl&#39;) . closest_ids = np.argsort(loaded_model[target_id, :])[::-1][0:10] closest_shirts = [matrix_id_to_shirt_id[matrix_id] for matrix_id in closest_ids] closest_shirts . [&#39;1015&#39;, &#39;1308&#39;, &#39;1187&#39;, &#39;2554&#39;, &#39;2420&#39;, &#39;2526&#39;, &#39;1174&#39;, &#39;2197&#39;, &#39;2545&#39;, &#39;1290&#39;] . . Credits .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/similarity%20vision%20retail/2021/04/23/similar-product-recommender.html",
            "relUrl": "/similarity%20vision%20retail/2021/04/23/similar-product-recommender.html",
            "date": " ‚Ä¢ Apr 23, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Simulating a news personalization scenario using Contextual Bandits",
            "content": "In the Contextual Bandit(CB) introduction tutorial, we learnt about CB and different CB algorithms. In this tutorial we will simulate the scenario of personalizing news content on a site, using CB, to users. The goal is to maximize user engagement quantified by measuring click through rate (CTR). . Let&#39;s recall that in a CB setting, a data point has four components, . Context | Action | Probability of choosing action | Reward/cost for chosen action | . In our simulator, we will need to generate a context, get an action/decision for the given context and also simulate generating a reward. . In our simulator, our goal is to maximize reward (click through rate/CTR) or minimize loss (-CTR) . We have two website visitors: &#39;Tom&#39; and &#39;Anna&#39; | Each of them may visit the website either in the morning or in the afternoon | . The context is therefore (user, time_of_day) . We have the option of recommending a variety of articles to Tom and Anna. Therefore, actions are the different choices of articles: &quot;politics&quot;, &quot;sports&quot;, &quot;music&quot;, &quot;food&quot;, &quot;finance&quot;, &quot;health&quot;, &quot;cheese&quot; . The reward is whether they click on the article or not: &#39;click&#39; or &#39;no click&#39; . Let&#39;s first start with importing the necessary packages: . Simulate reward . In the real world, we will have to learn Tom and Anna&#39;s preferences for articles as we observe their interactions. Since this is a simulation, we will have to define Tom and Anna&#39;s preference profile. The reward that we provide to the learner will follow this preference profile. Our hope is to see if the learner can take better and better decisions as we see more samples which in turn means we are maximizing the reward. . We will also modify the reward function in a few different ways and see if the CB learner picks up the changes. We will compare the CTR with and without learning. . VW optimizes to minimize cost which is negative of reward. Therefore, we will always pass negative of reward as cost to VW. . USER_LIKED_ARTICLE = -1.0 USER_DISLIKED_ARTICLE = 0.0 . The reward function below specifies that Tom likes politics in the morning and music in the afternoon whereas Anna likes sports in the morning and politics in the afternoon. It looks dense but we are just simulating our hypothetical world in the format of the feedback the learner understands: cost. If the learner recommends an article that aligns with the reward function, we give a positive reward. In our simulated world this is a click. . def get_cost(context,action): if context[&#39;user&#39;] == &quot;Tom&quot;: if context[&#39;time_of_day&#39;] == &quot;morning&quot; and action == &#39;politics&#39;: return USER_LIKED_ARTICLE elif context[&#39;time_of_day&#39;] == &quot;afternoon&quot; and action == &#39;music&#39;: return USER_LIKED_ARTICLE else: return USER_DISLIKED_ARTICLE elif context[&#39;user&#39;] == &quot;Anna&quot;: if context[&#39;time_of_day&#39;] == &quot;morning&quot; and action == &#39;sports&#39;: return USER_LIKED_ARTICLE elif context[&#39;time_of_day&#39;] == &quot;afternoon&quot; and action == &#39;politics&#39;: return USER_LIKED_ARTICLE else: return USER_DISLIKED_ARTICLE . Understanding VW format . There are some things we need to do to get our input into a format VW understands. This function handles converting from our context as a dictionary, list of articles and the cost if there is one into the text format VW understands. . def to_vw_example_format(context, actions, cb_label = None): if cb_label is not None: chosen_action, cost, prob = cb_label example_string = &quot;&quot; example_string += &quot;shared |User user={} time_of_day={} n&quot;.format(context[&quot;user&quot;], context[&quot;time_of_day&quot;]) for action in actions: if cb_label is not None and action == chosen_action: example_string += &quot;0:{}:{} &quot;.format(cost, prob) example_string += &quot;|Action article={} n&quot;.format(action) #Strip the last newline return example_string[:-1] . To understand what&#39;s going on here let&#39;s go through an example. Here, it&#39;s the morning and the user is Tom. There are four possible articles. So in the VW format there is one line that starts with shared, this is the shared context, followed by four lines each corresponding to an article. . context = {&quot;user&quot;:&quot;Tom&quot;,&quot;time_of_day&quot;:&quot;morning&quot;} actions = [&quot;politics&quot;, &quot;sports&quot;, &quot;music&quot;, &quot;food&quot;] print(to_vw_example_format(context,actions)) . shared |User user=Tom time_of_day=morning |Action article=politics |Action article=sports |Action article=music |Action article=food . Getting a decision . When we call VW we get a pmf, probability mass function, as the output. Since we are incorporating exploration into our strategy, VW will give us a list of probabilities over the set of actions. This means that the probability at a given index in the list corresponds to the likelihood of picking that specific action. In order to arrive at a decision/action, we will have to sample from this list. . So, given a list [0.7, 0.1, 0.1, 0.1], we would choose the first item with a 70% chance. sample_custom_pmf takes such a list and gives us the index it chose and what the probability of choosing that index was. . def sample_custom_pmf(pmf): total = sum(pmf) scale = 1/total pmf = [x * scale for x in pmf] draw = random.random() sum_prob = 0.0 for index, prob in enumerate(pmf): sum_prob += prob if(sum_prob &gt; draw): return index, prob . We have all of the information we need to choose an action for a specific user and context. To use VW to achieve this, we will do the following: . We convert our context and actions into the text format we need | We pass this example to vw and get the pmf out | Now, we sample this pmf to get what article we will end up showing | Finally we return the article chosen, and the probability of choosing it (we are going to need the probability when we learn form this example) | def get_action(vw, context, actions): vw_text_example = to_vw_example_format(context,actions) pmf = vw.predict(vw_text_example) chosen_action_index, prob = sample_custom_pmf(pmf) return actions[chosen_action_index], prob . Simulation set up . Now that we have done all of the setup work and know how to interface with VW, let&#39;s simulate the world of Tom and Anna. The scenario is they go to a website and are shown an article. Remember that the reward function allows us to define the worlds reaction to what VW recommends. . We will choose between Tom and Anna uniformly at random and also choose their time of visit uniformly at random. You can think of this as us tossing a coin to choose between Tom and Anna (Anna if heads and Tom if tails) and another coin toss for choosing time of day. . actions camping finance food health music politics sports . users times_of_day . Anna afternoon 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | -1.0 | 0.0 | . morning 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | -1.0 | . Tom afternoon 0.0 | 0.0 | 0.0 | 0.0 | -1.0 | 0.0 | 0.0 | . morning 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | -1.0 | 0.0 | . We will instantiate a CB learner in VW and then simulate Tom and Anna&#39;s website visits num_iterations number of times. In each visit, we: . Decide between Tom and Anna | Decide time of day | Pass context i.e. (user, time of day) to learner to get action i.e. article recommendation and probability of choosing action | Receive reward i.e. see if user clicked or not. Remember that cost is just negative reward. | Format context, action, probability, reward in VW format | Learn from the example VW reduces a CB problem to a cost sensitive multiclass classification problem. | . | This is the same for every one of our simulations, so we define the process in the run_simulation function. The cost function must be supplied as this is essentially us simulating how the world works. . def run_simulation(vw, num_iterations, users, times_of_day, actions, cost_function, do_learn = True): cost_sum = 0. ctr = [] for i in range(1, num_iterations+1): # 1. In each simulation choose a user user = choose_user(users) # 2. Choose time of day for a given user time_of_day = choose_time_of_day(times_of_day) # 3. Pass context to vw to get an action context = {&#39;user&#39;: user, &#39;time_of_day&#39;: time_of_day} action, prob = get_action(vw, context, actions) # 4. Get cost of the action we chose cost = cost_function(context, action) cost_sum += cost if do_learn: # 5. Inform VW of what happened so we can learn from it vw_format = vw.parse(to_vw_example_format(context, actions, (action, cost, prob)),pyvw.vw.lContextualBandit) # 6. Learn vw.learn(vw_format) # We negate this so that on the plot instead of minimizing cost, we are maximizing reward ctr.append(-1*cost_sum/i) return ctr . We want to be able to visualize what is occurring, so we are going to plot the click through rate over each iteration of the simulation. If VW is showing actions the get rewards the ctr will be higher. Below is a little utility function to make showing the plot easier. . Scenario 1 . We will use the first reward function get_cost and assume that Tom and Anna do not change their preferences over time and see what happens to user engagement as we learn. We will also see what happens when there is no learning. We will use the &quot;no learning&quot; case as our baseline to compare to. . With learning . vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations = 5000 ctr = run_simulation(vw, num_iterations, users, times_of_day, actions, get_cost) plot_ctr(num_iterations, ctr) . Aside: interactions . You&#39;ll notice in the arguments we supply to VW, we include -q UA. This is telling VW to create additional features which are the features in the (U)ser namespace and (A)ction namespaces multiplied together. This allows us to learn the interaction between when certain actions are good in certain times of days and for particular users. If we didn&#39;t do that, the learning wouldn&#39;t really work. We can see that in action below. . vw = pyvw.vw(&quot;--cb_explore_adf --quiet --epsilon 0.2&quot;) num_iterations = 5000 ctr = run_simulation(vw, num_iterations, users, times_of_day, actions, get_cost) plot_ctr(num_iterations, ctr) . Without learning . Let&#39;s do the same thing again (but with -q, but this time show the effect if we don&#39;t learn from what happens. The ctr never improves are we just hover around 0.2. . vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations = 5000 ctr = run_simulation(vw, num_iterations, users, times_of_day, actions, get_cost, do_learn=False) plot_ctr(num_iterations, ctr) . Scenario 2 . In the real world people&#39;s preferences change over time. So now in the simulation we are going to incorporate two different cost functions, and swap over to the second one halfway through. Below is a a table of the new reward function we are going to use, get_cost_1: . Tom . get_cost get_cost_new1 . Morning | Politics | Politics | . Afternoon | Music | Sports | . Anna . get_cost get_cost_new1 . Morning | Sports | Sports | . Afternoon | Politics | Sports | . This reward function is still working with actions that the learner has seen previously. . To make it easy to show the effect of the cost function changing we are going to modify the run_simulation function. It is a little less readable now, but it supports accepting a list of cost functions and it will operate over each cost function in turn. This is perfect for what we need. . With learning . Let us now switch to the second reward function after a few samples (running the first reward function). Recall that this reward function changes the preferences of the web users but it is still working with the same action space as before. We should see the learner pick up these changes and optimize towards the new preferences. . # Instantiate learner in VW vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations_per_cost_func = 5000 cost_functions = [get_cost, get_cost_new1] total_iterations = num_iterations_per_cost_func * len(cost_functions) ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions) plot_ctr(total_iterations, ctr) . Note: The initial spike in CTR depends on the rewards received for the first few examples. When you run on your own, you may see something different initially because our simulator is designed to have randomness. . Without learning . # use first reward function initially and then switch to second reward function # Instantiate learner in VW vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations_per_cost_func = 5000 cost_functions = [get_cost, get_cost_new1] total_iterations = num_iterations_per_cost_func * len(cost_functions) ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions, do_learn=False) plot_ctr(total_iterations, ctr) . Scenario 3 . In this scenario we are going to start rewarding actions that have never seen a reward previously when we change the cost function. . Tom . get_cost get_cost_new2 . Morning | Politics | Politics | . Afternoon | Music | Food | . Anna . get_cost get_cost_new2 . Morning | Sports | Food | . Afternoon | Politics | Food | . def get_cost_new2(context,action): if context[&#39;user&#39;] == &quot;Tom&quot;: if context[&#39;time_of_day&#39;] == &quot;morning&quot; and action == &#39;politics&#39;: return USER_LIKED_ARTICLE elif context[&#39;time_of_day&#39;] == &quot;afternoon&quot; and action == &#39;food&#39;: return USER_LIKED_ARTICLE else: return USER_DISLIKED_ARTICLE elif context[&#39;user&#39;] == &quot;Anna&quot;: if context[&#39;time_of_day&#39;] == &quot;morning&quot; and action == &#39;food&#39;: return USER_LIKED_ARTICLE elif context[&#39;time_of_day&#39;] == &quot;afternoon&quot; and action == &#39;food&#39;: return USER_LIKED_ARTICLE else: return USER_DISLIKED_ARTICLE . With learning . Let us now switch to the third reward function after a few samples (running the first reward function). Recall that this reward function changes the preferences of the users and is working with a different action space than before. We should see the learner pick up these changes and optimize towards the new preferences . # Instantiate learner in VW vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations_per_cost_func = 5000 cost_functions = [get_cost, get_cost_new2] total_iterations = num_iterations_per_cost_func * len(cost_functions) ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions) plot_ctr(total_iterations, ctr) . Without Learning . # use first reward function initially and then switch to third reward function # Instantiate learner in VW vw = pyvw.vw(&quot;--cb_explore_adf -q UA --quiet --epsilon 0.2&quot;) num_iterations_per_cost_func = 5000 cost_functions = [get_cost, get_cost_new2] total_iterations = num_iterations_per_cost_func * len(cost_functions) ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions, do_learn=False) plot_ctr(total_iterations, ctr) . Summary . This tutorial aimed at showcasing a real world scenario where contextual bandit algorithms can be used. We were able to take a context and set of actions and learn what actions worked best for a given context. We saw that the learner was able to respond rapidly to changes in the world. We showed that allowing the learner to interact with the world resulted in higher rewards than the no learning baseline. . This tutorial worked with simplistic features. VW supports high dimensional sparse features, different exploration algorithms and policy evaluation approaches. . credits .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/reinforcement%20contextual/2021/04/22/vowpal-wabbit-contextual-recommender.html",
            "relUrl": "/reinforcement%20contextual/2021/04/22/vowpal-wabbit-contextual-recommender.html",
            "date": " ‚Ä¢ Apr 22, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Neural Matrix Factorization from scratch in PyTorch",
            "content": "!pip install -q tensorboardX . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122kB 8.3MB/s . import os import time import random import argparse import numpy as np import pandas as pd import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as data from tensorboardX import SummaryWriter . Downloading Movielens-1M Ratings . DATA_URL = &quot;https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-1m-dat/ratings.dat&quot; MAIN_PATH = &#39;/content/&#39; DATA_PATH = MAIN_PATH + &#39;ratings.dat&#39; MODEL_PATH = MAIN_PATH + &#39;models/&#39; MODEL = &#39;ml-1m_Neu_MF&#39; . !wget -nc https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-1m-dat/ratings.dat . Defining Dataset Classes . class Rating_Datset(torch.utils.data.Dataset): def __init__(self, user_list, item_list, rating_list): super(Rating_Datset, self).__init__() self.user_list = user_list self.item_list = item_list self.rating_list = rating_list def __len__(self): return len(self.user_list) def __getitem__(self, idx): user = self.user_list[idx] item = self.item_list[idx] rating = self.rating_list[idx] return ( torch.tensor(user, dtype=torch.long), torch.tensor(item, dtype=torch.long), torch.tensor(rating, dtype=torch.float) ) . NCF Dataset Class . _reindex: process dataset to reindex userID and itemID, also set rating as binary feedback | _leave_one_out: leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf | negative_sampling: randomly selects n negative examples for each positive one | . class NCF_Data(object): &quot;&quot;&quot; Construct Dataset for NCF &quot;&quot;&quot; def __init__(self, args, ratings): self.ratings = ratings self.num_ng = args.num_ng self.num_ng_test = args.num_ng_test self.batch_size = args.batch_size self.preprocess_ratings = self._reindex(self.ratings) self.user_pool = set(self.ratings[&#39;user_id&#39;].unique()) self.item_pool = set(self.ratings[&#39;item_id&#39;].unique()) self.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings) self.negatives = self._negative_sampling(self.preprocess_ratings) random.seed(args.seed) def _reindex(self, ratings): &quot;&quot;&quot; Process dataset to reindex userID and itemID, also set rating as binary feedback &quot;&quot;&quot; user_list = list(ratings[&#39;user_id&#39;].drop_duplicates()) user2id = {w: i for i, w in enumerate(user_list)} item_list = list(ratings[&#39;item_id&#39;].drop_duplicates()) item2id = {w: i for i, w in enumerate(item_list)} ratings[&#39;user_id&#39;] = ratings[&#39;user_id&#39;].apply(lambda x: user2id[x]) ratings[&#39;item_id&#39;] = ratings[&#39;item_id&#39;].apply(lambda x: item2id[x]) ratings[&#39;rating&#39;] = ratings[&#39;rating&#39;].apply(lambda x: float(x &gt; 0)) return ratings def _leave_one_out(self, ratings): &quot;&quot;&quot; leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf &quot;&quot;&quot; ratings[&#39;rank_latest&#39;] = ratings.groupby([&#39;user_id&#39;])[&#39;timestamp&#39;].rank(method=&#39;first&#39;, ascending=False) test = ratings.loc[ratings[&#39;rank_latest&#39;] == 1] train = ratings.loc[ratings[&#39;rank_latest&#39;] &gt; 1] assert train[&#39;user_id&#39;].nunique()==test[&#39;user_id&#39;].nunique(), &#39;Not Match Train User with Test User&#39; return train[[&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;]], test[[&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;]] def _negative_sampling(self, ratings): interact_status = ( ratings.groupby(&#39;user_id&#39;)[&#39;item_id&#39;] .apply(set) .reset_index() .rename(columns={&#39;item_id&#39;: &#39;interacted_items&#39;})) interact_status[&#39;negative_items&#39;] = interact_status[&#39;interacted_items&#39;].apply(lambda x: self.item_pool - x) interact_status[&#39;negative_samples&#39;] = interact_status[&#39;negative_items&#39;].apply(lambda x: random.sample(x, self.num_ng_test)) return interact_status[[&#39;user_id&#39;, &#39;negative_items&#39;, &#39;negative_samples&#39;]] def get_train_instance(self): users, items, ratings = [], [], [] train_ratings = pd.merge(self.train_ratings, self.negatives[[&#39;user_id&#39;, &#39;negative_items&#39;]], on=&#39;user_id&#39;) train_ratings[&#39;negatives&#39;] = train_ratings[&#39;negative_items&#39;].apply(lambda x: random.sample(x, self.num_ng)) for row in train_ratings.itertuples(): users.append(int(row.user_id)) items.append(int(row.item_id)) ratings.append(float(row.rating)) for i in range(self.num_ng): users.append(int(row.user_id)) items.append(int(row.negatives[i])) ratings.append(float(0)) # negative samples get 0 rating dataset = Rating_Datset( user_list=users, item_list=items, rating_list=ratings) return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4) def get_test_instance(self): users, items, ratings = [], [], [] test_ratings = pd.merge(self.test_ratings, self.negatives[[&#39;user_id&#39;, &#39;negative_samples&#39;]], on=&#39;user_id&#39;) for row in test_ratings.itertuples(): users.append(int(row.user_id)) items.append(int(row.item_id)) ratings.append(float(row.rating)) for i in getattr(row, &#39;negative_samples&#39;): users.append(int(row.user_id)) items.append(int(i)) ratings.append(float(0)) dataset = Rating_Datset( user_list=users, item_list=items, rating_list=ratings) return torch.utils.data.DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=2) . Defining Metrics . Using Hit Rate and NDCG as our evaluation metrics . def hit(ng_item, pred_items): if ng_item in pred_items: return 1 return 0 def ndcg(ng_item, pred_items): if ng_item in pred_items: index = pred_items.index(ng_item) return np.reciprocal(np.log2(index+2)) return 0 def metrics(model, test_loader, top_k, device): HR, NDCG = [], [] for user, item, label in test_loader: user = user.to(device) item = item.to(device) predictions = model(user, item) _, indices = torch.topk(predictions, top_k) recommends = torch.take( item, indices).cpu().numpy().tolist() ng_item = item[0].item() # leave one-out evaluation has only one item per user HR.append(hit(ng_item, recommends)) NDCG.append(ndcg(ng_item, recommends)) return np.mean(HR), np.mean(NDCG) . Defining Model Architectures . Generalized Matrix Factorization | Multi Layer Perceptron | Neural Matrix Factorization | class Generalized_Matrix_Factorization(nn.Module): def __init__(self, args, num_users, num_items): super(Generalized_Matrix_Factorization, self).__init__() self.num_users = num_users self.num_items = num_items self.factor_num = args.factor_num self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num) self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num) self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1) self.logistic = nn.Sigmoid() def forward(self, user_indices, item_indices): user_embedding = self.embedding_user(user_indices) item_embedding = self.embedding_item(item_indices) element_product = torch.mul(user_embedding, item_embedding) logits = self.affine_output(element_product) rating = self.logistic(logits) return rating def init_weight(self): pass . class Multi_Layer_Perceptron(nn.Module): def __init__(self, args, num_users, num_items): super(Multi_Layer_Perceptron, self).__init__() self.num_users = num_users self.num_items = num_items self.factor_num = args.factor_num self.layers = args.layers self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num) self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num) self.fc_layers = nn.ModuleList() for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])): self.fc_layers.append(nn.Linear(in_size, out_size)) self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1) self.logistic = nn.Sigmoid() def forward(self, user_indices, item_indices): user_embedding = self.embedding_user(user_indices) item_embedding = self.embedding_item(item_indices) vector = torch.cat([user_embedding, item_embedding], dim=-1) # the concat latent vector for idx, _ in enumerate(range(len(self.fc_layers))): vector = self.fc_layers[idx](vector) vector = nn.ReLU()(vector) # vector = nn.BatchNorm1d()(vector) # vector = nn.Dropout(p=0.5)(vector) logits = self.affine_output(vector) rating = self.logistic(logits) return rating def init_weight(self): pass . class NeuMF(nn.Module): def __init__(self, args, num_users, num_items): super(NeuMF, self).__init__() self.num_users = num_users self.num_items = num_items self.factor_num_mf = args.factor_num self.factor_num_mlp = int(args.layers[0]/2) self.layers = args.layers self.dropout = args.dropout self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp) self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp) self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf) self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf) self.fc_layers = nn.ModuleList() for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])): self.fc_layers.append(torch.nn.Linear(in_size, out_size)) self.fc_layers.append(nn.ReLU()) self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_mf, out_features=1) self.logistic = nn.Sigmoid() self.init_weight() def init_weight(self): nn.init.normal_(self.embedding_user_mlp.weight, std=0.01) nn.init.normal_(self.embedding_item_mlp.weight, std=0.01) nn.init.normal_(self.embedding_user_mf.weight, std=0.01) nn.init.normal_(self.embedding_item_mf.weight, std=0.01) for m in self.fc_layers: if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight) nn.init.xavier_uniform_(self.affine_output.weight) for m in self.modules(): if isinstance(m, nn.Linear) and m.bias is not None: m.bias.data.zero_() def forward(self, user_indices, item_indices): user_embedding_mlp = self.embedding_user_mlp(user_indices) item_embedding_mlp = self.embedding_item_mlp(item_indices) user_embedding_mf = self.embedding_user_mf(user_indices) item_embedding_mf = self.embedding_item_mf(item_indices) mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1) mf_vector =torch.mul(user_embedding_mf, item_embedding_mf) for idx, _ in enumerate(range(len(self.fc_layers))): mlp_vector = self.fc_layers[idx](mlp_vector) vector = torch.cat([mlp_vector, mf_vector], dim=-1) logits = self.affine_output(vector) rating = self.logistic(logits) return rating.squeeze() . Setting Arguments . Here is the brief description of important ones: . Learning rate is 0.001 | Dropout rate is 0.2 | Running for 10 epochs | HitRate@10 and NDCG@10 | 4 negative samples for each positive one | . parser = argparse.ArgumentParser() parser.add_argument(&quot;--seed&quot;, type=int, default=42, help=&quot;Seed&quot;) parser.add_argument(&quot;--lr&quot;, type=float, default=0.001, help=&quot;learning rate&quot;) parser.add_argument(&quot;--dropout&quot;, type=float, default=0.2, help=&quot;dropout rate&quot;) parser.add_argument(&quot;--batch_size&quot;, type=int, default=256, help=&quot;batch size for training&quot;) parser.add_argument(&quot;--epochs&quot;, type=int, default=10, help=&quot;training epoches&quot;) parser.add_argument(&quot;--top_k&quot;, type=int, default=10, help=&quot;compute metrics@top_k&quot;) parser.add_argument(&quot;--factor_num&quot;, type=int, default=32, help=&quot;predictive factors numbers in the model&quot;) parser.add_argument(&quot;--layers&quot;, nargs=&#39;+&#39;, default=[64,32,16,8], help=&quot;MLP layers. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.&quot;) parser.add_argument(&quot;--num_ng&quot;, type=int, default=4, help=&quot;Number of negative samples for training set&quot;) parser.add_argument(&quot;--num_ng_test&quot;, type=int, default=100, help=&quot;Number of negative samples for test set&quot;) parser.add_argument(&quot;--out&quot;, default=True, help=&quot;save model or not&quot;) . . _StoreAction(option_strings=[&#39;--out&#39;], dest=&#39;out&#39;, nargs=None, const=None, default=True, type=None, choices=None, help=&#39;save model or not&#39;, metavar=None) . Training NeuMF Model . args = parser.parse_args(&quot;&quot;) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) writer = SummaryWriter() # seed for Reproducibility seed_everything(args.seed) # load data ml_1m = pd.read_csv( DATA_PATH, sep=&quot;::&quot;, names = [&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;, &#39;timestamp&#39;], engine=&#39;python&#39;) # set the num_users, items num_users = ml_1m[&#39;user_id&#39;].nunique()+1 num_items = ml_1m[&#39;item_id&#39;].nunique()+1 # construct the train and test datasets data = NCF_Data(args, ml_1m) train_loader = data.get_train_instance() test_loader = data.get_test_instance() # set model and loss, optimizer model = NeuMF(args, num_users, num_items) model = model.to(device) loss_function = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=args.lr) # train, evaluation best_hr = 0 for epoch in range(1, args.epochs+1): model.train() # Enable dropout (if have). start_time = time.time() for user, item, label in train_loader: user = user.to(device) item = item.to(device) label = label.to(device) optimizer.zero_grad() prediction = model(user, item) loss = loss_function(prediction, label) loss.backward() optimizer.step() writer.add_scalar(&#39;loss/Train_loss&#39;, loss.item(), epoch) model.eval() HR, NDCG = metrics(model, test_loader, args.top_k, device) writer.add_scalar(&#39;Perfomance/HR@10&#39;, HR, epoch) writer.add_scalar(&#39;Perfomance/NDCG@10&#39;, NDCG, epoch) elapsed_time = time.time() - start_time print(&quot;The time elapse of epoch {:03d}&quot;.format(epoch) + &quot; is: &quot; + time.strftime(&quot;%H: %M: %S&quot;, time.gmtime(elapsed_time))) print(&quot;HR: {:.3f} tNDCG: {:.3f}&quot;.format(np.mean(HR), np.mean(NDCG))) if HR &gt; best_hr: best_hr, best_ndcg, best_epoch = HR, NDCG, epoch if args.out: if not os.path.exists(MODEL_PATH): os.mkdir(MODEL_PATH) torch.save(model, &#39;{}{}.pth&#39;.format(MODEL_PATH, MODEL)) writer.close() . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) . The time elapse of epoch 001 is: 00: 02: 30 HR: 0.624 NDCG: 0.362 The time elapse of epoch 002 is: 00: 02: 31 HR: 0.663 NDCG: 0.392 The time elapse of epoch 003 is: 00: 02: 30 HR: 0.673 NDCG: 0.399 The time elapse of epoch 004 is: 00: 02: 30 HR: 0.677 NDCG: 0.402 The time elapse of epoch 005 is: 00: 02: 31 HR: 0.671 NDCG: 0.399 The time elapse of epoch 006 is: 00: 02: 32 HR: 0.671 NDCG: 0.400 The time elapse of epoch 007 is: 00: 02: 32 HR: 0.669 NDCG: 0.400 The time elapse of epoch 008 is: 00: 02: 31 HR: 0.665 NDCG: 0.395 The time elapse of epoch 009 is: 00: 02: 33 HR: 0.664 NDCG: 0.393 The time elapse of epoch 010 is: 00: 02: 32 HR: 0.667 NDCG: 0.394 . Final Output . print(&quot;Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}&quot;.format( best_epoch, best_hr, best_ndcg)) .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/matrixfactorization%20movielens%20pytorch%20scratch/2021/04/21/rec-algo-ncf-pytorch-pyy0715.html",
            "relUrl": "/matrixfactorization%20movielens%20pytorch%20scratch/2021/04/21/rec-algo-ncf-pytorch-pyy0715.html",
            "date": " ‚Ä¢ Apr 21, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Large-scale Document Retrieval with ElasticSearch",
            "content": "Retrieval Flow Overview . . Part 1 - Setting up Elasticsearch . Download the elasticsearch archive (linux), setup a local server | Create a client connection to the local elasticsearch instance | . # download the latest elasticsearch version !wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.11.1-linux-x86_64.tar.gz !tar -xzvf elasticsearch-7.11.1-linux-x86_64.tar.gz !chown -R daemon:daemon elasticsearch-7.11.1 # prep the elasticsearch server import os from subprocess import Popen, PIPE, STDOUT es_subprocess = Popen([&#39;elasticsearch-7.11.1/bin/elasticsearch&#39;], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda : os.setuid(1)) # wait for a few minutes for the local host to start !curl -X GET &quot;localhost:9200/&quot; # install elasticsearch python api !pip install -q elasticsearch . # check if elasticsearch server is properly running in the background from elasticsearch import Elasticsearch, helpers es_client = Elasticsearch([&#39;localhost&#39;]) es_client.info() . {&#39;cluster_name&#39;: &#39;elasticsearch&#39;, &#39;cluster_uuid&#39;: &#39;WQS1QVG8RX6FQ65LS6MyrA&#39;, &#39;name&#39;: &#39;50176241ce38&#39;, &#39;tagline&#39;: &#39;You Know, for Search&#39;, &#39;version&#39;: {&#39;build_date&#39;: &#39;2021-02-15T13:44:09.394032Z&#39;, &#39;build_flavor&#39;: &#39;default&#39;, &#39;build_hash&#39;: &#39;ff17057114c2199c9c1bbecc727003a907c0db7a&#39;, &#39;build_snapshot&#39;: False, &#39;build_type&#39;: &#39;tar&#39;, &#39;lucene_version&#39;: &#39;8.7.0&#39;, &#39;minimum_index_compatibility_version&#39;: &#39;6.0.0-beta1&#39;, &#39;minimum_wire_compatibility_version&#39;: &#39;6.8.0&#39;, &#39;number&#39;: &#39;7.11.1&#39;}} . . Part 2 - Walking through an embedding-based retrieval system . Download MovieLens dataset . !wget https://files.grouplens.org/datasets/movielens/ml-25m.zip --no-check-certificate !unzip ml-25m.zip . Archive: ml-25m.zip creating: ml-25m/ inflating: ml-25m/tags.csv inflating: ml-25m/links.csv inflating: ml-25m/README.txt inflating: ml-25m/ratings.csv inflating: ml-25m/genome-tags.csv inflating: ml-25m/genome-scores.csv inflating: ml-25m/movies.csv . import pandas as pd data = pd.read_csv(&#39;ml-25m/movies.csv&#39;).drop_duplicates() data.head() . movieId title genres . 0 1 | Toy Story (1995) | Adventure|Animation|Children|Comedy|Fantasy | . 1 2 | Jumanji (1995) | Adventure|Children|Fantasy | . 2 3 | Grumpier Old Men (1995) | Comedy|Romance | . 3 4 | Waiting to Exhale (1995) | Comedy|Drama|Romance | . 4 5 | Father of the Bride Part II (1995) | Comedy | . Build index with document vectors . import tensorflow_hub as hub from timeit import default_timer as timer import json . embed = hub.load(&quot;https://tfhub.dev/google/universal-sentence-encoder-large/5&quot;) . INDEX_NAME = &quot;movie_title&quot; BATCH_SIZE = 200 SEARCH_SIZE = 10 MAPPINGS = { &#39;mappings&#39;: {&#39;_source&#39;: {&#39;enabled&#39;: &#39;true&#39;}, &#39;dynamic&#39;: &#39;true&#39;, &#39;properties&#39;: {&#39;title_vector&#39;: {&#39;dims&#39;: 512, &#39;type&#39;: &#39;dense_vector&#39;}, &#39;movie_id&#39;: {&#39;type&#39;: &#39;keyword&#39;}, &#39;genres&#39;: {&#39;type&#39;: &#39;keyword&#39;} } }, &#39;settings&#39;: {&#39;number_of_replicas&#39;: 1, &#39;number_of_shards&#39;:2} } . Ref - https://youtu.be/F4D08uU3mPA . index_movie_lens(data, num_doc=2000) . creating the movie_title index. Indexed 400 documents in 27.59 seconds. Indexed 800 documents in 48.96 seconds. Indexed 1200 documents in 70.18 seconds. Indexed 1600 documents in 90.92 seconds. Indexed 2000 documents in 111.85 seconds. Done indexing 2000 documents in 111.85 seconds . Search with query vector . return_top_movies(&quot;war&quot;) . 2000 total hits. id: 335, score: 0.5282537 {&#39;genres&#39;: &#39;Adventure|Drama|War&#39;, &#39;title&#39;: &#39;War, The (1994)&#39;} id: 712, score: 0.43743240000000005 {&#39;genres&#39;: &#39;Documentary&#39;, &#39;title&#39;: &#39;War Stories (1995)&#39;} id: 1493, score: 0.3954858000000001 {&#39;genres&#39;: &#39;Drama&#39;, &#39;title&#39;: &#39;War at Home, The (1996)&#39;} id: 1362, score: 0.32700850000000004 {&#39;genres&#39;: &#39;Romance|War&#39;, &#39;title&#39;: &#39;In Love and War (1996)&#39;} id: 550, score: 0.3104720000000001 {&#39;genres&#39;: &#39;Documentary&#39;, &#39;title&#39;: &#39;War Room, The (1993)&#39;} id: 1828, score: 0.30568780000000007 {&#39;genres&#39;: &#39;Action|Romance|Sci-Fi|Thriller&#39;, &#39;title&#39;: &#39;Armageddon (1998)&#39;} id: 1932, score: 0.3055576 {&#39;genres&#39;: &#39;Adventure|Sci-Fi&#39;, &#39;title&#39;: &#39;Dune (1984)&#39;} id: 1265, score: 0.2961224 {&#39;genres&#39;: &#39;Drama|War&#39;, &#39;title&#39;: &#39;Killing Fields, The (1984)&#39;} id: 1063, score: 0.2951368999999999 {&#39;genres&#39;: &#39;Drama|War&#39;, &#39;title&#39;: &#39;Platoon (1986)&#39;} id: 1676, score: 0.2776048 {&#39;genres&#39;: &#39;Comedy&#39;, &#39;title&#39;: &#39;Senseless (1998)&#39;} . Part 3 - Approximate Nearest Neighbor (ANN) Algorithms . . !pip install faiss !pip install nmslib !apt-get install libomp-dev import faiss import nmslib . documents = data[&#39;title&#39;].to_list()[:2000] # # OOM for large document size embeddings = embed(documents).numpy() embeddings.shape . (2000, 512) . class DemoIndexLSH(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, num_bits=8): self.index = faiss.IndexLSH(self.dimension, num_bits) self.index.add(self.embeddings) def query(self, input_embedding, k=5): distances, indices = self.index.search(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances[0], indices[0])] index_lsh = DemoIndexLSH(512, documents, embeddings) index_lsh.build(num_bits=16) . class DemoIndexIVFPQ(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, number_of_partition=2, number_of_subquantizers=2, subvector_bits=4): quantizer = faiss.IndexFlatL2(self.dimension) self.index = faiss.IndexIVFPQ(quantizer, self.dimension, number_of_partition, number_of_subquantizers, subvector_bits) self.index.train(self.embeddings) self.index.add(self.embeddings) def query(self, input_embedding, k=5): distances, indices = self.index.search(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances[0], indices[0])] index_pq = DemoIndexIVFPQ(512, documents, embeddings) index_pq.build() . class DemoHNSW(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, num_bits=8): self.index = nmslib.init(method=&#39;hnsw&#39;, space=&#39;cosinesimil&#39;) self.index.addDataPointBatch(self.embeddings) self.index.createIndex({&#39;post&#39;: 2}, print_progress=True) def query(self, input_embedding, k=5): indices, distances = self.index.knnQuery(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances, indices)] index_hnsw = DemoHNSW(512, documents, embeddings) index_hnsw.build() . class DemoIndexFlatL2(): def __init__(self, dimension, documents, embeddings): self.dimension = dimension self.documents = documents self.embeddings = embeddings def build(self, num_bits=8): self.index = faiss.IndexFlatL2(self.dimension) self.index.add(self.embeddings) def query(self, input_embedding, k=5): distances, indices = self.index.search(input_embedding, k) return [(distance, self.documents[index]) for distance, index in zip(distances[0], indices[0])] index_flat = DemoIndexFlatL2(512, documents, embeddings) index_flat.build() . def return_ann_top_movies(ann_index, query, k=SEARCH_SIZE): query_vector = embed([query]).numpy() search_start = timer() top_docs = ann_index.query(query_vector, k) search_time = timer() - search_start print(&quot;search time: {:.2f} ms&quot;.format(search_time * 1000)) return top_docs . return_ann_top_movies(index_flat, &quot;romance&quot;) . search time: 0.82 ms . [(0.95573366, &#39;True Romance (1993)&#39;), (1.2160163, &#39;Love Serenade (1996)&#39;), (1.2626679, &#39;Love Affair (1994)&#39;), (1.3447753, &#39;Kissed (1996)&#39;), (1.3752131, &#39;In Love and War (1996)&#39;), (1.3804029, &#39;Casablanca (1942)&#39;), (1.3832319, &#39;Flirt (1995)&#39;), (1.38626, &#39;Moonlight and Valentino (1995)&#39;), (1.3862813, &#39;Hotel de Love (1996)&#39;), (1.3907104, &#39;Intimate Relations (1996)&#39;)] . return_ann_top_movies(index_lsh, &quot;romance&quot;) . search time: 0.56 ms . [(2.0, &#39;Visitors, The (Visiteurs, Les) (1993)&#39;), (2.0, &#39;City Hall (1996)&#39;), (2.0, &#39;Paradise Road (1997)&#39;), (3.0, &#39;When a Man Loves a Woman (1994)&#39;), (3.0, &#39;Cosi (1996)&#39;), (3.0, &#39;Haunted World of Edward D. Wood Jr., The (1996)&#39;), (3.0, &#39;Eddie (1996)&#39;), (3.0, &#39;Ransom (1996)&#39;), (3.0, &#39;Time to Kill, A (1996)&#39;), (3.0, &#39;Mirage (1995)&#39;)] . return_ann_top_movies(index_pq, &quot;romance&quot;) . search time: 0.19 ms . [(1.07124, &#39;Streetcar Named Desire, A (1951)&#39;), (1.07124, &#39;Moonlight Murder (1936)&#39;), (1.0847104, &#39;To Kill a Mockingbird (1962)&#39;), (1.0847104, &#39;Meet John Doe (1941)&#39;), (1.0867723, &#39;Moonlight and Valentino (1995)&#39;), (1.0901785, &#39;Laura (1944)&#39;), (1.0901785, &#39;Rebecca (1940)&#39;), (1.0901785, &#39;African Queen, The (1951)&#39;), (1.0901785, &#39;Gigi (1958)&#39;), (1.0901785, &#39;Scarlet Letter, The (1926)&#39;)] . return_ann_top_movies(index_hnsw, &quot;romance&quot;) . search time: 0.29 ms . [(0.47786665, &#39;True Romance (1993)&#39;), (0.6080081, &#39;Love Serenade (1996)&#39;), (0.63133395, &#39;Love Affair (1994)&#39;), (0.6723877, &#39;Kissed (1996)&#39;), (0.6876065, &#39;In Love and War (1996)&#39;), (0.6916158, &#39;Flirt (1995)&#39;), (0.69312984, &#39;Moonlight and Valentino (1995)&#39;), (0.69314075, &#39;Hotel de Love (1996)&#39;), (0.69535506, &#39;Intimate Relations (1996)&#39;), (0.6985383, &#39;Love in Bloom (1935)&#39;)] .",
            "url": "https://sparsh-ai.github.io/rec-tutorials/large-scale%20elasticsearch/2021/04/20/dl-retrieval.html",
            "relUrl": "/large-scale%20elasticsearch/2021/04/20/dl-retrieval.html",
            "date": " ‚Ä¢ Apr 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://sparsh-ai.github.io/rec-tutorials/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://sparsh-ai.github.io/rec-tutorials/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}